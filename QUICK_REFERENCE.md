# AIMS DQ Pipeline - Quick Reference Card

## ðŸš€ Quick Start (5 Minutes)

### 1. Activate Environment
```bash
conda activate fabric-dq
```

### 2. Run Pipeline
```bash
jupyter notebook notebooks/
```

**Execute in order:**
1. `01_AIMS_Data_Profiling.ipynb` â†’ Generates configs
2. `02_AIMS_Data_Ingestion.ipynb` â†’ Validates data
3. `03_AIMS_Monitoring.ipynb` â†’ Shows dashboards

---

## ðŸ“Š What Each Notebook Does

| Notebook | Purpose | Runtime | Output |
|----------|---------|---------|--------|
| **01_Profiling** | Generate validation rules | ~5 min | 68 YAML configs |
| **02_Ingestion** | Validate & track files | ~3 min | Watermarks + logs |
| **03_Monitoring** | Interactive dashboards | ~30 sec | 15 visualizations |

---

## ðŸ“ Key Files & Locations

### Inputs
```
data/Samples_LH_Bronze_Aims_26_parquet/  â† Source data (68 files)
```

### Outputs
```
dq_great_expectations/generated_configs/  â† YAML rules (68 files)
data/state/watermarks.json                â† Processed files tracker
data/state/dq_logs.jsonl                  â† Validation logs
data/state/dq_report_*.json               â† Summary reports
```

---

## ðŸŽ¯ Common Tasks

### First Time Setup
```bash
1. Run Notebook 01 (generate configs)
2. Run Notebook 02 (validate all files)
3. Run Notebook 03 (view results)
```

### Daily Operations
```bash
1. Skip Notebook 01 (configs already exist)
2. Run Notebook 02 (process new files only)
3. Run Notebook 03 (review results)
```

### Reset & Reprocess Everything
```bash
rm data/state/watermarks.json
# Then run Notebooks 02 & 03
```

### Investigate Failures
```bash
# Run Notebook 03, then check:
# - Section 6: Failure Analysis
# - Section 13: Failure Heatmap
```

---

## ðŸ“Š Dashboard Reference (Notebook 03)

### Basic Monitoring (Sections 1-5)
1. **Import** - Load libraries
2. **Config** - Set paths
3. **Watermarks** - Show processed files
4. **DQ Dashboard** - Stats + trend chart
5. **Alerts** - Failure notifications

### Advanced Analytics (Sections 6-10)
6. **Failure Analysis** - Root causes
7. **Performance** - Speed & distribution
8. **Historical** - Daily trends
9. **Freshness** - Staleness check
10. **Export** - JSON reports

### Interactive Dashboards (Sections 11-15) â­
11. **Main Dashboard** - KPIs + charts
12. **File Explorer** - Sunburst drill-down
13. **Failure Heatmap** - Visual root causes
14. **Timeline** - Processing activity
15. **Performance Analytics** - Cumulative metrics

---

## ðŸ”§ Configuration Quick Reference

### Notebook 01: Profiling
```python
DATA_PATH = "data/Samples_LH_Bronze_Aims_26_parquet"
OUTPUT_DIR = "dq_great_expectations/generated_configs"
NUM_WORKERS = 4         # Parallel processing
SAMPLE_SIZE = 100000    # Rows per file
```

### Notebook 02: Ingestion
```python
DATA_PATH = "data/Samples_LH_Bronze_Aims_26_parquet"
CONFIG_DIR = "dq_great_expectations/generated_configs"
STATE_DIR = "data/state"
```

### Notebook 03: Monitoring
```python
STATE_DIR = "data/state"
WATERMARK_FILE = STATE_DIR / "watermarks.json"
DQ_LOG_FILE = STATE_DIR / "dq_logs.jsonl"
```

---

## ðŸš¨ Troubleshooting

| Problem | Solution |
|---------|----------|
| **Import Error:** `No module named 'dq_framework'` | Install library: `pip install dq_great_expectations/dq_package_dist/*.whl` |
| **"No validation config found"** | Run Notebook 01 first |
| **All files "Already processed"** | Delete `watermarks.json` to reprocess |
| **Notebook 03 shows no data** | Run Notebook 02 first |
| **Plotly charts not showing** | Run: `%pip install plotly ipywidgets` |
| **Out of memory** | Reduce `SAMPLE_SIZE` or `NUM_WORKERS` |

---

## ðŸ“ˆ Key Metrics

| Metric | Current | Target |
|--------|---------|--------|
| **Pass Rate** | 64.7% | >95% |
| **Avg Score** | 98.8% | >98% |
| **Failed Files** | 24 | <5 |
| **Processed Files** | 44 | 68 |
| **Processing Time** | ~3 min | <5 min |

---

## ðŸŽ“ Best Practices

### âœ… Do
- Run Notebook 01 when schema changes
- Check Notebook 03 after every ingestion
- Review failure heatmap for patterns
- Export reports for stakeholders
- Commit YAML configs to git

### âŒ Don't
- Run Notebook 01 daily (unnecessary)
- Delete logs unless archiving
- Ignore persistent failures
- Skip validation to save time
- Modify configs without testing

---

## ðŸ“š Documentation

| Document | Purpose |
|----------|---------|
| **README_COMPLETE.md** | Complete reference guide |
| **ORCHESTRATION_GUIDE.md** | How to run pipeline |
| **GAPS_AND_IMPROVEMENTS.md** | Enhancement roadmap |
| **CRITICAL_ANALYSIS.md** | Architecture details |
| **THIS FILE** | Quick reference |

---

## ðŸ”— Useful Commands

### Check Environment
```bash
conda list | grep fabric
python -c "from dq_framework import DataProfiler; print('âœ… Ready!')"
```

### View Logs
```bash
cat data/state/dq_logs.jsonl | jq .
cat data/state/watermarks.json | jq .
```

### Count Files
```bash
ls data/Samples_LH_Bronze_Aims_26_parquet/*.parquet | wc -l
ls dq_great_expectations/generated_configs/*.yaml | wc -l
cat data/state/watermarks.json | jq '. | length'
```

### Archive Logs
```bash
tar -czf logs_backup_$(date +%Y%m%d).tar.gz data/state/*.jsonl
```

---

## ðŸŽ¯ Success Checklist

- [ ] Notebook 01 created 68 YAML files
- [ ] Notebook 02 processed files without errors
- [ ] Notebook 03 displayed 15 sections
- [ ] Interactive dashboards are clickable
- [ ] Watermarks.json has 44+ entries
- [ ] dq_logs.jsonl has 68+ entries
- [ ] Pass rate >95% (or failures explained)
- [ ] Rerunning Notebook 02 skips all files

---

## ðŸ’¡ Pro Tips

1. **Use Jupyter "Run All"** - Each notebook is designed to run top-to-bottom
2. **Watch the Progress** - Notebook 01 shows progress bar for each file
3. **Explore Interactively** - Click/hover on Plotly charts in Notebook 03
4. **Export for Presentations** - Use dq_report_*.json for stakeholder updates
5. **Test Locally First** - Always run complete pipeline locally before production

---

## ðŸŽ‰ Current Status

**Pipeline Version:** 2.0  
**Last Run:** December 6, 2025  
**Status:** âœ… Production Ready

**Results:**
- âœ… 68 configs generated
- âœ… 68 files validated
- âœ… 44 files passed (64.7%)
- âœ… 24 files flagged for review
- âœ… 98.8% average quality score
- âœ… 5 interactive dashboards working

---

**Print this card and keep it handy! ðŸ“Œ**
