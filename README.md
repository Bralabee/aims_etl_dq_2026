# AIMS Data Platform

![CI Status](https://github.com/Bralabee/aims_etl_dq_2026/actions/workflows/ci-cd.yml/badge.svg)
![Azure DevOps](https://dev.azure.com/{org}/AIMS-Data-Platform/_apis/build/status/aims-pipeline)
![Test Coverage](https://img.shields.io/badge/tests-74%2F74%20passing-brightgreen)
![DQ Pass Rate](https://img.shields.io/badge/DQ%20validation-73.5%25-yellow)
![Production Ready](https://img.shields.io/badge/production%20ready-90%25-green)

# AIMS Data Platform - Local Development Environment

**Version:** 1.3.1  
**Status:** Production Ready - Dual Platform Support (Local + MS Fabric)  
**Last Updated:** 2026-01-20

A comprehensive, governed data ingestion platform designed for SFTP-based file ingestion, data quality validation via Great Expectations, dual CLI/Notebook functionality, and seamless integration with Microsoft Fabric.

## ğŸ“Š Quick Stats

| Metric | Value |
|--------|-------|
| **Bronze Tables** | 68 |
| **DQ Configs Generated** | 68 |
| **Validation Pass Rate** | 73.5% (50/68) |
| **Average Quality Score** | 98.8% |
| **Test Suite** | 74/74 passing (100%) |
| **Notebooks Validated** | 9/9 passing (100%) |
| **Documentation** | 180+ pages |
| **CI/CD Pipelines** | Azure DevOps + GitHub Actions |
| **Platform Support** | Local + Microsoft Fabric |

## ğŸ†• What's New in v1.4.0

### Landing Zone Architecture
- **SFTP Integration** - Weekly file drops to landing zone
- **Auto-Archive** - Processed files archived with date stamps
- **Complete Overwrite** - No delta/append, fresh data each run
- **Notifications** - Teams webhook and email support

### Dual Platform Support
- **Platform Detection** - Auto-detects Local vs MS Fabric environment
- **Cross-Platform Ops** - \`PlatformFileOps\` class for file operations
- **Fabric API Compatible** - Uses \`mssparkutils.fs\` for lakehouse paths

## Key Features

- âœ… **Landing Zone Management** - SFTP drop â†’ archive flow with notifications
- âœ… **Complete Overwrite Strategy** - No residual data, fresh runs each time
- âœ… **Dual Platform** - Seamless Local â†” MS Fabric operation
- âœ… **Dual Functionality** - Complete CLI scripts AND interactive Jupyter notebooks
- âœ… **Incremental Loading** - Watermark-based incremental data ingestion
- âœ… **Data Quality** - Great Expectations validation (68 configs, 98.8% avg score)
- âœ… **Automated Profiling** - Auto-generates DQ configs via \`fabric_data_quality\`
- âœ… **Medallion Architecture** - Bronze â†’ Silver â†’ Gold layer transformation
- âœ… **CI/CD Integration** - Azure DevOps and GitHub Actions workflows
- âœ… **Governance** - Full audit trail with load history and watermarks
- âœ… **Production Ready** - 90% deployment ready with comprehensive testing

## ğŸ”„ Data Flow

\`\`\`mermaid
flowchart TD
    SFTP[SFTP Server] -->|Weekly fetch| LZ[landing/]
    LZ -->|Copy| BRONZE[Bronze/]
    BRONZE -->|Validate| SILVER[Silver/]
    LZ -->|Archive| ARCH[archive/YYYY-MM-DD/]
    SILVER -->|Transform| GOLD[Gold/]
\`\`\`

**Pipeline Phases:**
1. **Phase 0**: Landing â†’ Bronze (if files present)
2. **Phase 1**: Data Profiling (68 files, ~7.5s)
3. **Phase 2**: Validation & Silver Ingestion (50/68 passed)
4. **Phase 3**: Archive, Cleanup & Notify

## ğŸš€ Quick Start (5 Minutes)

\`\`\`bash
# 1. Navigate to project
cd /home/sanmi/Documents/HS2/HS2_PROJECTS_2025/1_AIMS_LOCAL_2026

# 2. Activate environment
conda activate aims_data_platform

# 3. Run full pipeline
python scripts/run_full_pipeline.py --skip-profiling

# Expected output:
# âœ… 50/68 passing (73.5%)
# âœ… Archive created
# âœ… Landing zone cleared
\`\`\`

### Pipeline Options

\`\`\`bash
# Full pipeline with all phases
python scripts/run_full_pipeline.py

# Skip profiling (faster)
python scripts/run_full_pipeline.py --skip-profiling

# Dry run (no archive/notifications)
python scripts/run_full_pipeline.py --dry-run

# Custom threshold
python scripts/run_full_pipeline.py --threshold 90.0

# Force Fabric mode (testing)
python scripts/run_full_pipeline.py --fabric

# Disable notifications
python scripts/run_full_pipeline.py --no-notify
\`\`\`

## ğŸ“ Project Structure

\`\`\`
AIMS_LOCAL/
â”œâ”€â”€ aims_data_platform/           # Core package
â”‚   â”œâ”€â”€ landing_zone_manager.py   # ğŸ†• Landing zone + archival
â”‚   â”œâ”€â”€ ingestion.py              # Data ingestion logic
â”‚   â”œâ”€â”€ config.py                 # Configuration management
â”‚   â””â”€â”€ watermark_manager.py      # Watermark tracking
â”œâ”€â”€ notebooks/                    # Jupyter notebooks (00-08)
â”‚   â”œâ”€â”€ lib/                      # Shared utilities
â”‚   â”‚   â”œâ”€â”€ platform_utils.py     # Platform detection
â”‚   â”‚   â”œâ”€â”€ storage.py            # StorageManager (Bronze/Silver/Gold)
â”‚   â”‚   â”œâ”€â”€ settings.py           # Centralized config
â”‚   â”‚   â””â”€â”€ logging_utils.py      # Logging setup
â”‚   â””â”€â”€ config/
â”‚       â””â”€â”€ notebook_settings.yaml
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_full_pipeline.py      # ğŸ†• Full pipeline orchestrator
â”‚   â””â”€â”€ run_validation_simple.py  # Simple validation
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ landing/                  # ğŸ†• SFTP drop zone
â”‚   â”œâ”€â”€ archive/                  # ğŸ†• Date-stamped archives
â”‚   â”œâ”€â”€ Samples_LH_Bronze_*/      # Bronze layer
â”‚   â”œâ”€â”€ Silver/                   # Silver layer (validated)
â”‚   â””â”€â”€ Gold/                     # Gold layer (analytics-ready)
â”œâ”€â”€ config/
â”‚   â””â”€â”€ data_quality/             # 68 DQ validation configs
â””â”€â”€ docs/                         # 180+ pages documentation
\`\`\`

## ğŸ”§ Landing Zone Management

### How It Works

1. **SFTP drops files** to \`data/landing/\`
2. **Pipeline discovers** files via \`list_landing_files()\`
3. **Files copied** to Bronze for processing
4. **Validation runs** with Great Expectations
5. **Valid data** written to Silver (complete overwrite)
6. **Original files archived** to \`archive/YYYY-MM-DD_run_xxx/\`
7. **Landing cleared** for next SFTP fetch
8. **Notifications sent** via Teams/Email

### Archive Contents

\`\`\`
archive/2026-01-19_run_20260119_152807/
â”œâ”€â”€ aims_assets.parquet           # Original file
â”œâ”€â”€ aims_attributes.parquet       # Original file
â”œâ”€â”€ _run_metadata.json            # Files list, errors, platform
â””â”€â”€ _run_summary.json             # DQ stats, pass rate
\`\`\`

### Configuration

\`\`\`python
from aims_data_platform import create_landing_zone_manager

manager = create_landing_zone_manager(
    teams_webhook_url="https://outlook.office.com/webhook/...",
    email_config={
        "smtp_server": "smtp.office365.com",
        "email_from": "data-platform@company.com",
        "email_to": ["team@company.com"]
    }
)
\`\`\`

## ğŸŒ Platform Support

### Auto-Detection

The platform automatically detects the runtime environment:

| Environment | Detection | Path Format |
|-------------|-----------|-------------|
| **Local** | Default | \`/home/user/project/data/\` |
| **MS Fabric** | \`/lakehouse/default/Files\` exists | \`/lakehouse/default/Files/\` or \`abfss://\` |

### Platform-Aware Operations

\`\`\`python
from aims_data_platform import PlatformFileOps, IS_FABRIC

# Auto-detected operations
ops = PlatformFileOps()
ops.copy_file(src, dst)      # Uses mssparkutils on Fabric
ops.move_file(src, dst)      # Uses mssparkutils on Fabric
ops.remove_directory(path)   # Uses mssparkutils.fs.rm on Fabric
\`\`\`

### Fabric Deployment

See [docs/02_Fabric_Migration/](docs/02_Fabric_Migration/) for:
- Notebook upload instructions
- Lakehouse configuration
- mssparkutils API reference
- Environment variable setup

## ğŸ“š Documentation

### ğŸ¯ Start Here
- **[QUICK_START_GUIDE.md](QUICK_START_GUIDE.md)** - Get started in 5 minutes
- **[CHANGELOG.md](CHANGELOG.md)** - Version history and changes
- **[docs/pipeline_flow.md](docs/pipeline_flow.md)** - Visual pipeline diagram

### ğŸ”§ Implementation Guides
- **[docs/03_Implementation_Guides/LANDING_ZONE_MANAGEMENT.md](docs/03_Implementation_Guides/LANDING_ZONE_MANAGEMENT.md)** - Landing zone setup
- **[docs/03_Implementation_Guides/ORCHESTRATION_GUIDE.md](docs/03_Implementation_Guides/ORCHESTRATION_GUIDE.md)** - Pipeline orchestration
- **[docs/02_Fabric_Migration/FABRIC_DEPLOYMENT_GUIDE.md](docs/02_Fabric_Migration/FABRIC_DEPLOYMENT_GUIDE.md)** - MS Fabric deployment

### ğŸ“– Reference
- **[notebooks/README.md](notebooks/README.md)** - Notebook documentation
- **[notebooks/lib/README.md](notebooks/lib/README.md)** - Shared library reference

## ğŸ§ª Testing

\`\`\`bash
# Run all tests
pytest tests/ -v

# Run with coverage
pytest tests/ --cov=aims_data_platform --cov-report=html

# Expected: 74/74 passing (100%)
\`\`\`

## ğŸ“ˆ Metrics

| Metric | Current | Target |
|--------|---------|--------|
| Test Coverage | 100% (74/74) | â‰¥95% |
| DQ Pass Rate | 73.5% | â‰¥85% |
| Avg Quality Score | 98.8% | â‰¥95% |
| Pipeline Duration | ~60s | <120s |
| Documentation | 180+ pages | Complete |

## ğŸ” Security

- No credentials stored in code
- Environment variables for sensitive config
- Teams webhook URLs via environment
- SMTP credentials via secure config

## ğŸ¤ Contributing

See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

## ğŸ“„ License

Proprietary - HS2

## ğŸ†˜ Support

For issues or questions, contact the HS2 Data Team.

---

**Built with â¤ï¸ for HS2 Data Platform Team**
