# AIMS Data Platform - File Structure

```
AIMS_LOCAL/
â”‚
â”œâ”€â”€ ğŸ“‹ Configuration Files
â”‚   â”œâ”€â”€ environment.yml              # Conda environment with Python 3.10
â”‚   â”œâ”€â”€ requirements.txt             # All Python dependencies
â”‚   â”œâ”€â”€ .env.example                 # Template for environment variables
â”‚   â”œâ”€â”€ Makefile                     # Convenient command shortcuts
â”‚   â””â”€â”€ setup.sh                     # Automated setup script (executable)
â”‚
â”œâ”€â”€ ğŸ“š Documentation
â”‚   â”œâ”€â”€ README.md                    # Main usage guide
â”‚   â”œâ”€â”€ IMPLEMENTATION_GUIDE.md      # Detailed implementation guide
â”‚   â””â”€â”€ FILE_STRUCTURE.md            # This file
â”‚
â”œâ”€â”€ ğŸ Python Package (src/)
â”‚   â”œâ”€â”€ __init__.py                  # Package initialization
â”‚   â”œâ”€â”€ config.py                    # Configuration & environment variables
â”‚   â”œâ”€â”€ watermark_manager.py        # SQLite watermark tracking
â”‚   â”œâ”€â”€ ingestion.py                 # Data ingestion & parquet repair
â”‚   â”œâ”€â”€ data_quality.py              # Great Expectations integration
â”‚   â””â”€â”€ cli.py                       # Rich CLI interface (Typer)
â”‚
â”œâ”€â”€ ğŸ“Š Data Directories (created on init)
â”‚   â”œâ”€â”€ data/                        # Target directory for ingested data
â”‚   â”‚   â””â”€â”€ repaired/                # Repaired parquet files
â”‚   â””â”€â”€ great_expectations/          # GE configuration and results
â”‚
â”œâ”€â”€ ğŸ—„ï¸ Database (created on init)
â”‚   â””â”€â”€ watermarks.db                # SQLite: watermarks + load history
â”‚
â””â”€â”€ ğŸ““ Notebooks (existing)
    â”œâ”€â”€ aims_data_simple.ipynb       # Simple standalone notebook
    â”œâ”€â”€ aims_data_exploration.ipynb  # Data exploration notebook
    â””â”€â”€ testing_aims_parquet_files.ipynb  # Testing notebook
```

## File Descriptions

### Configuration Layer

**environment.yml**
- Conda environment specification
- Python 3.10 base
- References requirements.txt for pip packages
- Includes conda-forge channel

**requirements.txt**
- Core: pandas, pyarrow, fastparquet
- Quality: great-expectations
- Database: sqlalchemy
- Azure: azure-identity, azure-storage-*
- CLI: typer, rich, click
- Utils: python-dotenv, pyyaml, structlog

**.env.example**
- Template for configuration
- Source/target paths
- Parquet engine settings
- Watermark column defaults
- Logging configuration
- Azure/Fabric credentials (optional)

**Makefile**
- `make setup` - Run setup.sh
- `make init` - Initialize platform
- `make repair` - Fix corrupted files
- `make ingest-all` - Ingest all sources
- `make watermarks` - Show current watermarks
- `make history` - Show load history
- `make clean` - Remove generated files

**setup.sh**
- Automated environment setup
- Detects conda vs pip
- Creates .env from template
- Shows next steps

### Python Package (src/)

**config.py** (Config Management)
```python
- Config class with all settings
- Environment variable loading
- Path management
- Directory creation
- Configuration as dictionary
```

**watermark_manager.py** (Watermark Tracking)
```python
- WatermarkManager class
- SQLite database operations
- get_watermark(source_name)
- update_watermark(source, value, count)
- start_load(source) / complete_load(load_id)
- list_watermarks() â†’ DataFrame
- get_load_history() â†’ DataFrame
```

**ingestion.py** (Data Ingestion)
```python
- DataIngester class
- repair_parquet_file(source, output)
- repair_directory(source_dir, output_dir)
- ingest_incremental(...) â†’ results dict
- validate_source_files(dir) â†’ validation results
- Multi-engine support (fastparquet/pyarrow)
- Watermark filtering
- Error handling & logging
```

**data_quality.py** (Quality Validation)
```python
- DataQualityValidator class
- initialize_context() - GE setup
- create_datasource(name, base_dir)
- create_expectation_suite(name, schema)
- validate_dataframe(df, suite) â†’ results
- validate_file(path, suite) â†’ results
- generate_data_profile(df) â†’ profile dict
- Basic validations without full GE
```

**cli.py** (Command-Line Interface)
```python
Commands:
- init - Initialize platform
- repair - Fix corrupted parquet files
- validate-source - Validate source files
- ingest - Incremental data ingestion
- list-watermarks - Show all watermarks
- load-history - Show load history

Features:
- Rich terminal output
- Tables and formatting
- Error handling
- Help system
```

### Data Flow

```
1. Source Files (corrupted)
   â†“
2. repair â†’ Repaired Files (data/repaired/)
   â†“
3. ingest â†’ Incremental Load â†’ Target Files (data/)
   â†“
4. validate â†’ Quality Check â†’ Results
   â†“
5. watermark â†’ Update Tracking â†’ watermarks.db
```

### Database Schema

**watermarks table:**
```sql
- source_name (TEXT, PRIMARY KEY)
- watermark_value (TEXT)
- watermark_type (TEXT)
- last_updated (TIMESTAMP)
- records_processed (INTEGER)
- metadata (TEXT)
```

**load_history table:**
```sql
- id (INTEGER, PRIMARY KEY)
- source_name (TEXT)
- load_start (TIMESTAMP)
- load_end (TIMESTAMP)
- records_processed (INTEGER)
- status (TEXT)
- error_message (TEXT)
- watermark_value (TEXT)
```

## Usage Patterns

### Pattern 1: First-Time Setup
```bash
./setup.sh
conda activate aims_data_platform
make init
make repair
make ingest-all
```

### Pattern 2: Incremental Updates
```bash
conda activate aims_data_platform
make ingest-all
make watermarks
```

### Pattern 3: Monitoring
```bash
make watermarks    # Check current state
make history       # Review past loads
```

### Pattern 4: Python API
```python
from src import WatermarkManager, DataIngester
watermark_mgr = WatermarkManager("watermarks.db")
ingester = DataIngester(watermark_mgr)
# ... use API
```

## Dependencies Overview

**Core Data:**
- pandas: DataFrame operations
- pyarrow: Modern parquet engine
- fastparquet: Legacy parquet compatibility

**Quality:**
- great-expectations: Data validation framework

**Storage:**
- sqlalchemy: Database abstraction
- sqlite3: Built-in (no install needed)

**Azure (Optional):**
- azure-identity: Authentication
- azure-storage-blob: Blob storage
- azure-storage-file-datalake: Data Lake Gen2

**CLI:**
- typer: CLI framework
- rich: Terminal formatting
- click: Command parsing

**Utils:**
- python-dotenv: Environment variables
- pyyaml: YAML parsing
- structlog: Structured logging

## Extension Points

1. **Add New Data Sources**: Update CLI and add to Makefile
2. **Custom Validations**: Extend data_quality.py
3. **New Storage Backends**: Extend ingestion.py
4. **Fabric Integration**: Implement in separate module
5. **Scheduling**: Use cron/Airflow with CLI commands

---

## Notebook Architecture Layer

**Version 1.3.0** introduces a unified notebook utilities layer that provides consistent behavior across local and Fabric environments.

### Notebook Utilities (`notebooks/lib/`)

```
notebooks/lib/
â”œâ”€â”€ __init__.py                  # Package initialization
â”œâ”€â”€ platform_utils.py            # Platform detection & cross-platform I/O
â”œâ”€â”€ storage.py                   # StorageManager (Bronze/Silver/Gold)
â”œâ”€â”€ settings.py                  # Settings singleton with YAML config
â”œâ”€â”€ logging_utils.py             # Logging setup & progress tracking
â””â”€â”€ config.py                    # Additional config helpers
```

#### Module Descriptions

**platform_utils.py** (Platform Detection)
```python
- IS_FABRIC constant (bool)
- _detect_fabric_environment()
- safe_import_mssparkutils()
- get_data_paths() â†’ Dict[str, Path]
- copy_file(src, dst) - platform-aware copy
- list_files(path) â†’ List[Path]
- Cross-platform path resolution
```

**storage.py** (Storage Abstraction)
```python
- StorageManager class
- read_from_bronze(table, sample_size) â†’ DataFrame
- read_from_silver(table) â†’ DataFrame
- write_to_silver(df, table, partition_cols) â†’ Path
- write_to_gold(df, table) â†’ Path
- write_validated_data(df, table, validation_result) â†’ Path
- quarantine_data(df, table, reason) â†’ Path
- get_storage_format() â†’ "parquet" | "delta"
- Schema evolution support
- Partition inference
```

**settings.py** (Configuration Management)
```python
- Settings dataclass (singleton)
- Settings.load(environment) â†’ Settings
- Auto-detection: local | fabric_dev | fabric_prod
- YAML config loading from notebook_settings.yaml
- Environment variable overrides
- Type-safe path accessors:
  - bronze_dir, silver_dir, gold_dir
  - config_dir, state_dir, reports_dir
- Data quality accessors:
  - dq_threshold, null_tolerance
  - get_dq_threshold(severity) â†’ float
```

**logging_utils.py** (Logging & Progress)
```python
- setup_notebook_logger(name, level) â†’ Logger
- @log_phase_start(phase_name) decorator
- ProgressTracker context manager
- Consistent JSON/text formatting
- File rotation support
```

### Notebook Configuration (`notebooks/config/`)

```
notebooks/config/
â”œâ”€â”€ notebook_settings.yaml       # Main configuration file
â”œâ”€â”€ data_quality/                # Per-table DQ configs
â””â”€â”€ validation_results/          # Validation output storage
```

**notebook_settings.yaml** Structure:
```yaml
environments:           # Environment-specific settings
  local: {...}
  fabric_dev: {...}
  fabric_prod: {...}

data_quality:           # DQ thresholds and tolerances
  threshold: 85.0
  severity_levels: {...}

paths:                  # Medallion architecture paths
  bronze: data/...
  silver: data/Silver
  gold: data/Gold

pipeline:               # Pipeline behavior settings
  phases: {...}
  continue_on_error: false

storage:                # Storage format settings
  parquet_engine: pyarrow
  compression: snappy

table_overrides: {}     # Per-table customization
```

### Configuration Priority

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Environment Variables (highest)  â”‚
â”‚    AIMS_DQ_THRESHOLD=90.0           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2. .env File Values                 â”‚
â”‚    LOCAL_OVERRIDE=value             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 3. YAML Configuration               â”‚
â”‚    notebook_settings.yaml           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 4. Built-in Defaults (lowest)       â”‚
â”‚    Hardcoded fallbacks              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow with Notebook Utilities

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Notebook Layer                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  00_Orchestration â†’ 01_Profiling â†’ 02_Ingestion â†’ 03_Monitoring  â”‚
â”‚                           â†“                                       â”‚
â”‚  04_Schema_Recon â†’ 05_Insights â†’ 06_BI â†’ 07_DQ_Matrix â†’ 08_BI    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   lib/ Utilities   â”‚
                    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                    â”‚ â€¢ platform_utils  â”‚ â† Platform Detection
                    â”‚ â€¢ storage         â”‚ â† StorageManager
                    â”‚ â€¢ settings        â”‚ â† Settings Singleton
                    â”‚ â€¢ logging_utils   â”‚ â† Logging Setup
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  config/ Settings  â”‚
                    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                    â”‚ notebook_settings â”‚
                    â”‚      .yaml        â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â–¼                  â–¼                  â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  Local   â”‚      â”‚  Fabric  â”‚       â”‚  Fabric  â”‚
     â”‚   Dev    â”‚      â”‚   Dev    â”‚       â”‚   Prod   â”‚
     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
     â”‚ Parquet  â”‚      â”‚  Delta   â”‚       â”‚  Delta   â”‚
     â”‚ 4 workersâ”‚      â”‚ 8 workersâ”‚       â”‚16 workersâ”‚
     â”‚  DEBUG   â”‚      â”‚   INFO   â”‚       â”‚ WARNING  â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

**Total Files Created**: 11 + 5 (notebook utilities)
**Lines of Code**: ~4,500+
**Test Coverage**: Ready for pytest integration
**Production Ready**: Yes (with proper .env configuration)
