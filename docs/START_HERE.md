# ğŸ‰ AIMS Data Profiling - Ready to Use!

## What Was Done

I've configured your AIMS_LOCAL project to use the `fabric_data_quality` framework for profiling parquet files **without duplicating any code**.

### âœ… Solution: Editable Package Installation

The `fabric_data_quality` project is installed as a **reusable Python package** that AIMS_LOCAL (and any other project) can import and use.

## ğŸ“ Files Created

| File | Purpose |
|------|---------|
| `setup_aims_profiling.sh` | One-time setup script (installs framework) |
| `profile_aims_parquet.py` | CLI tool for profiling AIMS parquet files |
| `notebooks/03_Profile_AIMS_Data.ipynb` | Interactive profiling notebook |
| `test_profiling_integration.py` | Test script to verify installation |
| `README_PROFILING.md` | Complete documentation |
| `PROFILING_QUICK_START.md` | Quick reference guide |
| `SETUP_SUMMARY.md` | Detailed setup summary |
| `ARCHITECTURE_DIAGRAM.txt` | Visual architecture diagram |

## ğŸš€ Quick Start (3 Steps)

### 1ï¸âƒ£ Run Setup (One-Time)

```bash
cd /home/sanmi/Documents/HS2/HS2_PROJECTS_2025/AIMS_LOCAL
bash setup_aims_profiling.sh
```

This will:
- Install `fabric_data_quality` as an editable package
- Create necessary directories
- Validate the installation
- Test with your actual data

### 2ï¸âƒ£ Verify Installation (Optional)

```bash
python test_profiling_integration.py
```

### 3ï¸âƒ£ Profile Your Data

**Option A: Command Line (Quick)**
```bash
# Profile all parquet files
python profile_aims_parquet.py
```

**Option B: Jupyter Notebook (Interactive)**
```bash
jupyter notebook notebooks/03_Profile_AIMS_Data.ipynb
```

## ğŸ“Š What You'll Get

After profiling, you'll have validation config files in `config/data_quality/`:

```
config/data_quality/
â”œâ”€â”€ aims_activitydates_validation.yml
â”œâ”€â”€ aims_assetattributes_validation.yml
â”œâ”€â”€ aims_assetclassattributes_validation.yml
â””â”€â”€ aims_assetclasschangelogs_validation.yml
```

Each YAML file contains:
- Column names and data types
- Null value checks
- Statistical expectations
- Unique value validations
- Customizable business rules

## ğŸ”§ Using Generated Configs

```python
from dq_framework import ConfigLoader, DataValidator
import pandas as pd

# Load your data
df = pd.read_parquet('data/aims_activitydates.parquet')

# Load validation config
config = ConfigLoader.load('config/data_quality/aims_activitydates_validation.yml')

# Validate
validator = DataValidator(config)
results = validator.validate(df)

if results['success']:
    print("âœ… Data quality checks passed")
else:
    print("âŒ Issues found:", results['failures'])
```

## ğŸ’¡ Key Advantages

### âœ… No Code Duplication
- `fabric_data_quality` lives in one place: `../fabric_data_quality`
- AIMS_LOCAL imports it: `from dq_framework import DataProfiler`
- Other projects can do the same

### âœ… Easy Updates
- Update `fabric_data_quality` once
- Changes automatically available to all projects
- No need to sync code

### âœ… Clean Architecture
```
fabric_data_quality/          â† Framework (installed as package)
    â””â”€â”€ dq_framework/         â† Imported by projects

AIMS_LOCAL/                   â† Your project
    â”œâ”€â”€ profile_aims_parquet.py   â† Uses dq_framework
    â””â”€â”€ config/data_quality/      â† Generated configs
```

## ğŸ“– Documentation

- **Quick Start**: `PROFILING_QUICK_START.md`
- **Full Guide**: `README_PROFILING.md`
- **Setup Details**: `SETUP_SUMMARY.md`
- **Architecture**: `ARCHITECTURE_DIAGRAM.txt`
- **Framework Docs**: `../fabric_data_quality/README.md`

## ğŸ¯ Common Commands

```bash
# Profile all files
python profile_aims_parquet.py

# Profile specific file
python profile_aims_parquet.py --file aims_activitydates.parquet

# View profile only (no config generation)
python profile_aims_parquet.py --profile-only

# Custom settings
python profile_aims_parquet.py --null-tolerance 5.0 --severity high

# Interactive notebook
jupyter notebook notebooks/03_Profile_AIMS_Data.ipynb

# Test integration
python test_profiling_integration.py
```

## ğŸ”„ Workflow

```
Setup â†’ Profile â†’ Customize â†’ Integrate â†’ Validate
  â†“        â†“          â†“           â†“          â†“
Run      Generate   Edit YAML   Add to     Run in
setup    configs    files       pipeline   production
```

## ğŸ¤ Reusable Across Projects

Same approach works for:
- `full_stack_hss`
- `ACA_COMMERCIAL`
- `fabric_data_quality` (self-testing)
- Any other HS2 project

Just run the same setup pattern:
1. Install framework: `cd ../fabric_data_quality && pip install -e .`
2. Import in code: `from dq_framework import DataProfiler`
3. Use for profiling and validation

## ğŸ› Troubleshooting

**Import Error**: `ModuleNotFoundError: No module named 'dq_framework'`
```bash
# Re-run setup
bash setup_aims_profiling.sh

# Or manually
cd ../fabric_data_quality
pip install -e .
```

**No Data Found**: The profiler will still work with other data directories
```bash
python profile_aims_parquet.py --data-dir /path/to/other/data
```

## ğŸ“ Next Steps

1. âœ… Run `bash setup_aims_profiling.sh`
2. âœ… Profile your data: `python profile_aims_parquet.py`
3. âœ… Review generated configs in `config/data_quality/`
4. âœ… Customize validation rules in YAML files
5. âœ… Integrate with your AIMS data pipeline
6. âœ… Set up automated validation

---

**Status**: âœ… Ready to use  
**Framework**: fabric_data_quality v1.0.0  
**Created**: December 2025

**Start now**: `bash setup_aims_profiling.sh`
