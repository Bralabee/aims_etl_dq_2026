{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a61e69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìä DATA QUALITY DASHBOARD\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'DQ_LOG_FILE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müìä DATA QUALITY DASHBOARD\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m60\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mDQ_LOG_FILE\u001b[49m\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Read all DQ logs\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     logs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(DQ_LOG_FILE, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DQ_LOG_FILE' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 4: Data Quality Monitoring Dashboard\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä DATA QUALITY DASHBOARD\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if DQ_LOG_FILE.exists():\n",
    "    # Read all DQ logs\n",
    "    logs = []\n",
    "    with open(DQ_LOG_FILE, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                logs.append(json.loads(line))\n",
    "    \n",
    "    if logs:\n",
    "        df_logs = pd.DataFrame(logs)\n",
    "        df_logs['timestamp'] = pd.to_datetime(df_logs['timestamp'])\n",
    "        \n",
    "        # --- HIGH-LEVEL METRICS ---\n",
    "        total_runs = len(df_logs)\n",
    "        passed_runs = len(df_logs[df_logs['status'] == 'PASSED'])\n",
    "        failed_runs = len(df_logs[df_logs['status'] == 'FAILED'])\n",
    "        error_runs = len(df_logs[df_logs['status'] == 'ERROR'])\n",
    "        avg_score = df_logs['score'].mean()\n",
    "        \n",
    "        print(f\"\\nüìà Summary Statistics:\")\n",
    "        print(f\"   Total Validations: {total_runs}\")\n",
    "        print(f\"   ‚úÖ Passed: {passed_runs} ({passed_runs/total_runs*100:.1f}%)\")\n",
    "        print(f\"   ‚ùå Failed: {failed_runs} ({failed_runs/total_runs*100:.1f}%)\")\n",
    "        print(f\"   üí• Errors: {error_runs}\")\n",
    "        print(f\"   üìä Average Quality Score: {avg_score:.1f}%\")\n",
    "        \n",
    "        # --- RECENT ACTIVITY TABLE ---\n",
    "        print(\"\\nüìã Recent Validation Runs:\")\n",
    "        display_cols = ['timestamp', 'file', 'status', 'score']\n",
    "        display(df_logs[display_cols].sort_values('timestamp', ascending=False).head(15))\n",
    "        \n",
    "        # --- TREND CHART ---\n",
    "        if len(df_logs) > 1:\n",
    "            print(\"\\nüìà Quality Score Trend:\")\n",
    "            plt.figure(figsize=(12, 4))\n",
    "            df_plot = df_logs.sort_values('timestamp')\n",
    "            plt.plot(df_plot['timestamp'], df_plot['score'], marker='o', linestyle='-', linewidth=2)\n",
    "            plt.axhline(y=100, color='g', linestyle='--', alpha=0.3, label='Perfect Score')\n",
    "            plt.axhline(y=90, color='orange', linestyle='--', alpha=0.3, label='Warning Threshold')\n",
    "            plt.title('Data Quality Score Over Time')\n",
    "            plt.xlabel('Timestamp')\n",
    "            plt.ylabel('Quality Score (%)')\n",
    "            plt.ylim(0, 105)\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        # --- FAILURES BREAKDOWN ---\n",
    "        if failed_runs > 0:\n",
    "            print(f\"\\n‚ùå Failed Files ({failed_runs} total):\")\n",
    "            failed_df = df_logs[df_logs['status'] == 'FAILED'][['timestamp', 'file', 'score']].copy()\n",
    "            display(failed_df.sort_values('score'))\n",
    "        \n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  DQ log file exists but is empty.\")\n",
    "else:\n",
    "    print(\"\\n‚ùå DQ log file not found. Run Notebook 02 (Ingestion) first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f7d7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Alerting System (Simulation)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üö® ACTIVE ALERTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if 'df_logs' in locals() and not df_logs.empty:\n",
    "    # Filter for failures\n",
    "    failures = df_logs[df_logs['status'] == 'FAILED'].copy()\n",
    "    \n",
    "    if not failures.empty:\n",
    "        print(f\"\\n‚ö†Ô∏è  Found {len(failures)} failed runs requiring attention.\\n\")\n",
    "        \n",
    "        for idx, (_, row) in enumerate(failures.iterrows(), 1):\n",
    "            # Construct Alert Payload (for Slack/Teams/PagerDuty integration)\n",
    "            alert_payload = {\n",
    "                \"alert_type\": \"DataQualityFailure\",\n",
    "                \"severity\": \"High\" if row['score'] < 70 else \"Medium\",\n",
    "                \"source\": \"AIMS_Ingestion_Pipeline\",\n",
    "                \"timestamp\": row['timestamp'].isoformat(),\n",
    "                \"file\": row['file'],\n",
    "                \"score\": row['score'],\n",
    "                \"failed_checks\": row.get('details', {}).get('failed_count', 'Unknown'),\n",
    "                \"action_required\": \"Review quarantined file and fix data quality issues\"\n",
    "            }\n",
    "            \n",
    "            print(f\"üî¥ ALERT #{idx}: {row['file']}\")\n",
    "            print(f\"   Severity: {alert_payload['severity']}\")\n",
    "            print(f\"   Score: {row['score']:.1f}%\")\n",
    "            print(f\"   Time: {row['timestamp']}\")\n",
    "            print(f\"   Payload: {json.dumps(alert_payload, indent=2)}\")\n",
    "            print(\"-\" * 60)\n",
    "    else:\n",
    "        print(\"\\n‚úÖ No active alerts. All validations passed!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No data available for alert analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e82f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: View Watermark Status\n",
    "print(\"=\"*60)\n",
    "print(\"üìã WATERMARK STATUS - Processed Files\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if WATERMARK_FILE.exists():\n",
    "    with open(WATERMARK_FILE, 'r') as f:\n",
    "        watermarks = json.load(f)\n",
    "    \n",
    "    if watermarks:\n",
    "        df_watermarks = pd.DataFrame(\n",
    "            list(watermarks.items()), \n",
    "            columns=['Filename', 'Processed_Timestamp']\n",
    "        )\n",
    "        df_watermarks['Processed_Timestamp'] = pd.to_datetime(df_watermarks['Processed_Timestamp'])\n",
    "        df_watermarks = df_watermarks.sort_values('Processed_Timestamp', ascending=False)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Total Processed Files: {len(df_watermarks)}\")\n",
    "        print(\"\\nMost Recent Ingestions:\")\n",
    "        display(df_watermarks.head(10))\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  No files have been processed yet.\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Watermark file not found. Run Notebook 02 (Ingestion) first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374df91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Configuration - Point to Local State Files\n",
    "BASE_DIR = Path(\"/home/sanmi/Documents/HS2/HS2_PROJECTS_2025/AIMS_LOCAL\")\n",
    "STATE_DIR = BASE_DIR / \"data/state\"\n",
    "\n",
    "WATERMARK_FILE = STATE_DIR / \"watermarks.json\"\n",
    "DQ_LOG_FILE = STATE_DIR / \"dq_logs.jsonl\"\n",
    "\n",
    "print(\"‚úÖ Monitoring Configuration:\")\n",
    "print(f\"   State Directory: {STATE_DIR}\")\n",
    "print(f\"   Watermarks: {WATERMARK_FILE}\")\n",
    "print(f\"   DQ Logs: {DQ_LOG_FILE}\")\n",
    "print(f\"   Files exist: Watermarks={WATERMARK_FILE.exists()}, Logs={DQ_LOG_FILE.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06085b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import Libraries\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14f300a",
   "metadata": {},
   "source": [
    "# AIMS Data Quality Monitoring Dashboard\n",
    "\n",
    "This notebook provides monitoring and observability for the AIMS Data Platform.\n",
    "\n",
    "## Purpose\n",
    "- View watermark status (which files have been processed)\n",
    "- Review Data Quality validation results\n",
    "- Analyze DQ trends and patterns\n",
    "- Generate alerts for failures\n",
    "\n",
    "## Local Execution\n",
    "Reads logs from `data/state/` directory generated by Notebook 02"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fabric-dq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
