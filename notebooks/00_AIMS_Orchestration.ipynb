{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "248078a2",
   "metadata": {},
   "source": [
    "#  AIMS Data Platform - Orchestration Pipeline\n",
    "\n",
    "## Pipeline Architecture Overview\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502                        EXTERNAL DATA SOURCE                                  \u2502\n",
    "\u2502                    (SFTP Server / Manual Upload)                            \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                                    \u2502\n",
    "                                    \u25bc\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502  PHASE 0: LANDING ZONE \u2192 BRONZE (Full Refresh)                              \u2502\n",
    "\u2502  \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550                              \u2502\n",
    "\u2502                                                                             \u2502\n",
    "\u2502  \ud83d\udcc1 /landing/                                                               \u2502\n",
    "\u2502      \u2514\u2500\u2500 *.parquet (raw files arrive here via SFTP)                        \u2502\n",
    "\u2502                                                                             \u2502\n",
    "\u2502  Actions:                                                                   \u2502\n",
    "\u2502  1. Scan landing zone for new parquet files                                \u2502\n",
    "\u2502  2. CLEAR Bronze layer (remove old files)                                  \u2502\n",
    "\u2502  3. COPY files from landing \u2192 Bronze                                       \u2502\n",
    "\u2502  4. Track filenames in LANDING_FILES_TO_ARCHIVE list                       \u2502\n",
    "\u2502                                                                             \u2502\n",
    "\u2502  \u26a0\ufe0f IMPORTANT: Landing is PRESERVED as safety net until Phase 4            \u2502\n",
    "\u2502     If pipeline fails, landing files remain for retry                       \u2502\n",
    "\u2502                                                                             \u2502\n",
    "\u2502  Code: copy_file_fabric(), clear_directory()                               \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                                    \u2502\n",
    "                                    \u25bc\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502  PHASE 1: DATA PROFILING                                                    \u2502\n",
    "\u2502  \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550                                                    \u2502\n",
    "\u2502                                                                             \u2502\n",
    "\u2502  \ud83d\udcc2 Input:  /Bronze/*.parquet                                               \u2502\n",
    "\u2502  \ud83d\udcc2 Output: /config/data_quality/*_validation.yml                          \u2502\n",
    "\u2502                                                                             \u2502\n",
    "\u2502  Actions:                                                                   \u2502\n",
    "\u2502  1. Scan Bronze layer for all parquet files                                \u2502\n",
    "\u2502  2. For each file (parallel workers):                                       \u2502\n",
    "\u2502     \u2022 Load sample (100K rows default)                                       \u2502\n",
    "\u2502     \u2022 Profile columns (types, nulls, unique values, patterns)              \u2502\n",
    "\u2502     \u2022 Generate Great Expectations suite                                     \u2502\n",
    "\u2502     \u2022 Write YAML validation config                                         \u2502\n",
    "\u2502                                                                             \u2502\n",
    "\u2502  Code: BatchProfiler.run_parallel_profiling()                              \u2502\n",
    "\u2502                                                                             \u2502\n",
    "\u2502  Output Example (table_validation.yml):                                    \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                  \u2502\n",
    "\u2502  \u2502 expectations:                          \u2502                                  \u2502\n",
    "\u2502  \u2502   - expect_column_to_exist: id        \u2502                                  \u2502\n",
    "\u2502  \u2502   - expect_column_values_to_not_be_null: id                              \u2502\n",
    "\u2502  \u2502   - expect_column_values_to_be_unique: id                                \u2502\n",
    "\u2502  \u2502   - expect_column_values_to_be_of_type: date_col, datetime              \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                  \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                                    \u2502\n",
    "                                    \u25bc\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502  PHASE 2: DATA VALIDATION & INGESTION                                       \u2502\n",
    "\u2502  \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550                                       \u2502\n",
    "\u2502                                                                             \u2502\n",
    "\u2502  \ud83d\udcc2 Input:  /Bronze/*.parquet + /config/data_quality/*.yml                 \u2502\n",
    "\u2502  \ud83d\udcc2 Output: /Silver/*.parquet                                               \u2502\n",
    "\u2502                                                                             \u2502\n",
    "\u2502  Actions:                                                                   \u2502\n",
    "\u2502  1. CLEAR Silver layer (fresh data each run)                               \u2502\n",
    "\u2502  2. For each Bronze file:                                                   \u2502\n",
    "\u2502     \u2022 Load validation config (YAML)                                        \u2502\n",
    "\u2502     \u2022 Load parquet data                                                     \u2502\n",
    "\u2502     \u2022 Run Great Expectations validation                                     \u2502\n",
    "\u2502     \u2022 Calculate success_rate (% expectations passed)                       \u2502\n",
    "\u2502                                                                             \u2502\n",
    "\u2502  Decision:                                                                  \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n",
    "\u2502  \u2502  IF success_rate >= threshold (default 85%):                        \u2502   \u2502\n",
    "\u2502  \u2502     \u2705 PASSED \u2192 Write to Silver layer                               \u2502   \u2502\n",
    "\u2502  \u2502  ELSE:                                                              \u2502   \u2502\n",
    "\u2502  \u2502     \u274c FAILED \u2192 Skip ingestion                                      \u2502   \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n",
    "\u2502                                                                             \u2502\n",
    "\u2502  Code: DataQualityValidator.validate() \u2192 df.to_parquet(Silver/)            \u2502\n",
    "\u2502  Results: validation_results.json                                          \u2502\n",
    "\u2502                                                                             \u2502\n",
    "\u2502  \u26a0\ufe0f Silver is COMPLETE OVERWRITE each run (not append/merge)               \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                                    \u2502\n",
    "                                    \u25bc\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502  PHASE 3: DATA QUALITY MONITORING                                           \u2502\n",
    "\u2502  \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550                                            \u2502\n",
    "\u2502                                                                             \u2502\n",
    "\u2502  \ud83d\udcc2 Input:  /config/validation_results/validation_results.json             \u2502\n",
    "\u2502  \ud83d\udcc2 Output: Console metrics, execution log                                  \u2502\n",
    "\u2502                                                                             \u2502\n",
    "\u2502  Actions:                                                                   \u2502\n",
    "\u2502  \u2022 Load validation results JSON                                             \u2502\n",
    "\u2502  \u2022 Calculate aggregate metrics:                                             \u2502\n",
    "\u2502    - Average Quality Score                                                  \u2502\n",
    "\u2502    - Pass Rate (% tables passed)                                            \u2502\n",
    "\u2502    - Tables monitored count                                                 \u2502\n",
    "\u2502  \u2022 Generate summary DataFrame                                               \u2502\n",
    "\u2502                                                                             \u2502\n",
    "\u2502  Output Metrics:                                                            \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n",
    "\u2502  \u2502  Table          \u2502 Success % \u2502 Status \u2502 Evaluated \u2502 Successful       \u2502   \u2502\n",
    "\u2502  \u2502  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502 \u2500\u2500\u2500\u2500\u2500\u2500 \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500       \u2502   \u2502\n",
    "\u2502  \u2502  customers      \u2502   95.2    \u2502 Passed \u2502    12     \u2502    11           \u2502   \u2502\n",
    "\u2502  \u2502  orders         \u2502   87.5    \u2502 Passed \u2502    16     \u2502    14           \u2502   \u2502\n",
    "\u2502  \u2502  products       \u2502   72.0    \u2502 Failed \u2502    10     \u2502     7           \u2502   \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n",
    "\u2502                                                                             \u2502\n",
    "\u2502  Pipeline Success Check: success_rate >= 80% \u2192 proceed to Phase 4          \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                                    \u2502\n",
    "                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "                    \u2502                               \u2502\n",
    "              success >= 80%                  success < 80%\n",
    "                    \u2502                               \u2502\n",
    "                    \u25bc                               \u25bc\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502  PHASE 4: ARCHIVE & CLEANUP     \u2502  \u2502  \u26a0\ufe0f PHASE 4 SKIPPED             \u2502\n",
    "\u2502  \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550  \u2502  \u2502                                 \u2502\n",
    "\u2502                                 \u2502  \u2502  Landing preserved for:         \u2502\n",
    "\u2502  \ud83d\udcc2 Input:  /landing/*.parquet  \u2502  \u2502  \u2022 Investigation                \u2502\n",
    "\u2502  \ud83d\udcc2 Output: /archive/YYYYMMDD_  \u2502  \u2502  \u2022 Retry after fixes            \u2502\n",
    "\u2502             HHMMSS/             \u2502  \u2502                                 \u2502\n",
    "\u2502                                 \u2502  \u2502  Re-run pipeline after          \u2502\n",
    "\u2502  Actions:                       \u2502  \u2502  resolving issues               \u2502\n",
    "\u2502  1. Create timestamped archive  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "\u2502     folder                      \u2502\n",
    "\u2502                                 \u2502\n",
    "\u2502  2. COPY landing files \u2192        \u2502\n",
    "\u2502     /archive/YYYYMMDD_HHMMSS/   \u2502\n",
    "\u2502                                 \u2502\n",
    "\u2502  3. CLEAR landing zone          \u2502\n",
    "\u2502     (only after successful      \u2502\n",
    "\u2502      archive)                   \u2502\n",
    "\u2502                                 \u2502\n",
    "\u2502  4. Save manifest.json with:    \u2502\n",
    "\u2502     \u2022 archive_date              \u2502\n",
    "\u2502     \u2022 pipeline_run timestamp    \u2502\n",
    "\u2502     \u2022 files_archived list       \u2502\n",
    "\u2502     \u2022 validation_summary        \u2502\n",
    "\u2502     \u2022 success_rate              \u2502\n",
    "\u2502                                 \u2502\n",
    "\u2502  Code: copy_file_fabric(),      \u2502\n",
    "\u2502        delete_file_fabric()     \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                    \u2502\n",
    "                    \u25bc\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502  \u2705 PIPELINE COMPLETE                                                        \u2502\n",
    "\u2502                                                                             \u2502\n",
    "\u2502  Final State:                                                               \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n",
    "\u2502  \u2502  /landing/  \u2192 EMPTY (ready for next SFTP batch)                     \u2502   \u2502\n",
    "\u2502  \u2502  /Bronze/   \u2192 Current batch raw data (CLEARED next run)             \u2502   \u2502\n",
    "\u2502  \u2502  /Silver/   \u2192 Validated data (CLEARED next run)                     \u2502   \u2502\n",
    "\u2502  \u2502  /archive/  \u2192 Source of truth (timestamped historical backups)      \u2502   \u2502\n",
    "\u2502  \u2502  /Gold/     \u2192 (Future: aggregated/business-ready data)              \u2502   \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n",
    "\u2502                                                                             \u2502\n",
    "\u2502  Recovery: Copy from /archive/YYYYMMDD_HHMMSS/ back to /landing/           \u2502\n",
    "\u2502            and re-run pipeline to regenerate Bronze & Silver               \u2502\n",
    "\u2502                                                                             \u2502\n",
    "\u2502  Execution log saved to: /config/validation_results/orchestration_log_*.json\u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Data Flow Summary\n",
    "\n",
    "```\n",
    "  SFTP \u2192 /landing/ \u2500\u2500COPY\u2500\u2500\u25ba /Bronze/ \u2500\u2500VALIDATE\u2500\u2500\u25ba /Silver/\n",
    "              \u2502                  \u2502\n",
    "              \u2502                  \u2514\u2500\u2500 (regeneratable)\n",
    "              \u2502\n",
    "              \u2514\u2500\u2500ARCHIVE\u2500\u2500\u25ba /archive/YYYYMMDD_HHMMSS/  (source of truth)\n",
    "```\n",
    "\n",
    "| Layer | Archived? | Cleared Each Run? | Purpose |\n",
    "|-------|-----------|-------------------|---------|\n",
    "| `/landing/` | \u2705 Yes | \u2705 Phase 4 | Raw SFTP files (temporary) |\n",
    "| `/Bronze/` | \u274c No | \u2705 Phase 0 | Working copy (regeneratable) |\n",
    "| `/Silver/` | \u274c No | \u2705 Phase 2 | Validated data (regeneratable) |\n",
    "| `/archive/` | N/A | \u274c Never | Historical source of truth |\n",
    "| `/Gold/` | TBD | TBD | Future: Business aggregates |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec5da61",
   "metadata": {},
   "source": [
    "## \ud83d\udd27 Environment Detection & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963915de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T14:39:26.525995Z",
     "iopub.status.busy": "2025-12-10T14:39:26.525873Z",
     "iopub.status.idle": "2025-12-10T14:39:26.531081Z",
     "shell.execute_reply": "2025-12-10T14:39:26.530616Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting configuration...\n",
      "\u2705 Loaded configuration for environment: local\n",
      "   Platform: Local\n",
      "Environment detection took 0.00s\n",
      "Scanning Bronze directory: /home/sanmi/Documents/HS2/HS2_PROJECTS_2025/1_AIMS_LOCAL_2026/data/Samples_LH_Bronze_Aims_26_parquet\n",
      "\n",
      "Configuration Summary:\n",
      "   Environment: Local\n",
      "   Bronze (Source): /home/sanmi/Documents/HS2/HS2_PROJECTS_2025/1_AIMS_LOCAL_2026/data/Samples_LH_Bronze_Aims_26_parquet\n",
      "   Silver (Target): /home/sanmi/Documents/HS2/HS2_PROJECTS_2025/1_AIMS_LOCAL_2026/data/Silver\n",
      "   Config Dir: /home/sanmi/Documents/HS2/HS2_PROJECTS_2025/1_AIMS_LOCAL_2026/config/data_quality\n",
      "   Workers: 4\n",
      "   Found 68 parquet files\n",
      "Configuration complete in 0.00s\n"
     ]
    }
   ],
   "source": [
    "# --- UNIFIED CONFIGURATION ---\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"Starting configuration...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Add project root to path for imports\n",
    "project_root = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Import centralized configuration\n",
    "try:\n",
    "    from notebooks.config import settings\n",
    "    from notebooks.lib import platform_utils, logging_utils\n",
    "    from notebooks.lib.storage import StorageManager\n",
    "    \n",
    "    IS_FABRIC = platform_utils.IS_FABRIC\n",
    "    BASE_DIR = settings.base_dir\n",
    "    BRONZE_DIR = settings.bronze_dir\n",
    "    SILVER_DIR = settings.silver_dir\n",
    "    GOLD_DIR = settings.gold_dir\n",
    "    CONFIG_DIR = settings.config_dir\n",
    "    RESULTS_DIR = settings.validation_results_dir\n",
    "    NUM_WORKERS = settings.max_workers\n",
    "    SAMPLE_SIZE = settings.sample_size\n",
    "    STORAGE_FORMAT = settings.storage_format\n",
    "    storage_manager = StorageManager()\n",
    "    logger = logging_utils.setup_notebook_logger(\"orchestration\")\n",
    "    \n",
    "    print(f\"\u2705 Loaded configuration for environment: {settings.environment}\")\n",
    "    print(f\"   Platform: {'Microsoft Fabric' if IS_FABRIC else 'Local'}\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"\u26a0\ufe0f Falling back to inline configuration: {e}\")\n",
    "    \n",
    "    IS_FABRIC = Path(\"/lakehouse/default/Files\").exists()\n",
    "    \n",
    "    if IS_FABRIC:\n",
    "        BASE_DIR = Path(\"/lakehouse/default/Files\")\n",
    "    else:\n",
    "        _notebook_dir = Path.cwd()\n",
    "        _candidate = _notebook_dir\n",
    "        for _ in range(5):\n",
    "            if (_candidate / \"aims_data_platform\").exists() or (_candidate / \"pyproject.toml\").exists():\n",
    "                BASE_DIR = _candidate\n",
    "                break\n",
    "            _candidate = _candidate.parent\n",
    "        else:\n",
    "            BASE_DIR = _notebook_dir.parent if _notebook_dir.name == \"notebooks\" else _notebook_dir\n",
    "    \n",
    "    BRONZE_DIR = BASE_DIR / \"Bronze\"\n",
    "    SILVER_DIR = BASE_DIR / \"Silver\"\n",
    "    GOLD_DIR = BASE_DIR / \"Gold\"\n",
    "    CONFIG_DIR = BASE_DIR / \"config\" / \"data_quality\"\n",
    "    RESULTS_DIR = BASE_DIR / \"config\" / \"validation_results\"\n",
    "    \n",
    "    NUM_WORKERS = 8 if IS_FABRIC else 4\n",
    "    SAMPLE_SIZE = 100000\n",
    "    STORAGE_FORMAT = \"parquet\"\n",
    "    storage_manager = None\n",
    "    logger = None\n",
    "    \n",
    "    print(f\"   Using fallback configuration for {'Fabric' if IS_FABRIC else 'Local'}\")\n",
    "\n",
    "# Define landing and archive directories\n",
    "LANDING_DIR = BASE_DIR / \"landing\"\n",
    "ARCHIVE_DIR = BASE_DIR / \"archive\"\n",
    "\n",
    "print(f\"Environment detection took {time.time() - start_time:.2f}s\")\n",
    "\n",
    "# ============================================================================\n",
    "# FABRIC PATH HELPER & FILE OPERATIONS\n",
    "# ============================================================================\n",
    "def fabric_path(path):\n",
    "    \"\"\"Convert Path to Fabric-compatible string (relative from lakehouse root)\"\"\"\n",
    "    path_str = str(path)\n",
    "    if \"/lakehouse/default/\" in path_str:\n",
    "        return path_str.replace(\"/lakehouse/default/\", \"\")\n",
    "    return path_str\n",
    "\n",
    "def list_parquet_files(directory):\n",
    "    \"\"\"List parquet files in directory (works on both Fabric and local)\"\"\"\n",
    "    files = []\n",
    "    if IS_FABRIC:\n",
    "        try:\n",
    "            from notebookutils import mssparkutils\n",
    "            fab_dir = fabric_path(directory)\n",
    "            try:\n",
    "                files_info = mssparkutils.fs.ls(fab_dir)\n",
    "                files = [f for f in files_info if f.name.endswith('.parquet')]\n",
    "            except Exception:\n",
    "                pass\n",
    "        except ImportError:\n",
    "            if Path(directory).exists():\n",
    "                files = list(Path(directory).glob(\"*.parquet\"))\n",
    "    else:\n",
    "        if Path(directory).exists():\n",
    "            files = list(Path(directory).glob(\"*.parquet\"))\n",
    "    return files\n",
    "\n",
    "def copy_file_fabric(src, dst):\n",
    "    \"\"\"Copy file (works on both Fabric and local)\"\"\"\n",
    "    if IS_FABRIC:\n",
    "        from notebookutils import mssparkutils\n",
    "        mssparkutils.fs.cp(fabric_path(src), fabric_path(dst))\n",
    "    else:\n",
    "        import shutil\n",
    "        shutil.copy2(str(src), str(dst))\n",
    "\n",
    "def delete_file_fabric(path):\n",
    "    \"\"\"Delete file (works on both Fabric and local)\"\"\"\n",
    "    if IS_FABRIC:\n",
    "        from notebookutils import mssparkutils\n",
    "        mssparkutils.fs.rm(fabric_path(path))\n",
    "    else:\n",
    "        Path(path).unlink()\n",
    "\n",
    "def clear_directory(directory):\n",
    "    \"\"\"Clear all parquet files from a directory (works on both Fabric and local)\"\"\"\n",
    "    cleared = 0\n",
    "    if IS_FABRIC:\n",
    "        try:\n",
    "            from notebookutils import mssparkutils\n",
    "            fab_dir = fabric_path(directory)\n",
    "            try:\n",
    "                files = mssparkutils.fs.ls(fab_dir)\n",
    "                for f in files:\n",
    "                    if f.name.endswith('.parquet'):\n",
    "                        mssparkutils.fs.rm(f\"{fab_dir}/{f.name}\")\n",
    "                        cleared += 1\n",
    "            except Exception:\n",
    "                pass\n",
    "        except ImportError:\n",
    "            pass\n",
    "    else:\n",
    "        if Path(directory).exists():\n",
    "            for f in Path(directory).glob(\"*.parquet\"):\n",
    "                f.unlink()\n",
    "                cleared += 1\n",
    "    return cleared\n",
    "\n",
    "def ensure_dir_exists(directory):\n",
    "    \"\"\"Ensure directory exists (works on both Fabric and local)\"\"\"\n",
    "    if IS_FABRIC:\n",
    "        try:\n",
    "            from notebookutils import mssparkutils\n",
    "            fab_dir = fabric_path(directory)\n",
    "            try:\n",
    "                mssparkutils.fs.ls(fab_dir)\n",
    "            except Exception:\n",
    "                mssparkutils.fs.mkdirs(fab_dir)\n",
    "        except ImportError:\n",
    "            Path(directory).mkdir(exist_ok=True, parents=True)\n",
    "    else:\n",
    "        Path(directory).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# ============================================================================\n",
    "# AUTO-CREATE DIRECTORIES\n",
    "# ============================================================================\n",
    "print(\"\\n\ud83d\udcc1 Ensuring directories exist...\")\n",
    "for dir_path in [LANDING_DIR, ARCHIVE_DIR, BRONZE_DIR, SILVER_DIR, GOLD_DIR, CONFIG_DIR, RESULTS_DIR]:\n",
    "    ensure_dir_exists(dir_path)\n",
    "    print(f\"   \u2713 {dir_path.name}/\")\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 0: COPY LANDING \u2192 BRONZE (with full Bronze refresh)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PHASE 0: LANDING ZONE \u2192 BRONZE (full refresh)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "landing_files = list_parquet_files(LANDING_DIR)\n",
    "print(f\"\ud83d\udcc2 Scanning landing zone: {LANDING_DIR}\")\n",
    "print(f\"   Found {len(landing_files)} parquet files in landing\")\n",
    "\n",
    "# Track files for Phase 4 cleanup\n",
    "LANDING_FILES_TO_ARCHIVE = []\n",
    "\n",
    "if len(landing_files) > 0:\n",
    "    # Step 1: Clear Bronze layer (fresh data each run)\n",
    "    print(f\"\\n\ud83e\uddf9 Clearing Bronze layer for fresh data...\")\n",
    "    bronze_cleared = clear_directory(BRONZE_DIR)\n",
    "    print(f\"   Removed {bronze_cleared} old files from Bronze\")\n",
    "    \n",
    "    # Step 2: Copy new files from landing to Bronze\n",
    "    print(f\"\\n\ud83d\udccb Copying {len(landing_files)} files to Bronze...\")\n",
    "    copied_count = 0\n",
    "    \n",
    "    for f in landing_files:\n",
    "        # Get filename (handle both Fabric FileInfo and Path objects)\n",
    "        filename = f.name if hasattr(f, 'name') else f.name\n",
    "        \n",
    "        # Build paths\n",
    "        if IS_FABRIC:\n",
    "            src_path = f\"{LANDING_DIR}/{filename}\"\n",
    "            bronze_path = f\"{BRONZE_DIR}/{filename}\"\n",
    "        else:\n",
    "            src_path = LANDING_DIR / filename\n",
    "            bronze_path = BRONZE_DIR / filename\n",
    "        \n",
    "        try:\n",
    "            # Copy to Bronze (keep original in landing as safety net)\n",
    "            copy_file_fabric(src_path, bronze_path)\n",
    "            print(f\"   \u2705 Copied to Bronze: {filename}\")\n",
    "            copied_count += 1\n",
    "            LANDING_FILES_TO_ARCHIVE.append(filename)\n",
    "        except Exception as e:\n",
    "            print(f\"   \u274c Copy failed: {filename} - {e}\")\n",
    "    \n",
    "    print(f\"\\n   \ud83d\udcca Phase 0 Summary:\")\n",
    "    print(f\"      Bronze cleared: {bronze_cleared} old files removed\")\n",
    "    print(f\"      Copied to Bronze: {copied_count}/{len(landing_files)}\")\n",
    "    print(f\"      Landing preserved until Phase 4 (safety net)\")\n",
    "else:\n",
    "    print(\"   \u2139\ufe0f No new files in landing zone (waiting for SFTP data)\")\n",
    "\n",
    "# ============================================================================\n",
    "# SCAN BRONZE DATA\n",
    "# ============================================================================\n",
    "print(f\"\\n\ud83d\udcc2 Scanning Bronze directory: {BRONZE_DIR}\")\n",
    "parquet_files = list_parquet_files(BRONZE_DIR)\n",
    "print(f\"   Found {len(parquet_files)} parquet files\")\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY\n",
    "# ============================================================================\n",
    "print(f\"\\n\ud83d\udcca Configuration Summary:\")\n",
    "print(f\"   Environment: {'Fabric' if IS_FABRIC else 'Local'}\")\n",
    "print(f\"   Landing: {LANDING_DIR} (preserved until Phase 4)\")\n",
    "print(f\"   Archive: {ARCHIVE_DIR}\")\n",
    "print(f\"   Bronze (Source): {BRONZE_DIR} (CLEARED each run)\")\n",
    "print(f\"   Silver (Target): {SILVER_DIR} (CLEARED each run)\")\n",
    "print(f\"   Config Dir: {CONFIG_DIR}\")\n",
    "print(f\"   Workers: {NUM_WORKERS}\")\n",
    "print(f\"   Bronze files: {len(parquet_files)}\")\n",
    "\n",
    "if len(parquet_files) == 0:\n",
    "    print(f\"\\n\u26a0\ufe0f No parquet files in Bronze directory.\")\n",
    "    print(f\"   Upload data to Files/landing/ and re-run this cell.\")\n",
    "    BRONZE_DATA_AVAILABLE = False\n",
    "else:\n",
    "    print(f\"\\n\u2705 Ready to process {len(parquet_files)} files\")\n",
    "    BRONZE_DATA_AVAILABLE = True\n",
    "\n",
    "print(f\"\\nConfiguration complete in {time.time() - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ed394d",
   "metadata": {},
   "source": [
    "## \ud83d\udce6 Package Installation & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b587d802",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T14:39:26.532895Z",
     "iopub.status.busy": "2025-12-10T14:39:26.532646Z",
     "iopub.status.idle": "2025-12-10T14:39:30.234809Z",
     "shell.execute_reply": "2025-12-10T14:39:30.234131Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install --quiet --upgrade great-expectations==0.18.8 ydata-profiling==4.5.1 pyarrow fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94fca6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Disable Great Expectations analytics to speed up import\n",
    "os.environ[\"GX_ANALYTICS_ENABLED\"] = \"False\"\n",
    "\n",
    "# Use the local library to ensure end-to-end alignment\n",
    "from aims_data_platform import BatchProfiler, DataQualityValidator, DataLoader, ConfigLoader\n",
    "\n",
    "# Import logging utilities if available\n",
    "try:\n",
    "    from notebooks.lib.logging_utils import timed_operation, log_phase\n",
    "    LOGGING_UTILS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LOGGING_UTILS_AVAILABLE = False\n",
    "    # Fallback: simple context manager\n",
    "    from contextlib import contextmanager\n",
    "    @contextmanager\n",
    "    def timed_operation(description, logger=None):\n",
    "        print(f\"\u23f1\ufe0f {description}...\")\n",
    "        start = time.time()\n",
    "        yield\n",
    "        print(f\"\u23f1\ufe0f {description} completed in {time.time() - start:.2f}s\")\n",
    "\n",
    "print(\"\u2705 Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ec081f",
   "metadata": {},
   "source": [
    "## \u2699\ufe0f Pipeline Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82a4363",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T14:39:30.242089Z",
     "iopub.status.busy": "2025-12-10T14:39:30.241976Z",
     "iopub.status.idle": "2025-12-10T14:39:30.245127Z",
     "shell.execute_reply": "2025-12-10T14:39:30.244662Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2699\ufe0f Pipeline Configuration:\n",
      "   run_profiling: True\n",
      "   run_ingestion: True\n",
      "   run_monitoring: True\n",
      "   run_dq_modeling: False\n",
      "   run_bi_analytics: False\n",
      "   force_reprocess: False\n",
      "   dq_threshold: 85.0\n",
      "   max_workers: 4\n",
      "   continue_on_error: False\n",
      "\n",
      "\u2705 Configuration Complete\n"
     ]
    }
   ],
   "source": [
    "# Pipeline configuration - using settings where available\n",
    "try:\n",
    "    # Use settings pipeline_phases if available\n",
    "    PIPELINE_CONFIG = {\n",
    "        \"run_profiling\": settings.pipeline_phases.get(\"profiling\", True),\n",
    "        \"run_ingestion\": settings.pipeline_phases.get(\"ingestion\", True),\n",
    "        \"run_monitoring\": settings.pipeline_phases.get(\"monitoring\", True),\n",
    "        \"run_dq_modeling\": settings.pipeline_phases.get(\"dq_modeling\", False),\n",
    "        \"run_bi_analytics\": settings.pipeline_phases.get(\"bi_analytics\", False),\n",
    "        \"force_reprocess\": False,\n",
    "        \"dq_threshold\": settings.get_dq_threshold(\"medium\"),\n",
    "        \"max_workers\": settings.max_workers,\n",
    "        \"continue_on_error\": False,\n",
    "    }\n",
    "except (NameError, AttributeError):\n",
    "    # Fallback to hardcoded defaults\n",
    "    PIPELINE_CONFIG = {\n",
    "        \"run_profiling\": True,\n",
    "        \"run_ingestion\": True,\n",
    "        \"run_monitoring\": True,\n",
    "        \"run_dq_modeling\": False,\n",
    "        \"run_bi_analytics\": False,\n",
    "        \"force_reprocess\": False,\n",
    "        \"dq_threshold\": 85.0,\n",
    "        \"max_workers\": NUM_WORKERS,\n",
    "        \"continue_on_error\": False,\n",
    "    }\n",
    "\n",
    "# Check if Bronze data is available (set in previous cell)\n",
    "try:\n",
    "    if not BRONZE_DATA_AVAILABLE:\n",
    "        print(\"\u26a0\ufe0f No Bronze data available - disabling data processing phases\")\n",
    "        PIPELINE_CONFIG[\"run_profiling\"] = False\n",
    "        PIPELINE_CONFIG[\"run_ingestion\"] = False\n",
    "except NameError:\n",
    "    BRONZE_DATA_AVAILABLE = len(parquet_files) > 0 if 'parquet_files' in dir() else False\n",
    "\n",
    "# Display Configuration\n",
    "print(\"\u2699\ufe0f Pipeline Configuration:\")\n",
    "for key, value in PIPELINE_CONFIG.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "# Initialize Execution Log\n",
    "execution_log = {\n",
    "    \"start_time\": datetime.now().isoformat(),\n",
    "    \"environment\": \"Fabric\" if IS_FABRIC else \"Local\",\n",
    "    \"storage_format\": STORAGE_FORMAT,\n",
    "    \"bronze_data_available\": BRONZE_DATA_AVAILABLE,\n",
    "    \"config\": PIPELINE_CONFIG,\n",
    "    \"phases\": []\n",
    "}\n",
    "\n",
    "if BRONZE_DATA_AVAILABLE:\n",
    "    print(\"\\n\u2705 Configuration Complete - Ready to process data\")\n",
    "else:\n",
    "    print(\"\\n\u26a0\ufe0f Configuration Complete - Waiting for Bronze data\")\n",
    "    print(\"   Upload parquet files to Bronze directory to begin processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6105ff7",
   "metadata": {},
   "source": [
    "## \ud83d\ude80 Phase 1: Data Profiling\n",
    "\n",
    "**Purpose:** Generate DQ validation configs for all Bronze layer tables\n",
    "\n",
    "**Process:**\n",
    "1. Profile each Bronze parquet file\n",
    "2. Generate validation YAML configs\n",
    "3. Save configs to `config/data_quality/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f200ba33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T14:39:30.246344Z",
     "iopub.status.busy": "2025-12-10T14:39:30.246221Z",
     "iopub.status.idle": "2025-12-10T14:39:30.254000Z",
     "shell.execute_reply": "2025-12-10T14:39:30.253498Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 1: DATA PROFILING\n",
      "================================================================================\n",
      "2026-01-19 13:09:35 | INFO     | orchestration | \u23f1\ufe0f Phase 1: Data Profiling...\n",
      "\n",
      "\ud83d\udcca Profiling Bronze layer: /home/sanmi/Documents/HS2/HS2_PROJECTS_2025/1_AIMS_LOCAL_2026/data/Samples_LH_Bronze_Aims_26_parquet\n",
      "   Workers: 4\n",
      "   Output: /home/sanmi/Documents/HS2/HS2_PROJECTS_2025/1_AIMS_LOCAL_2026/config/data_quality\n",
      "\n",
      "\u2705 Profiling Complete:\n",
      "   Files Profiled: 68\n",
      "   Configs Generated: 68\n",
      "2026-01-19 13:09:43 | INFO     | orchestration | \u23f1\ufe0f Phase 1: Data Profiling completed in 7.52s\n"
     ]
    }
   ],
   "source": [
    "if PIPELINE_CONFIG[\"run_profiling\"]:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PHASE 1: DATA PROFILING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    phase_start = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        with timed_operation(\"Phase 1: Data Profiling\", logger):\n",
    "            # Import profiling modules\n",
    "            from aims_data_platform import BatchProfiler\n",
    "            \n",
    "            print(f\"\\n\ud83d\udcca Profiling Bronze layer: {BRONZE_DIR}\")\n",
    "            print(f\"   Workers: {PIPELINE_CONFIG['max_workers']}\")\n",
    "            print(f\"   Output: {CONFIG_DIR}\")\n",
    "            \n",
    "            # Define custom thresholds\n",
    "            custom_thresholds = {\n",
    "                \"severity_threshold\": \"medium\",\n",
    "                \"null_tolerance\": 5.0,\n",
    "                \"include_structural\": True,\n",
    "                \"include_completeness\": True,\n",
    "                \"include_validity\": True\n",
    "            }\n",
    "            \n",
    "            # Run parallel profiling using BatchProfiler\n",
    "            results = BatchProfiler.run_parallel_profiling(\n",
    "                input_dir=str(BRONZE_DIR),\n",
    "                output_dir=str(CONFIG_DIR),\n",
    "                workers=PIPELINE_CONFIG['max_workers'],\n",
    "                sample_size=SAMPLE_SIZE,\n",
    "                thresholds=custom_thresholds\n",
    "            )\n",
    "            \n",
    "            # Count successes and errors\n",
    "            success_results = [r for r in results if r.get('status') == 'success']\n",
    "            error_results = [r for r in results if r.get('status') != 'success']\n",
    "            \n",
    "            # Display results\n",
    "            print(f\"\\n\u2705 Profiling Complete:\")\n",
    "            print(f\"   Files Profiled: {len(success_results)}\")\n",
    "            print(f\"   Configs Generated: {len(list(CONFIG_DIR.glob('*.yml')))}\")\n",
    "            if error_results:\n",
    "                print(f\"   Errors: {len(error_results)}\")\n",
    "                for err in error_results[:5]:\n",
    "                    print(f\"      - {err.get('file', 'unknown')}: {err.get('error', 'unknown error')}\")\n",
    "        \n",
    "        # Log phase execution\n",
    "        execution_log[\"phases\"].append({\n",
    "            \"phase\": \"profiling\",\n",
    "            \"status\": \"success\" if len(error_results) == 0 else \"partial\",\n",
    "            \"duration_seconds\": (datetime.now() - phase_start).total_seconds(),\n",
    "            \"files_profiled\": len(success_results),\n",
    "            \"configs_generated\": len(list(CONFIG_DIR.glob('*.yml'))),\n",
    "            \"errors\": len(error_results)\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n\u274c Profiling Failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        execution_log[\"phases\"].append({\n",
    "            \"phase\": \"profiling\",\n",
    "            \"status\": \"failed\",\n",
    "            \"error\": str(e),\n",
    "            \"duration_seconds\": (datetime.now() - phase_start).total_seconds()\n",
    "        })\n",
    "        \n",
    "        if not PIPELINE_CONFIG.get(\"continue_on_error\", False):\n",
    "            raise\n",
    "else:\n",
    "    print(\"\u23ed\ufe0f Skipping Phase 1: Data Profiling (disabled in config)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212c8a76",
   "metadata": {},
   "source": [
    "## \u2705 Phase 2: Data Validation & Ingestion\n",
    "\n",
    "**Purpose:** Validate Bronze data and ingest to Silver layer\n",
    "\n",
    "**Process:**\n",
    "1. Load validation configs\n",
    "2. Validate each Bronze table\n",
    "3. Ingest passing records to Silver (Delta Lake in Fabric, Parquet locally)\n",
    "4. Quarantine failing records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a98c6fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T14:39:30.255285Z",
     "iopub.status.busy": "2025-12-10T14:39:30.255153Z",
     "iopub.status.idle": "2025-12-10T14:39:30.264057Z",
     "shell.execute_reply": "2025-12-10T14:39:30.263509Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 2: DATA VALIDATION & INGESTION\n",
      "================================================================================\n",
      "2026-01-19 13:09:47 | INFO     | orchestration | \u23f1\ufe0f Phase 2: Validation & Ingestion...\n",
      "   Found 68 parquet files to validate\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13440c074c9e4523b5468fd945e9560b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 PASSED: aims_activitydates.parquet (100.0%)\n",
      "   \u2192 Ingested to Silver via StorageManager: aims_activitydates\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b11b5d859060452bb68972b905e012f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/526 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 PASSED: aims_assetattributes.parquet (100.0%)\n",
      "   \u2192 Ingested to Silver via StorageManager: aims_assetattributes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "576e359e5c7b41e2947ec5421939f441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/193 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 PASSED: aims_assetclassattributes.parquet (100.0%)\n",
      "   \u2192 Ingested to Silver via StorageManager: aims_assetclassattributes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e7f86f885064ada9f73912998edfd7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 PASSED: aims_assetclasschangelogs.parquet (100.0%)\n",
      "   \u2192 Ingested to Silver via StorageManager: aims_assetclasschangelogs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "424123d8178e4b77867db0f52f5785ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/382 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 PASSED: aims_assetclasses.parquet (99.1%)\n",
      "   \u2192 Ingested to Silver via StorageManager: aims_assetclasses\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d022b21519964c7fa47465246aa3489c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 PASSED: aims_assetclassrelationships.parquet (91.5%)\n",
      "   \u2192 Ingested to Silver via StorageManager: aims_assetclassrelationships\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32a95fe2957e412185b76f170762a9b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 PASSED: aims_assetconsents.parquet (100.0%)\n",
      "   \u2192 Ingested to Silver via StorageManager: aims_assetconsents\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b3b09c3f22141faae69489e12fa6911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation FAILED: 1 checks failed. Reasons: Severity 'critical' threshold 100.0% failed (actual: 85.7%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u274c FAILED: aims_assethierarchymap.parquet (92.9%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faf6bb0f699448afaa2be4ac7ab3abe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/319 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation FAILED: 3 checks failed. Reasons: Severity 'critical' threshold 100.0% failed (actual: 97.4%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u274c FAILED: aims_assetlocations.parquet (97.0%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0460d194aa224e9aac8d3a8b8795f1e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/219 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 PASSED: aims_assets.parquet (100.0%)\n",
      "   \u2192 Ingested to Silver via StorageManager: aims_assets\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d43b6e9765ec4cfdae59b908c0b08b26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 PASSED: aims_attributedomains.parquet (100.0%)\n",
      "   \u2192 Ingested to Silver via StorageManager: aims_attributedomains\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a34df92473d40628fb2d8def325eebe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 PASSED: aims_attributedomainvalues.parquet (100.0%)\n",
      "   \u2192 Ingested to Silver via StorageManager: aims_attributedomainvalues\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2701f7d9cce541d19ed51c4b0cd4712e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 PASSED: aims_attributegroups.parquet (100.0%)\n",
      "   \u2192 Ingested to Silver via StorageManager: aims_attributegroups\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f50089b09724481ac8ca819a895cb44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/301 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 PASSED: aims_attributes.parquet (96.7%)\n",
      "   \u2192 Ingested to Silver via StorageManager: aims_attributes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f4e612bba5942e58c651d5777f1f4e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation FAILED: 1 checks failed. Reasons: Severity 'critical' threshold 100.0% failed (actual: 90.9%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u274c FAILED: aims_consentlinks.parquet (95.5%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a404eefffae463ab1b05121bcfc0ab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 PASSED: aims_consentmilestones.parquet (100.0%)\n",
      "   \u2192 Ingested to Silver via StorageManager: aims_consentmilestones\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9756a8c7d8894597b1c4cbde7f3f7a1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 PASSED: aims_consentmilestonetypes.parquet (100.0%)\n",
      "   \u2192 Ingested to Silver via StorageManager: aims_consentmilestonetypes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "267ec634666d44f699cc799374583425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/238 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 PASSED: aims_consents.parquet (100.0%)\n",
      "   \u2192 Ingested to Silver via StorageManager: aims_consents\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ffce8481ab0466aa90937e50b755827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 PASSED: aims_consenttypemilestones.parquet (100.0%)\n",
      "   \u2192 Ingested to Silver via StorageManager: aims_consenttypemilestones\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "130f921703b346b6b9b87aa9a793442e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/67 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 PASSED: aims_consenttypes.parquet (100.0%)\n",
      "   \u2192 Ingested to Silver via StorageManager: aims_consenttypes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da59272114d144ed905f976c6e87ca16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 PASSED: aims_informationneedassetclass.parquet (100.0%)\n",
      "   \u2192 Ingested to Silver via StorageManager: aims_informationneedassetclass\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79d247780c1248a397bc01953c1e46a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/131 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 PASSED: aims_informationneedattributes.parquet (100.0%)\n",
      "   \u2192 Ingested to Silver via StorageManager: aims_informationneedattributes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a881f937ba744ccb283e1e57d656e02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/74 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation FAILED: 1 checks failed. Reasons: Severity 'critical' threshold 100.0% failed (actual: 93.3%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u274c FAILED: aims_informationneeddocs.parquet (96.7%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8617a312bb64074b391a0fc12ac82c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/214 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 PASSED: aims_informationneedgeometries.parquet (100.0%)\n",
      "   \u2192 Ingested to Silver via StorageManager: aims_informationneedgeometries\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41535725d4044082bf4ef473221a4da1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 PASSED: aims_informationneedlinks.parquet (100.0%)\n",
      "   \u2192 Ingested to Silver via StorageManager: aims_informationneedlinks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2c2162de28b49debf75663b99c34f05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/338 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 PASSED: aims_informationneedpropchngs.parquet (100.0%)\n",
      "   \u2192 Ingested to Silver via StorageManager: aims_informationneedpropchngs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a7267e213c940e79a1b22c7922b318a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 PASSED: aims_informationneeds.parquet (100.0%)\n",
      "   \u2192 Ingested to Silver via StorageManager: aims_informationneeds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b69e156a29cc450f8e2b1af7f14c75fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 PASSED: aims_informationneedsourcedocs.parquet (100.0%)\n",
      "   \u2192 Ingested to Silver via StorageManager: aims_informationneedsourcedocs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3d6bed1beb14d4e827112abbacff594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation FAILED: 1 checks failed. Reasons: Severity 'critical' threshold 100.0% failed (actual: 90.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u274c FAILED: aims_informationneedstatusupd.parquet (95.5%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6002afc16e846588a64b0ee396e28cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation FAILED: 1 checks failed. Reasons: Severity 'critical' threshold 100.0% failed (actual: 90.9%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u274c FAILED: aims_informationpackages.parquet (95.5%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "103009a7971f4efa811baf9f50c63064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/126 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 PASSED: aims_links.parquet (97.6%)\n",
      "   \u2192 Ingested to Silver via StorageManager: aims_links\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51cbddff576b4995a243cda97b1ec5fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/136 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 PASSED: aims_linktypes.parquet (100.0%)\n",
      "   \u2192 Ingested to Silver via StorageManager: aims_linktypes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5088d1734bdb43f28576056565755d18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/1045 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 PASSED: aims_noncompliances.parquet (98.7%)\n",
      "   \u2192 Ingested to Silver via StorageManager: aims_noncompliances\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7a2f32b3c874451a3edd0935b537eea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 PASSED: aims_organisations.parquet (96.3%)\n",
      "   \u2192 Ingested to Silver via StorageManager: aims_organisations\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd797c5d506b4c78bd12596d90bc3cd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 PASSED: aims_owners.parquet (100.0%)\n",
      "   \u2192 Ingested to Silver via StorageManager: aims_owners\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7be04238d6d94cefae3ae3b45cb872ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/138 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation FAILED: 1 checks failed. Reasons: Severity 'critical' threshold 100.0% failed (actual: 94.1%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u274c FAILED: aims_people.parquet (97.6%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0de5d064067e48b6892043b5a0a90a25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 PASSED: aims_phases.parquet (100.0%)\n",
      "   \u2192 Ingested to Silver via StorageManager: aims_phases\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d09e51b8501e4bf789b9bc2ca164d928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation FAILED: 1 checks failed. Reasons: Severity 'critical' threshold 100.0% failed (actual: 91.7%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u274c FAILED: aims_productassetclasses.parquet (95.8%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7359152ff2d45739ae4a20d25cc107b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation FAILED: 1 checks failed. Reasons: Severity 'critical' threshold 100.0% failed (actual: 95.2%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u274c FAILED: aims_productcharacteristics.parquet (97.6%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12251c8134444fa6b3134bdc562744ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation FAILED: 1 checks failed. Reasons: Severity 'critical' threshold 100.0% failed (actual: 90.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u274c FAILED: aims_productlinks.parquet (95.0%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a31842cddfd4be488d827399e870f8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation FAILED: 1 checks failed. Reasons: Severity 'critical' threshold 100.0% failed (actual: 95.5%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u274c FAILED: aims_products.parquet (97.7%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ea9541f84354c26bcee5c97979538a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/255 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 PASSED: aims_projectitemactions.parquet (100.0%)\n",
      "   \u2192 Ingested to Silver via StorageManager: aims_projectitemactions\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb0f196146ab470c9de13e45278d52f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 PASSED: aims_projectitemassignedroles.parquet (100.0%)\n",
      "   \u2192 Ingested to Silver via StorageManager: aims_projectitemassignedroles\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8017e9d824a04bfdb9dbd9b382b76a8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/221 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 PASSED: aims_projectitemattributes.parquet (100.0%)\n",
      "   \u2192 Ingested to Silver via StorageManager: aims_projectitemattributes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27c3cf97068147ba86ad94717a16a69a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation FAILED: 1 checks failed. Reasons: Severity 'critical' threshold 100.0% failed (actual: 92.3%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u274c FAILED: aims_projectitemlinks.parquet (96.2%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "658050f46f23453a85a455fcd850f28e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/121 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 PASSED: aims_projectitems.parquet (100.0%)\n",
      "   \u2192 Ingested to Silver via StorageManager: aims_projectitems\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1802dfb72c343b0be8049101af2f5b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 PASSED: aims_relationships.parquet (100.0%)\n",
      "   \u2192 Ingested to Silver via StorageManager: aims_relationships\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f6b6686419047a4be973c95a977fd27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 PASSED: aims_relationshiptypes.parquet (100.0%)\n",
      "   \u2192 Ingested to Silver via StorageManager: aims_relationshiptypes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6080a13149eb407189cc12b1880fe200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/139 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 PASSED: aims_routes.parquet (100.0%)\n",
      "   \u2192 Ingested to Silver via StorageManager: aims_routes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2843bc67f1214100908825b0ff40d364",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 PASSED: aims_secondaryassetclasscodes.parquet (100.0%)\n",
      "   \u2192 Ingested to Silver via StorageManager: aims_secondaryassetclasscodes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df8935c3c98f406dac2fa5217b775ff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 PASSED: aims_stages.parquet (100.0%)\n",
      "   \u2192 Ingested to Silver via StorageManager: aims_stages\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16a86caada5b481c85d35127d75d13d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/178 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 PASSED: aims_taskdefinitions.parquet (100.0%)\n",
      "   \u2192 Ingested to Silver via StorageManager: aims_taskdefinitions\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e222f28798d2455e8ad53f4041d44f52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 PASSED: aims_tracks.parquet (100.0%)\n",
      "   \u2192 Ingested to Silver via StorageManager: aims_tracks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b7007edbd7844838a6bf6c69609efa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/93 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 PASSED: aims_ua_beneficiaries.parquet (100.0%)\n",
      "   \u2192 Ingested to Silver via StorageManager: aims_ua_beneficiaries\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee8b7e55bf7045e1993def4d28ba633e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 PASSED: aims_ua_comments.parquet (100.0%)\n",
      "   \u2192 Ingested to Silver via StorageManager: aims_ua_comments\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db183c7b48de4f24993d728a9b14b0e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 PASSED: aims_ua_entities.parquet (100.0%)\n",
      "   \u2192 Ingested to Silver via StorageManager: aims_ua_entities\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85aaeece82684503ab4eeeda9f40fc78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 PASSED: aims_ua_meetingattendees.parquet (100.0%)\n",
      "   \u2192 Ingested to Silver via StorageManager: aims_ua_meetingattendees\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0487cf4db99492ba0b1608c965e0fd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/88 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 PASSED: aims_ua_meetings.parquet (100.0%)\n",
      "   \u2192 Ingested to Silver via StorageManager: aims_ua_meetings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89e056c2af5c42e5b145807ddf49337a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 PASSED: aims_ua_noncompimppartytypes.parquet (100.0%)\n",
      "   \u2192 Ingested to Silver via StorageManager: aims_ua_noncompimppartytypes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6468ee11577d4c99890e879955ccfb8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 PASSED: aims_ua_noncomplianceimpacts.parquet (100.0%)\n",
      "   \u2192 Ingested to Silver via StorageManager: aims_ua_noncomplianceimpacts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf4fb9b2eac04ae0a12e810e3f409c0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 PASSED: aims_ua_noncompotheruas.parquet (100.0%)\n",
      "   \u2192 Ingested to Silver via StorageManager: aims_ua_noncompotheruas\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f0b38c2ce0c4b74af6ef356a8209447",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 PASSED: aims_ua_optionvalues.parquet (100.0%)\n",
      "   \u2192 Ingested to Silver via StorageManager: aims_ua_optionvalues\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "986a5c1402034f3eb0ae16900a6e52b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/283 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation FAILED: 1 checks failed. Reasons: Severity 'critical' threshold 100.0% failed (actual: 97.3%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u274c FAILED: aims_undertakings_assurances.parquet (98.9%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8f246db03c94512b16b38047a4ca1f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation FAILED: 1 checks failed. Reasons: Severity 'critical' threshold 100.0% failed (actual: 94.4%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u274c FAILED: aims_workbanks.parquet (97.2%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ad2a5a7001641dd8a1f2e61a015f3ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation FAILED: 1 checks failed. Reasons: Severity 'critical' threshold 100.0% failed (actual: 90.9%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u274c FAILED: aims_workbankworkorders.parquet (95.5%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f148ceb755b4671acb8f3f66b92fb34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation FAILED: 1 checks failed. Reasons: Severity 'critical' threshold 100.0% failed (actual: 94.4%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u274c FAILED: aims_workorderattributes.parquet (97.2%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b35433fe492544efada9356a63aa762b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/119 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation FAILED: 1 checks failed. Reasons: Severity 'critical' threshold 100.0% failed (actual: 95.8%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u274c FAILED: aims_workorders.parquet (97.9%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e39b6e9467fe4840973c6804c9b60c66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation FAILED: 1 checks failed. Reasons: Severity 'critical' threshold 100.0% failed (actual: 94.4%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u274c FAILED: aims_workorderstatustransition.parquet (97.2%)\n",
      "\n",
      "======================================================================\n",
      "VALIDATION SUMMARY\n",
      "======================================================================\n",
      "Total Files:  68\n",
      "\u2705 Passed:     50\n",
      "\u274c Failed:     18\n",
      "\u26a0\ufe0f  Skipped:    0\n",
      "\ud83d\udca5 Errors:     0\n",
      "\n",
      "Pass Rate: 73.5%\n",
      "Results saved to: /home/sanmi/Documents/HS2/HS2_PROJECTS_2025/1_AIMS_LOCAL_2026/notebooks/config/validation_results/validation_results.json\n",
      "======================================================================\n",
      "2026-01-19 13:10:51 | INFO     | orchestration | \u23f1\ufe0f Phase 2: Validation & Ingestion completed in 64.40s\n"
     ]
    }
   ],
   "source": [
    "if PIPELINE_CONFIG[\"run_ingestion\"]:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PHASE 2: DATA VALIDATION & INGESTION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    phase_start = datetime.now()\n",
    "    \n",
    "    # Helper function to write JSON (works on both Fabric and local)\n",
    "    def write_json_file(file_path, data):\n",
    "        \"\"\"Write JSON file - uses mssparkutils on Fabric, standard IO locally\"\"\"\n",
    "        content = json.dumps(data, indent=2)\n",
    "        if IS_FABRIC:\n",
    "            from notebookutils import mssparkutils\n",
    "            fab_path = fabric_path(file_path)\n",
    "            mssparkutils.fs.put(fab_path, content, overwrite=True)\n",
    "        else:\n",
    "            with open(str(file_path), 'w') as f:\n",
    "                f.write(content)\n",
    "    \n",
    "    # Helper function to read JSON (works on both Fabric and local)\n",
    "    def read_json_file(file_path):\n",
    "        \"\"\"Read JSON file - uses mssparkutils on Fabric, standard IO locally\"\"\"\n",
    "        if IS_FABRIC:\n",
    "            from notebookutils import mssparkutils\n",
    "            fab_path = fabric_path(file_path)\n",
    "            content = mssparkutils.fs.head(fab_path, 1000000)  # Read up to 1MB\n",
    "            return json.loads(content)\n",
    "        else:\n",
    "            with open(str(file_path), 'r') as f:\n",
    "                return json.load(f)\n",
    "    \n",
    "    # Helper to check if file exists on Fabric\n",
    "    def file_exists_fabric(file_path):\n",
    "        \"\"\"Check if file exists - works on both Fabric and local\"\"\"\n",
    "        if IS_FABRIC:\n",
    "            try:\n",
    "                from notebookutils import mssparkutils\n",
    "                fab_path = fabric_path(file_path)\n",
    "                mssparkutils.fs.head(fab_path, 1)\n",
    "                return True\n",
    "            except Exception:\n",
    "                return False\n",
    "        else:\n",
    "            return Path(file_path).exists()\n",
    "    \n",
    "    try:\n",
    "        with timed_operation(\"Phase 2: Validation & Ingestion\", logger):\n",
    "            # Import validation modules\n",
    "            from aims_data_platform import DataQualityValidator, DataLoader\n",
    "            \n",
    "            # Clear Silver layer for complete overwrite (no append/delta)\n",
    "            if storage_manager is not None:\n",
    "                clear_result = storage_manager.clear_layer(\"silver\")\n",
    "                print(f\"   Cleared Silver layer: {clear_result['files_cleared']} tables removed\")\n",
    "            elif IS_FABRIC:\n",
    "                # Use mssparkutils to clear Silver on Fabric\n",
    "                try:\n",
    "                    from notebookutils import mssparkutils\n",
    "                    fab_silver = fabric_path(SILVER_DIR)\n",
    "                    try:\n",
    "                        existing = mssparkutils.fs.ls(fab_silver)\n",
    "                        for f in existing:\n",
    "                            if f.name.endswith('.parquet'):\n",
    "                                mssparkutils.fs.rm(f\"{fab_silver}/{f.name}\")\n",
    "                        print(f\"   Cleared Silver directory for fresh write\")\n",
    "                    except Exception:\n",
    "                        print(f\"   Silver directory empty or doesn't exist yet\")\n",
    "                except ImportError:\n",
    "                    pass\n",
    "            elif SILVER_DIR.exists():\n",
    "                for f in SILVER_DIR.glob(\"*.parquet\"):\n",
    "                    f.unlink()\n",
    "                print(f\"   Cleared Silver directory for fresh write\")\n",
    "            \n",
    "            # Track validation results\n",
    "            validation_results = {\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"threshold\": PIPELINE_CONFIG['dq_threshold'],\n",
    "                \"storage_format\": STORAGE_FORMAT,\n",
    "                \"files\": {},\n",
    "                \"summary\": {\"total\": 0, \"passed\": 0, \"failed\": 0, \"skipped\": 0, \"errors\": 0}\n",
    "            }\n",
    "            \n",
    "            validation_results[\"summary\"][\"total\"] = len(parquet_files)\n",
    "            print(f\"   Found {len(parquet_files)} parquet files to validate\\n\")\n",
    "            \n",
    "            # Sort files by name (handles both FileInfo and Path objects)\n",
    "            sorted_files = sorted(parquet_files, key=lambda f: f.name if hasattr(f, 'name') else str(f))\n",
    "            \n",
    "            # Track tables for Delta persistence\n",
    "            TABLES_TO_PERSIST = []\n",
    "            \n",
    "            # Validate each file\n",
    "            for parquet_file in sorted_files:\n",
    "                # Handle both Fabric FileInfo and Path objects\n",
    "                if hasattr(parquet_file, 'path'):\n",
    "                    # Fabric FileInfo object\n",
    "                    file_name = parquet_file.name\n",
    "                    table_name = file_name.replace('.parquet', '')\n",
    "                    file_path = str(BRONZE_DIR / file_name)\n",
    "                else:\n",
    "                    # Local Path object\n",
    "                    file_name = parquet_file.name\n",
    "                    table_name = parquet_file.stem\n",
    "                    file_path = str(parquet_file)\n",
    "                \n",
    "                config_file = CONFIG_DIR / f\"{table_name}_validation.yml\"\n",
    "                \n",
    "                # Check if config exists\n",
    "                config_exists = file_exists_fabric(config_file) if IS_FABRIC else config_file.exists()\n",
    "                \n",
    "                if not config_exists:\n",
    "                    print(f\"\u26a0\ufe0f SKIPPED: {file_name} (no config)\")\n",
    "                    validation_results[\"summary\"][\"skipped\"] += 1\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    # Load validator and data\n",
    "                    validator = DataQualityValidator(config_path=str(config_file))\n",
    "                    \n",
    "                    # Use DataLoader for safe loading (handles sampling)\n",
    "                    df_batch = DataLoader.load_data(file_path, sample_size=SAMPLE_SIZE)\n",
    "                    result = validator.validate(df_batch)\n",
    "                    \n",
    "                    # Store results\n",
    "                    validation_results[\"files\"][table_name] = {\n",
    "                        \"overall_success\": result.get('success', False),\n",
    "                        \"success_percentage\": result.get('success_rate', 0.0),\n",
    "                        \"statistics\": {\n",
    "                            \"evaluated_expectations\": result.get('evaluated_checks', 0),\n",
    "                            \"successful_expectations\": result.get('successful_checks', 0)\n",
    "                        }\n",
    "                    }\n",
    "                    \n",
    "                    # Update summary\n",
    "                    if result.get('success', False):\n",
    "                        validation_results[\"summary\"][\"passed\"] += 1\n",
    "                        print(f\"\u2705 PASSED: {file_name} ({result.get('success_rate', 0):.1f}%)\")\n",
    "                        \n",
    "                        # Ingest to Silver layer\n",
    "                        silver_file = SILVER_DIR / f\"{table_name}.parquet\"\n",
    "                        \n",
    "                        if storage_manager is not None:\n",
    "                            try:\n",
    "                                storage_manager.write_to_silver(df_batch, table_name)\n",
    "                                print(f\"   \u2192 Ingested to Silver via StorageManager: {table_name}\")\n",
    "                            except Exception as sm_err:\n",
    "                                df_batch.to_parquet(str(silver_file), index=False, engine='pyarrow')\n",
    "                                print(f\"   \u2192 Ingested to Silver (fallback): {silver_file.name}\")\n",
    "                        else:\n",
    "                            df_batch.to_parquet(str(silver_file), index=False, engine='pyarrow')\n",
    "                            print(f\"   \u2192 Ingested to Silver: {silver_file.name}\")\n",
    "                        \n",
    "                        # Track for table persistence\n",
    "                        TABLES_TO_PERSIST.append(table_name)\n",
    "                            \n",
    "                    else:\n",
    "                        validation_results[\"summary\"][\"failed\"] += 1\n",
    "                        print(f\"\u274c FAILED: {file_name} ({result.get('success_rate', 0):.1f}%)\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    validation_results[\"summary\"][\"errors\"] += 1\n",
    "                    print(f\"\ud83d\udca5 ERROR: {file_name} - {e}\")\n",
    "            \n",
    "            # Save validation results using Fabric-compatible write\n",
    "            results_file = RESULTS_DIR / \"validation_results.json\"\n",
    "            write_json_file(results_file, validation_results)\n",
    "            \n",
    "            # Display summary\n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(\"VALIDATION SUMMARY\")\n",
    "            print(f\"{'='*70}\")\n",
    "            summary = validation_results[\"summary\"]\n",
    "            print(f\"Total Files:  {summary['total']}\")\n",
    "            print(f\"\u2705 Passed:     {summary['passed']}\")\n",
    "            print(f\"\u274c Failed:     {summary['failed']}\")\n",
    "            print(f\"\u26a0\ufe0f  Skipped:    {summary['skipped']}\")\n",
    "            print(f\"\ud83d\udca5 Errors:     {summary['errors']}\")\n",
    "            print(f\"\\nPass Rate: {(summary['passed']/max(summary['total'], 1)*100):.1f}%\")\n",
    "            print(f\"Results saved to: {results_file}\")\n",
    "            print(f\"{'='*70}\")\n",
    "            \n",
    "            # ================================================================\n",
    "            # PERSIST SILVER TO DELTA TABLES (Full Overwrite)\n",
    "            # ================================================================\n",
    "            if len(TABLES_TO_PERSIST) > 0:\n",
    "                print(f\"\\n{'='*70}\")\n",
    "                print(\"PERSISTING SILVER TO TABLES (Full Overwrite)\")\n",
    "                print(f\"{'='*70}\")\n",
    "                \n",
    "                tables_created = 0\n",
    "                tables_failed = 0\n",
    "                \n",
    "                if IS_FABRIC:\n",
    "                    # Use Spark to create Delta tables in Fabric\n",
    "                    try:\n",
    "                        from pyspark.sql import SparkSession\n",
    "                        spark = SparkSession.builder.getOrCreate()\n",
    "                        \n",
    "                        for table_name in TABLES_TO_PERSIST:\n",
    "                            try:\n",
    "                                # Build the Fabric-compatible path for Spark\n",
    "                                # Spark in Fabric needs relative path: Files/Silver/table.parquet\n",
    "                                silver_spark_path = f\"Files/Silver/{table_name}.parquet\"\n",
    "                                \n",
    "                                # Read parquet into Spark DataFrame using Fabric path\n",
    "                                spark_df = spark.read.parquet(silver_spark_path)\n",
    "                                \n",
    "                                # Write as managed Delta table with OVERWRITE mode\n",
    "                                # This drops and recreates the table with fresh data\n",
    "                                spark_df.write \\\n",
    "                                    .format(\"delta\") \\\n",
    "                                    .mode(\"overwrite\") \\\n",
    "                                    .option(\"overwriteSchema\", \"true\") \\\n",
    "                                    .saveAsTable(f\"silver_{table_name}\")\n",
    "                                \n",
    "                                print(f\"   \u2705 Table created: silver_{table_name} ({spark_df.count()} rows)\")\n",
    "                                tables_created += 1\n",
    "                                \n",
    "                            except Exception as table_err:\n",
    "                                print(f\"   \u274c Table failed: silver_{table_name} - {table_err}\")\n",
    "                                tables_failed += 1\n",
    "                        \n",
    "                    except ImportError as spark_err:\n",
    "                        print(f\"   \u26a0\ufe0f Spark not available: {spark_err}\")\n",
    "                        print(f\"   Tables not created - Silver parquet files are available\")\n",
    "                else:\n",
    "                    # Local environment - skip Delta table creation\n",
    "                    print(f\"   \u2139\ufe0f Local environment - Delta tables not created\")\n",
    "                    print(f\"   Silver parquet files available at: {SILVER_DIR}\")\n",
    "                    print(f\"   Tables to create on Fabric: {TABLES_TO_PERSIST}\")\n",
    "                \n",
    "                print(f\"\\n   \ud83d\udcca Table Persistence Summary:\")\n",
    "                print(f\"      Tables Created: {tables_created}\")\n",
    "                print(f\"      Tables Failed: {tables_failed}\")\n",
    "                print(f\"      Mode: FULL OVERWRITE (drop & reload)\")\n",
    "        \n",
    "        # Log phase execution\n",
    "        execution_log[\"phases\"].append({\n",
    "            \"phase\": \"validation_ingestion\",\n",
    "            \"status\": \"success\",\n",
    "            \"duration_seconds\": (datetime.now() - phase_start).total_seconds(),\n",
    "            \"validation_summary\": validation_results[\"summary\"],\n",
    "            \"tables_persisted\": len(TABLES_TO_PERSIST) if IS_FABRIC else 0\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n\u274c Validation/Ingestion Failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        execution_log[\"phases\"].append({\n",
    "            \"phase\": \"validation_ingestion\",\n",
    "            \"status\": \"failed\",\n",
    "            \"error\": str(e),\n",
    "            \"duration_seconds\": (datetime.now() - phase_start).total_seconds()\n",
    "        })\n",
    "        \n",
    "        if not PIPELINE_CONFIG.get(\"continue_on_error\", False):\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedc744b",
   "metadata": {},
   "source": [
    "## \ud83d\udcc8 Phase 3: Data Quality Monitoring\n",
    "\n",
    "**Purpose:** Generate DQ dashboards and monitoring reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e233ca06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T14:39:30.265256Z",
     "iopub.status.busy": "2025-12-10T14:39:30.265127Z",
     "iopub.status.idle": "2025-12-10T14:39:30.271767Z",
     "shell.execute_reply": "2025-12-10T14:39:30.271307Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 3: DATA QUALITY MONITORING\n",
      "================================================================================\n",
      "2026-01-19 13:10:58 | INFO     | orchestration | \u23f1\ufe0f Phase 3: DQ Monitoring...\n",
      "\n",
      "\ud83d\udcca Generating monitoring dashboards...\n",
      "   Data source: /home/sanmi/Documents/HS2/HS2_PROJECTS_2025/1_AIMS_LOCAL_2026/notebooks/config/validation_results/validation_results.json\n",
      "\n",
      "\ud83d\udccb DQ Summary:\n",
      "                       Table  Success % Status  Evaluated  Successful\n",
      "          aims_activitydates 100.000000 Passed         23          23\n",
      "        aims_assetattributes 100.000000 Passed        159         159\n",
      "   aims_assetclassattributes 100.000000 Passed         63          63\n",
      "   aims_assetclasschangelogs 100.000000 Passed         15          15\n",
      "           aims_assetclasses  99.137931 Passed        116         115\n",
      "aims_assetclassrelationships  91.489362 Passed         47          43\n",
      "          aims_assetconsents 100.000000 Passed         17          17\n",
      "      aims_assethierarchymap  92.857143 Failed         14          13\n",
      "         aims_assetlocations  97.000000 Failed        100          97\n",
      "                 aims_assets 100.000000 Passed         70          70\n",
      "\n",
      "\ud83d\udcca Key Metrics:\n",
      "   Average Quality Score: 98.8%\n",
      "   Pass Rate: 73.5%\n",
      "   Tables Monitored: 68\n",
      "2026-01-19 13:10:58 | INFO     | orchestration | \u23f1\ufe0f Phase 3: DQ Monitoring completed in 0.00s\n"
     ]
    }
   ],
   "source": [
    "if PIPELINE_CONFIG[\"run_monitoring\"]:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PHASE 3: DATA QUALITY MONITORING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    phase_start = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        with timed_operation(\"Phase 3: DQ Monitoring\", logger):\n",
    "            # Load validation results using Fabric-compatible read\n",
    "            results_file = RESULTS_DIR / \"validation_results.json\"\n",
    "            \n",
    "            # Check if results file exists\n",
    "            results_exist = False\n",
    "            if IS_FABRIC:\n",
    "                try:\n",
    "                    from notebookutils import mssparkutils\n",
    "                    fab_path = fabric_path(results_file)\n",
    "                    mssparkutils.fs.head(fab_path, 1)\n",
    "                    results_exist = True\n",
    "                except Exception:\n",
    "                    results_exist = False\n",
    "            else:\n",
    "                results_exist = results_file.exists()\n",
    "            \n",
    "            if not results_exist:\n",
    "                print(\"\u26a0\ufe0f No validation results found. Skipping monitoring.\")\n",
    "            else:\n",
    "                # Read validation results\n",
    "                if IS_FABRIC:\n",
    "                    from notebookutils import mssparkutils\n",
    "                    fab_path = fabric_path(results_file)\n",
    "                    content = mssparkutils.fs.head(fab_path, 1000000)\n",
    "                    validation_data = json.loads(content)\n",
    "                else:\n",
    "                    with open(results_file, 'r') as f:\n",
    "                        validation_data = json.load(f)\n",
    "                \n",
    "                print(f\"\\n\ud83d\udcca Generating monitoring dashboards...\")\n",
    "                print(f\"   Data source: {results_file}\")\n",
    "                \n",
    "                # Check if we have file results\n",
    "                files_data = validation_data.get(\"files\", {})\n",
    "                if not files_data:\n",
    "                    print(\"\u26a0\ufe0f No file validation results available. Run validation first.\")\n",
    "                    print(f\"\\n\ud83d\udcca Summary Statistics:\")\n",
    "                    summary = validation_data.get(\"summary\", {})\n",
    "                    print(f\"   Total Files: {summary.get('total', 0)}\")\n",
    "                    print(f\"   Passed: {summary.get('passed', 0)}\")\n",
    "                    print(f\"   Failed: {summary.get('failed', 0)}\")\n",
    "                    print(f\"   Skipped: {summary.get('skipped', 0)}\")\n",
    "                    print(f\"   Errors: {summary.get('errors', 0)}\")\n",
    "                else:\n",
    "                    # Create summary DataFrame\n",
    "                    summary_data = []\n",
    "                    for table_name, result in files_data.items():\n",
    "                        summary_data.append({\n",
    "                            \"Table\": table_name,\n",
    "                            \"Success %\": result.get(\"success_percentage\", 0),\n",
    "                            \"Status\": \"Passed\" if result.get(\"overall_success\") else \"Failed\",\n",
    "                            \"Evaluated\": result.get(\"statistics\", {}).get(\"evaluated_expectations\", 0),\n",
    "                            \"Successful\": result.get(\"statistics\", {}).get(\"successful_expectations\", 0)\n",
    "                        })\n",
    "                    \n",
    "                    df_summary = pd.DataFrame(summary_data)\n",
    "                    \n",
    "                    print(f\"\\n\ud83d\udccb DQ Summary:\")\n",
    "                    print(df_summary.head(10).to_string(index=False))\n",
    "                    \n",
    "                    # Calculate key metrics\n",
    "                    avg_quality = df_summary[\"Success %\"].mean()\n",
    "                    pass_rate = (df_summary[\"Status\"] == \"Passed\").sum() / len(df_summary) * 100\n",
    "                    \n",
    "                    print(f\"\\n\ud83d\udcca Key Metrics:\")\n",
    "                    print(f\"   Average Quality Score: {avg_quality:.1f}%\")\n",
    "                    print(f\"   Pass Rate: {pass_rate:.1f}%\")\n",
    "                    print(f\"   Tables Monitored: {len(df_summary)}\")\n",
    "                    \n",
    "                    # Log phase execution\n",
    "                    execution_log[\"phases\"].append({\n",
    "                        \"phase\": \"monitoring\",\n",
    "                        \"status\": \"success\",\n",
    "                        \"duration_seconds\": (datetime.now() - phase_start).total_seconds(),\n",
    "                        \"metrics\": {\n",
    "                            \"avg_quality_score\": float(avg_quality),\n",
    "                            \"pass_rate\": float(pass_rate),\n",
    "                            \"tables_monitored\": len(df_summary)\n",
    "                        }\n",
    "                    })\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"\\n\u274c Monitoring Failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        execution_log[\"phases\"].append({\n",
    "            \"phase\": \"monitoring\",\n",
    "            \"status\": \"failed\",\n",
    "            \"error\": str(e),\n",
    "            \"duration_seconds\": (datetime.now() - phase_start).total_seconds()\n",
    "        })\n",
    "        \n",
    "        if not PIPELINE_CONFIG.get(\"continue_on_error\", False):\n",
    "            raise\n",
    "else:\n",
    "    print(\"\u23ed\ufe0f Skipping Phase 3: Monitoring (disabled in config)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36083e7",
   "metadata": {},
   "source": [
    "## \ud83d\udcdd Pipeline Execution Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa189291",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T14:39:30.272982Z",
     "iopub.status.busy": "2025-12-10T14:39:30.272863Z",
     "iopub.status.idle": "2025-12-10T14:39:30.276944Z",
     "shell.execute_reply": "2025-12-10T14:39:30.276463Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\ud83d\udcca Overall Status:\n",
      "   Phases Completed: 3/3\n",
      "   Success Rate: 100.0%\n",
      "   Total Duration: 71.93s\n",
      "\n",
      "\ud83d\udcbe Execution log saved to: /home/sanmi/Documents/HS2/HS2_PROJECTS_2025/1_AIMS_LOCAL_2026/notebooks/config/validation_results/orchestration_log_20260119_131102.json\n",
      "\n",
      "================================================================================\n",
      "\ud83c\udf89 ALL PHASES COMPLETED SUCCESSFULLY!\n",
      "================================================================================\n",
      "2026-01-19 13:11:02 | INFO     | orchestration | Pipeline completed: 3/3 phases successful\n"
     ]
    }
   ],
   "source": [
    "# Calculate success rate\n",
    "successful_phases = sum(1 for p in execution_log[\"phases\"] if p[\"status\"] in [\"success\", \"partial\"])\n",
    "total_phases = len(execution_log[\"phases\"])\n",
    "success_rate = (successful_phases / total_phases * 100) if total_phases > 0 else 0\n",
    "\n",
    "# Finalize execution log\n",
    "execution_log[\"end_time\"] = datetime.now().isoformat()\n",
    "execution_log[\"total_duration_seconds\"] = sum(\n",
    "    p.get(\"duration_seconds\", 0) for p in execution_log[\"phases\"]\n",
    ")\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Pipeline Status (Phases 1-3):\")\n",
    "print(f\"   Phases Completed: {successful_phases}/{total_phases}\")\n",
    "print(f\"   Success Rate: {success_rate:.1f}%\")\n",
    "print(f\"   Total Duration: {execution_log['total_duration_seconds']:.2f}s\")\n",
    "\n",
    "# Determine if pipeline succeeded\n",
    "PIPELINE_SUCCESS = success_rate >= 80\n",
    "\n",
    "if PIPELINE_SUCCESS:\n",
    "    print(f\"\\n\u2705 Phases 1-3 PASSED - Ready for Phase 4 (Archive & Cleanup)\")\n",
    "else:\n",
    "    print(f\"\\n\u26a0\ufe0f Pipeline had issues - Phase 4 will be SKIPPED\")\n",
    "    print(f\"   Landing zone preserved for investigation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccee5780",
   "metadata": {},
   "source": [
    "## \ud83d\udce6 Phase 4: Archive & Cleanup\n",
    "\n",
    "**Purpose:** Archive processed files and clear landing zone (ONLY after successful pipeline)\n",
    "\n",
    "**Process:**\n",
    "1. Archive Bronze files to `/archive/YYYYMMDD_HHMMSS/`\n",
    "2. Clear landing zone (ready for next SFTP batch)\n",
    "3. Save final execution log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381eaada",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 4: ARCHIVE & CLEANUP\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "phase_start = datetime.now()\n",
    "\n",
    "if not PIPELINE_SUCCESS:\n",
    "    print(\"\\n\u26a0\ufe0f SKIPPING Phase 4 - Pipeline did not complete successfully\")\n",
    "    print(\"   Landing zone preserved for investigation/retry\")\n",
    "    print(\"   Fix issues and re-run the pipeline\")\n",
    "    \n",
    "    execution_log[\"phases\"].append({\n",
    "        \"phase\": \"archive_cleanup\",\n",
    "        \"status\": \"skipped\",\n",
    "        \"reason\": \"Pipeline success rate below threshold\",\n",
    "        \"duration_seconds\": 0\n",
    "    })\n",
    "else:\n",
    "    try:\n",
    "        # Create dated archive folder\n",
    "        archive_date = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        ARCHIVE_BATCH_DIR = ARCHIVE_DIR / archive_date\n",
    "        ensure_dir_exists(ARCHIVE_BATCH_DIR)\n",
    "        \n",
    "        print(f\"\\n\ud83d\udce6 Archiving to: {ARCHIVE_BATCH_DIR}\")\n",
    "        \n",
    "        archived_count = 0\n",
    "        cleared_count = 0\n",
    "        \n",
    "        # Step 1: Archive files from landing to archive folder\n",
    "        if len(LANDING_FILES_TO_ARCHIVE) > 0:\n",
    "            print(f\"\\n\ud83d\udccb Archiving {len(LANDING_FILES_TO_ARCHIVE)} files from landing...\")\n",
    "            \n",
    "            for filename in LANDING_FILES_TO_ARCHIVE:\n",
    "                if IS_FABRIC:\n",
    "                    src_path = f\"{LANDING_DIR}/{filename}\"\n",
    "                    archive_path = f\"{ARCHIVE_BATCH_DIR}/{filename}\"\n",
    "                else:\n",
    "                    src_path = LANDING_DIR / filename\n",
    "                    archive_path = ARCHIVE_BATCH_DIR / filename\n",
    "                \n",
    "                try:\n",
    "                    # Copy to archive\n",
    "                    copy_file_fabric(src_path, archive_path)\n",
    "                    archived_count += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"   \u26a0\ufe0f Archive failed: {filename} - {e}\")\n",
    "            \n",
    "            print(f\"   \u2705 Archived {archived_count}/{len(LANDING_FILES_TO_ARCHIVE)} files\")\n",
    "        \n",
    "        # Step 2: Clear landing zone (only after successful archive)\n",
    "        if archived_count == len(LANDING_FILES_TO_ARCHIVE) and archived_count > 0:\n",
    "            print(f\"\\n\ud83e\uddf9 Clearing landing zone...\")\n",
    "            \n",
    "            for filename in LANDING_FILES_TO_ARCHIVE:\n",
    "                if IS_FABRIC:\n",
    "                    src_path = f\"{LANDING_DIR}/{filename}\"\n",
    "                else:\n",
    "                    src_path = LANDING_DIR / filename\n",
    "                \n",
    "                try:\n",
    "                    delete_file_fabric(src_path)\n",
    "                    cleared_count += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"   \u26a0\ufe0f Delete failed: {filename} - {e}\")\n",
    "            \n",
    "            print(f\"   \u2705 Cleared {cleared_count}/{len(LANDING_FILES_TO_ARCHIVE)} files from landing\")\n",
    "        else:\n",
    "            print(f\"\\n\u26a0\ufe0f Skipping landing cleanup - archive incomplete\")\n",
    "        \n",
    "        # Verify landing is empty\n",
    "        remaining = list_parquet_files(LANDING_DIR)\n",
    "        if len(remaining) == 0:\n",
    "            print(f\"\\n   \u2705 Landing zone is now EMPTY (ready for next SFTP batch)\")\n",
    "        else:\n",
    "            print(f\"\\n   \u26a0\ufe0f {len(remaining)} files still in landing\")\n",
    "        \n",
    "        # Step 3: Save manifest to archive\n",
    "        manifest = {\n",
    "            \"archive_date\": archive_date,\n",
    "            \"pipeline_run\": execution_log[\"start_time\"],\n",
    "            \"files_archived\": LANDING_FILES_TO_ARCHIVE,\n",
    "            \"validation_summary\": execution_log.get(\"phases\", [{}])[-1].get(\"validation_summary\", {}),\n",
    "            \"success_rate\": success_rate\n",
    "        }\n",
    "        \n",
    "        manifest_path = ARCHIVE_BATCH_DIR / \"manifest.json\"\n",
    "        manifest_content = json.dumps(manifest, indent=2)\n",
    "        \n",
    "        if IS_FABRIC:\n",
    "            from notebookutils import mssparkutils\n",
    "            mssparkutils.fs.put(fabric_path(manifest_path), manifest_content, overwrite=True)\n",
    "        else:\n",
    "            with open(manifest_path, 'w') as f:\n",
    "                f.write(manifest_content)\n",
    "        \n",
    "        print(f\"   \ud83d\udcdd Manifest saved: {manifest_path}\")\n",
    "        \n",
    "        # Log phase execution\n",
    "        execution_log[\"phases\"].append({\n",
    "            \"phase\": \"archive_cleanup\",\n",
    "            \"status\": \"success\",\n",
    "            \"duration_seconds\": (datetime.now() - phase_start).total_seconds(),\n",
    "            \"files_archived\": archived_count,\n",
    "            \"files_cleared\": cleared_count,\n",
    "            \"archive_location\": str(ARCHIVE_BATCH_DIR)\n",
    "        })\n",
    "        \n",
    "        print(f\"\\n   \ud83d\udcca Phase 4 Summary:\")\n",
    "        print(f\"      Files Archived: {archived_count}\")\n",
    "        print(f\"      Landing Cleared: {cleared_count}\")\n",
    "        print(f\"      Archive Location: {ARCHIVE_BATCH_DIR}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n\u274c Archive/Cleanup Failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        execution_log[\"phases\"].append({\n",
    "            \"phase\": \"archive_cleanup\",\n",
    "            \"status\": \"failed\",\n",
    "            \"error\": str(e),\n",
    "            \"duration_seconds\": (datetime.now() - phase_start).total_seconds()\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcacdf5a",
   "metadata": {},
   "source": [
    "## \u2705 Pipeline Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26471c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Pipeline Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PIPELINE EXECUTION COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Recalculate with all phases including Phase 4\n",
    "successful_phases = sum(1 for p in execution_log[\"phases\"] if p[\"status\"] in [\"success\", \"partial\"])\n",
    "total_phases = len(execution_log[\"phases\"])\n",
    "final_success_rate = (successful_phases / total_phases * 100) if total_phases > 0 else 0\n",
    "\n",
    "execution_log[\"end_time\"] = datetime.now().isoformat()\n",
    "execution_log[\"total_duration_seconds\"] = sum(\n",
    "    p.get(\"duration_seconds\", 0) for p in execution_log[\"phases\"]\n",
    ")\n",
    "execution_log[\"final_success_rate\"] = final_success_rate\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Final Status:\")\n",
    "print(f\"   Phases Completed: {successful_phases}/{total_phases}\")\n",
    "print(f\"   Success Rate: {final_success_rate:.1f}%\")\n",
    "print(f\"   Total Duration: {execution_log['total_duration_seconds']:.2f}s\")\n",
    "\n",
    "# Save final execution log\n",
    "log_file = RESULTS_DIR / f\"orchestration_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "log_content = json.dumps(execution_log, indent=2)\n",
    "\n",
    "if IS_FABRIC:\n",
    "    try:\n",
    "        from notebookutils import mssparkutils\n",
    "        fab_path = fabric_path(log_file)\n",
    "        mssparkutils.fs.put(fab_path, log_content, overwrite=True)\n",
    "    except Exception as e:\n",
    "        print(f\"   \u26a0\ufe0f Could not save log to Fabric: {e}\")\n",
    "else:\n",
    "    with open(log_file, 'w') as f:\n",
    "        f.write(log_content)\n",
    "\n",
    "print(f\"\\n\ud83d\udcbe Execution log saved to: {log_file}\")\n",
    "\n",
    "# Final state summary\n",
    "print(f\"\\n\ud83d\udcc1 Final Directory State:\")\n",
    "landing_count = len(list_parquet_files(LANDING_DIR))\n",
    "bronze_count = len(list_parquet_files(BRONZE_DIR))\n",
    "silver_count = len(list_parquet_files(SILVER_DIR))\n",
    "\n",
    "print(f\"   /landing/  \u2192 {landing_count} files {'(EMPTY - ready for SFTP)' if landing_count == 0 else '(\u26a0\ufe0f not cleared)'}\")\n",
    "print(f\"   /Bronze/   \u2192 {bronze_count} files (raw data)\")\n",
    "print(f\"   /Silver/   \u2192 {silver_count} files (validated parquet)\")\n",
    "print(f\"   /archive/  \u2192 Historical backups with timestamps\")\n",
    "\n",
    "# Show Delta tables status\n",
    "if IS_FABRIC:\n",
    "    print(f\"\\n\ud83d\udcca Delta Tables (Lakehouse):\")\n",
    "    try:\n",
    "        tables_persisted = [t for t in TABLES_TO_PERSIST] if 'TABLES_TO_PERSIST' in dir() else []\n",
    "        if tables_persisted:\n",
    "            for t in tables_persisted:\n",
    "                print(f\"   \u2705 silver_{t} (OVERWRITTEN)\")\n",
    "        else:\n",
    "            print(f\"   \u2139\ufe0f No tables persisted this run\")\n",
    "    except NameError:\n",
    "        print(f\"   \u2139\ufe0f Table info not available\")\n",
    "else:\n",
    "    print(f\"\\n\ud83d\udcca Delta Tables:\")\n",
    "    print(f\"   \u2139\ufe0f Local mode - tables created on Fabric deployment\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "if final_success_rate == 100:\n",
    "    print(\"\ud83c\udf89 ALL PHASES COMPLETED SUCCESSFULLY!\")\n",
    "elif final_success_rate >= 80:\n",
    "    print(\"\u2705 PIPELINE COMPLETED WITH MINOR ISSUES\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f PIPELINE COMPLETED WITH ERRORS - Check logs\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Log final summary if logger available\n",
    "if logger:\n",
    "    logger.info(f\"Pipeline completed: {successful_phases}/{total_phases} phases, {final_success_rate:.1f}% success\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aims_data_platform",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}