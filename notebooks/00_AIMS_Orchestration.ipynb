{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ec5da61",
   "metadata": {},
   "source": [
    "## üîß Environment Detection & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "963915de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T14:39:26.525995Z",
     "iopub.status.busy": "2025-12-10T14:39:26.525873Z",
     "iopub.status.idle": "2025-12-10T14:39:26.531081Z",
     "shell.execute_reply": "2025-12-10T14:39:26.530616Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíª Running Locally\n",
      "\n",
      "‚úÖ Environment Detection Complete\n"
     ]
    }
   ],
   "source": [
    "# --- ENVIRONMENT DETECTION & VALIDATION ---\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Detect Environment\n",
    "try:\n",
    "    from notebookutils import mssparkutils\n",
    "    IS_FABRIC = True\n",
    "    print(\"üåê Running in Microsoft Fabric\")\n",
    "except ImportError:\n",
    "    IS_FABRIC = False\n",
    "    print(\"üíª Running Locally\")\n",
    "\n",
    "# Fabric-Specific Validation\n",
    "if IS_FABRIC:\n",
    "    # Validate Lakehouse attachment\n",
    "    try:\n",
    "        workspace_id = mssparkutils.env.getWorkspaceId()\n",
    "        print(f\"‚úÖ Workspace ID: {workspace_id}\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\n",
    "            \"‚ùå No Lakehouse attached to this notebook!\\n\"\n",
    "            \"Please attach a Lakehouse: Notebook toolbar > Add Lakehouse > Select your lakehouse\\n\"\n",
    "            f\"Error: {e}\"\n",
    "        )\n",
    "\n",
    "print(f\"\\n‚úÖ Environment Detection Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ed394d",
   "metadata": {},
   "source": [
    "## üì¶ Package Installation & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b587d802",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T14:39:26.532895Z",
     "iopub.status.busy": "2025-12-10T14:39:26.532646Z",
     "iopub.status.idle": "2025-12-10T14:39:30.234809Z",
     "shell.execute_reply": "2025-12-10T14:39:30.234131Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Base Directory: /home/sanmi/Documents/HS2/HS2_PROJECTS_2025/1_AIMS_LOCAL_2026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ dq_framework already installed\n",
      "‚úÖ Loaded .env from local filesystem\n",
      "\n",
      "‚úÖ Package Installation Complete\n"
     ]
    }
   ],
   "source": [
    "# --- PACKAGE INSTALLATION ---\n",
    "\n",
    "# Set base directory first\n",
    "if IS_FABRIC:\n",
    "    BASE_DIR = Path(\"/lakehouse/default/Files\")\n",
    "else:\n",
    "    # Local: Use project root (parent of notebooks directory)\n",
    "    current_dir = Path.cwd()\n",
    "    if current_dir.name == \"notebooks\":\n",
    "        BASE_DIR = current_dir.parent\n",
    "    else:\n",
    "        BASE_DIR = current_dir\n",
    "\n",
    "print(f\"üìÇ Base Directory: {BASE_DIR}\")\n",
    "\n",
    "# Install dq_framework package if needed\n",
    "try:\n",
    "    import dq_framework\n",
    "    print(f\"‚úÖ dq_framework already installed\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è dq_framework not found. Installing...\")\n",
    "    \n",
    "    if IS_FABRIC:\n",
    "        # Try to install from Lakehouse Files/libs/\n",
    "        wheel_path = BASE_DIR / \"libs/fabric_data_quality-1.2.0-py3-none-any.whl\"\n",
    "        \n",
    "        if wheel_path.exists():\n",
    "            print(f\"üì¶ Installing from: {wheel_path}\")\n",
    "            %pip install {str(wheel_path)} --quiet\n",
    "            print(\"‚úÖ Package installed successfully\")\n",
    "        else:\n",
    "            raise FileNotFoundError(\n",
    "                f\"‚ùå Wheel file not found at: {wheel_path}\\n\"\n",
    "                f\"Please upload fabric_data_quality-1.2.0-py3-none-any.whl to Lakehouse Files/libs/\\n\"\n",
    "                f\"Or install via Fabric Environment in Workspace Settings\"\n",
    "            )\n",
    "    else:\n",
    "        # Local: Install from local dist or editable install\n",
    "        print(\"üì¶ Installing locally (editable mode)...\")\n",
    "        %pip install -e {str(BASE_DIR)} --quiet\n",
    "        print(\"‚úÖ Package installed\")\n",
    "\n",
    "# Import required modules\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "\n",
    "# Load environment variables\n",
    "if IS_FABRIC:\n",
    "    # Try to load .env from Lakehouse if it exists\n",
    "    env_path = BASE_DIR / \".env\"\n",
    "    if env_path.exists():\n",
    "        load_dotenv(dotenv_path=env_path)\n",
    "        print(\"‚úÖ Loaded .env from Lakehouse\")\n",
    "else:\n",
    "    load_dotenv()\n",
    "    print(\"‚úÖ Loaded .env from local filesystem\")\n",
    "\n",
    "print(\"\\n‚úÖ Package Installation Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1ba7b2",
   "metadata": {},
   "source": [
    "## üìÇ Path Configuration & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fe98f91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T14:39:30.236118Z",
     "iopub.status.busy": "2025-12-10T14:39:30.235989Z",
     "iopub.status.idle": "2025-12-10T14:39:30.240976Z",
     "shell.execute_reply": "2025-12-10T14:39:30.240605Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÇ Configuration:\n",
      "   Environment: Local\n",
      "   Base Directory: /home/sanmi/Documents/HS2/HS2_PROJECTS_2025/1_AIMS_LOCAL_2026\n",
      "   Bronze Layer: /home/sanmi/Documents/HS2/HS2_PROJECTS_2025/1_AIMS_LOCAL_2026/data/Samples_LH_Bronze_Aims_26_parquet\n",
      "   Silver Layer: /home/sanmi/Documents/HS2/HS2_PROJECTS_2025/1_AIMS_LOCAL_2026/data/Silver\n",
      "   Gold Layer: /home/sanmi/Documents/HS2/HS2_PROJECTS_2025/1_AIMS_LOCAL_2026/data/Gold\n",
      "   Config Directory: /home/sanmi/Documents/HS2/HS2_PROJECTS_2025/1_AIMS_LOCAL_2026/config/data_quality\n",
      "   Results Directory: /home/sanmi/Documents/HS2/HS2_PROJECTS_2025/1_AIMS_LOCAL_2026/config/validation_results\n",
      "   Storage Format: parquet\n",
      "\n",
      "‚úÖ Found 68 Bronze parquet files\n",
      "‚úÖ Path Validation Complete\n"
     ]
    }
   ],
   "source": [
    "# --- PATH CONFIGURATION ---\n",
    "\n",
    "# Configure Paths based on Environment\n",
    "if IS_FABRIC:\n",
    "    # Fabric: Use Lakehouse Paths\n",
    "    BRONZE_DIR = BASE_DIR / \"data/Samples_LH_Bronze_Aims_26_parquet\"\n",
    "    SILVER_DIR = BASE_DIR / \"data/Silver\"\n",
    "    GOLD_DIR = BASE_DIR / \"data/Gold\"\n",
    "    CONFIG_DIR = BASE_DIR / \"config/data_quality\"\n",
    "    RESULTS_DIR = BASE_DIR / \"config/validation_results\"\n",
    "    NOTEBOOK_DIR = BASE_DIR / \"notebooks\"\n",
    "    \n",
    "    # Storage format\n",
    "    STORAGE_FORMAT = \"delta\"  # Use Delta Lake in Fabric\n",
    "else:\n",
    "    # Local: Use environment variables or defaults\n",
    "    BRONZE_DIR = BASE_DIR / \"data/Samples_LH_Bronze_Aims_26_parquet\"\n",
    "    SILVER_DIR = BASE_DIR / \"data/Silver\"\n",
    "    GOLD_DIR = BASE_DIR / \"data/Gold\"\n",
    "    CONFIG_DIR = BASE_DIR / \"config/data_quality\"\n",
    "    RESULTS_DIR = BASE_DIR / \"config/validation_results\"\n",
    "    NOTEBOOK_DIR = BASE_DIR / \"notebooks\"\n",
    "    \n",
    "    # Storage format\n",
    "    STORAGE_FORMAT = \"parquet\"  # Use Parquet locally\n",
    "\n",
    "# Ensure directories exist\n",
    "for directory in [SILVER_DIR, GOLD_DIR, CONFIG_DIR, RESULTS_DIR]:\n",
    "    directory.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(f\"\\nüìÇ Configuration:\")\n",
    "print(f\"   Environment: {'Fabric' if IS_FABRIC else 'Local'}\")\n",
    "print(f\"   Base Directory: {BASE_DIR}\")\n",
    "print(f\"   Bronze Layer: {BRONZE_DIR}\")\n",
    "print(f\"   Silver Layer: {SILVER_DIR}\")\n",
    "print(f\"   Gold Layer: {GOLD_DIR}\")\n",
    "print(f\"   Config Directory: {CONFIG_DIR}\")\n",
    "print(f\"   Results Directory: {RESULTS_DIR}\")\n",
    "print(f\"   Storage Format: {STORAGE_FORMAT}\")\n",
    "\n",
    "# Validate Bronze data exists\n",
    "if not BRONZE_DIR.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"‚ùå Bronze data directory not found!\\n\"\n",
    "        f\"Expected location: {BRONZE_DIR}\\n\"\n",
    "        f\"{'Please upload parquet files to Lakehouse Files/data/Samples_LH_Bronze_Aims_26_parquet/' if IS_FABRIC else 'Please check your data directory path'}\"\n",
    "    )\n",
    "\n",
    "# Count Bronze files\n",
    "parquet_files = list(BRONZE_DIR.glob(\"*.parquet\"))\n",
    "if len(parquet_files) == 0:\n",
    "    raise FileNotFoundError(\n",
    "        f\"‚ùå No parquet files found in {BRONZE_DIR}\\n\"\n",
    "        f\"Expected: 68 parquet files\"\n",
    "    )\n",
    "\n",
    "print(f\"\\n‚úÖ Found {len(parquet_files)} Bronze parquet files\")\n",
    "print(f\"‚úÖ Path Validation Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ec081f",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Pipeline Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d82a4363",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T14:39:30.242089Z",
     "iopub.status.busy": "2025-12-10T14:39:30.241976Z",
     "iopub.status.idle": "2025-12-10T14:39:30.245127Z",
     "shell.execute_reply": "2025-12-10T14:39:30.244662Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Pipeline Configuration:\n",
      "   run_profiling: True\n",
      "   run_ingestion: True\n",
      "   run_monitoring: True\n",
      "   run_dq_modeling: False\n",
      "   run_bi_analytics: False\n",
      "   force_reprocess: False\n",
      "   dq_threshold: 85.0\n",
      "   max_workers: 4\n",
      "   continue_on_error: False\n",
      "\n",
      "‚úÖ Configuration Complete\n"
     ]
    }
   ],
   "source": [
    "PIPELINE_CONFIG = {\n",
    "    \"run_profiling\": True,          # Phase 1: Generate DQ configs\n",
    "    \"run_ingestion\": True,          # Phase 2: Bronze ‚Üí Silver with validation\n",
    "    \"run_monitoring\": True,         # Phase 3: DQ monitoring dashboards\n",
    "    \"run_dq_modeling\": False,       # Phase 4: Advanced DQ modeling (optional)\n",
    "    \"run_bi_analytics\": False,      # Phase 5: BI analytics (optional)\n",
    "    \"force_reprocess\": False,       # Force reprocessing even if files exist\n",
    "    \"dq_threshold\": 85.0,          # Global DQ threshold (85%)\n",
    "    \"max_workers\": 8 if IS_FABRIC else 4,  # Parallel processing workers\n",
    "    \"continue_on_error\": False,    # Continue pipeline even if phase fails\n",
    "}\n",
    "\n",
    "# Display Configuration\n",
    "print(\"‚öôÔ∏è Pipeline Configuration:\")\n",
    "for key, value in PIPELINE_CONFIG.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "# Initialize Execution Log\n",
    "execution_log = {\n",
    "    \"start_time\": datetime.now().isoformat(),\n",
    "    \"environment\": \"Fabric\" if IS_FABRIC else \"Local\",\n",
    "    \"storage_format\": STORAGE_FORMAT,\n",
    "    \"config\": PIPELINE_CONFIG,\n",
    "    \"phases\": []\n",
    "}\n",
    "\n",
    "print(\"\\n‚úÖ Configuration Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6105ff7",
   "metadata": {},
   "source": [
    "## üöÄ Phase 1: Data Profiling\n",
    "\n",
    "**Purpose:** Generate DQ validation configs for all Bronze layer tables\n",
    "\n",
    "**Process:**\n",
    "1. Profile each Bronze parquet file\n",
    "2. Generate validation YAML configs\n",
    "3. Save configs to `config/data_quality/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f200ba33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T14:39:30.246344Z",
     "iopub.status.busy": "2025-12-10T14:39:30.246221Z",
     "iopub.status.idle": "2025-12-10T14:39:30.254000Z",
     "shell.execute_reply": "2025-12-10T14:39:30.253498Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 1: DATA PROFILING\n",
      "================================================================================\n",
      "\n",
      "üìä Profiling Bronze layer: /home/sanmi/Documents/HS2/HS2_PROJECTS_2025/1_AIMS_LOCAL_2026/data/Samples_LH_Bronze_Aims_26_parquet\n",
      "   Workers: 4\n",
      "   Output: /home/sanmi/Documents/HS2/HS2_PROJECTS_2025/1_AIMS_LOCAL_2026/config/data_quality\n",
      "   Profiling: aims_activitydates.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_assetattributes.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_assetclassattributes.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_assetclasschangelogs.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_assetclasses.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_assetclassrelationships.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_assetconsents.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_assethierarchymap.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_assetlocations.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_assets.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_attributedomains.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_attributedomainvalues.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_attributegroups.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_attributes.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_consentlinks.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_consentmilestones.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_consentmilestonetypes.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_consents.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_consenttypemilestones.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_consenttypes.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_informationneedassetclass.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_informationneedattributes.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_informationneeddocs.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_informationneedgeometries.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_informationneedlinks.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_informationneedpropchngs.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_informationneeds.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_informationneedsourcedocs.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_informationneedstatusupd.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_informationpackages.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_links.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_linktypes.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_noncompliances.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_organisations.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_owners.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_people.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_phases.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_productassetclasses.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_productcharacteristics.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_productlinks.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_products.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_projectitemactions.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_projectitemassignedroles.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_projectitemattributes.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_projectitemlinks.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_projectitems.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_relationships.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_relationshiptypes.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_routes.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_secondaryassetclasscodes.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_stages.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_taskdefinitions.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_tracks.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_ua_beneficiaries.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_ua_comments.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_ua_entities.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_ua_meetingattendees.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_ua_meetings.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_ua_noncompimppartytypes.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_ua_noncomplianceimpacts.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_ua_noncompotheruas.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_ua_optionvalues.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_undertakings_assurances.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_workbanks.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_workbankworkorders.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_workorderattributes.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_workorders.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "   Profiling: aims_workorderstatustransition.parquet... ‚ùå Error: 'DataProfiler' object has no attribute 'generate_validation_config'\n",
      "\n",
      "‚úÖ Profiling Complete:\n",
      "   Files Profiled: 0\n",
      "   Configs Generated: 68\n",
      "   Errors: 68\n"
     ]
    }
   ],
   "source": [
    "if PIPELINE_CONFIG[\"run_profiling\"]:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PHASE 1: DATA PROFILING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    phase_start = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        # Add project root to path for imports\n",
    "        if str(BASE_DIR) not in sys.path:\n",
    "            sys.path.insert(0, str(BASE_DIR))\n",
    "        \n",
    "        # Import profiling modules\n",
    "        from dq_framework import DataProfiler\n",
    "        \n",
    "        print(f\"\\nüìä Profiling Bronze layer: {BRONZE_DIR}\")\n",
    "        print(f\"   Workers: {PIPELINE_CONFIG['max_workers']}\")\n",
    "        print(f\"   Output: {CONFIG_DIR}\")\n",
    "        \n",
    "        # Profile each parquet file\n",
    "        profiled_files = []\n",
    "        errors = []\n",
    "        \n",
    "        for parquet_file in sorted(parquet_files):\n",
    "            table_name = parquet_file.stem\n",
    "            config_file = CONFIG_DIR / f\"{table_name}_validation.yml\"\n",
    "            \n",
    "            try:\n",
    "                print(f\"   Profiling: {table_name}.parquet...\", end=\" \")\n",
    "                \n",
    "                # Profile and generate config\n",
    "                profiler = DataProfiler(str(parquet_file))\n",
    "                config = profiler.generate_validation_config()\n",
    "                \n",
    "                # Save config\n",
    "                with open(config_file, 'w') as f:\n",
    "                    import yaml\n",
    "                    yaml.dump(config, f, default_flow_style=False)\n",
    "                \n",
    "                profiled_files.append(table_name)\n",
    "                print(\"‚úÖ\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                errors.append({\"table\": table_name, \"error\": str(e)})\n",
    "                print(f\"‚ùå Error: {e}\")\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"\\n‚úÖ Profiling Complete:\")\n",
    "        print(f\"   Files Profiled: {len(profiled_files)}\")\n",
    "        print(f\"   Configs Generated: {len(list(CONFIG_DIR.glob('*.yml')))}\")\n",
    "        if errors:\n",
    "            print(f\"   Errors: {len(errors)}\")\n",
    "        \n",
    "        # Log phase execution\n",
    "        execution_log[\"phases\"].append({\n",
    "            \"phase\": \"profiling\",\n",
    "            \"status\": \"success\" if len(errors) == 0 else \"partial\",\n",
    "            \"duration_seconds\": (datetime.now() - phase_start).total_seconds(),\n",
    "            \"files_profiled\": len(profiled_files),\n",
    "            \"configs_generated\": len(list(CONFIG_DIR.glob('*.yml'))),\n",
    "            \"errors\": len(errors)\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Profiling Failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        execution_log[\"phases\"].append({\n",
    "            \"phase\": \"profiling\",\n",
    "            \"status\": \"failed\",\n",
    "            \"error\": str(e),\n",
    "            \"duration_seconds\": (datetime.now() - phase_start).total_seconds()\n",
    "        })\n",
    "        \n",
    "        if not PIPELINE_CONFIG.get(\"continue_on_error\", False):\n",
    "            raise\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping Phase 1: Data Profiling (disabled in config)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212c8a76",
   "metadata": {},
   "source": [
    "## ‚úÖ Phase 2: Data Validation & Ingestion\n",
    "\n",
    "**Purpose:** Validate Bronze data and ingest to Silver layer\n",
    "\n",
    "**Process:**\n",
    "1. Load validation configs\n",
    "2. Validate each Bronze table\n",
    "3. Ingest passing records to Silver (Delta Lake in Fabric, Parquet locally)\n",
    "4. Quarantine failing records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a98c6fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T14:39:30.255285Z",
     "iopub.status.busy": "2025-12-10T14:39:30.255153Z",
     "iopub.status.idle": "2025-12-10T14:39:30.264057Z",
     "shell.execute_reply": "2025-12-10T14:39:30.263509Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 2: DATA VALIDATION & INGESTION\n",
      "================================================================================\n",
      "   Found 68 parquet files to validate\n",
      "\n",
      "üí• ERROR: aims_activitydates.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_assetattributes.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_assetclassattributes.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_assetclasschangelogs.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_assetclasses.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_assetclassrelationships.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_assetconsents.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_assethierarchymap.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_assetlocations.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_assets.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_attributedomains.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_attributedomainvalues.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_attributegroups.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_attributes.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_consentlinks.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_consentmilestones.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_consentmilestonetypes.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_consents.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_consenttypemilestones.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_consenttypes.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_informationneedassetclass.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_informationneedattributes.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_informationneeddocs.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_informationneedgeometries.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_informationneedlinks.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_informationneedpropchngs.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_informationneeds.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_informationneedsourcedocs.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_informationneedstatusupd.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_informationpackages.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_links.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_linktypes.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_noncompliances.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_organisations.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_owners.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_people.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_phases.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_productassetclasses.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_productcharacteristics.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_productlinks.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_products.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_projectitemactions.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_projectitemassignedroles.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_projectitemattributes.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_projectitemlinks.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_projectitems.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_relationships.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_relationshiptypes.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_routes.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_secondaryassetclasscodes.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_stages.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_taskdefinitions.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_tracks.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_ua_beneficiaries.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_ua_comments.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_ua_entities.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_ua_meetingattendees.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_ua_meetings.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_ua_noncompimppartytypes.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_ua_noncomplianceimpacts.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_ua_noncompotheruas.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_ua_optionvalues.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_undertakings_assurances.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_workbanks.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_workbankworkorders.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_workorderattributes.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_workorders.parquet - name 'DataValidator' is not defined\n",
      "üí• ERROR: aims_workorderstatustransition.parquet - name 'DataValidator' is not defined\n",
      "\n",
      "======================================================================\n",
      "VALIDATION SUMMARY\n",
      "======================================================================\n",
      "Total Files:  68\n",
      "‚úÖ Passed:     0\n",
      "‚ùå Failed:     0\n",
      "‚ö†Ô∏è  Skipped:    0\n",
      "üí• Errors:     68\n",
      "\n",
      "Pass Rate: 0.0%\n",
      "Results saved to: /home/sanmi/Documents/HS2/HS2_PROJECTS_2025/1_AIMS_LOCAL_2026/config/validation_results/validation_results.json\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "if PIPELINE_CONFIG[\"run_ingestion\"]:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PHASE 2: DATA VALIDATION & INGESTION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    phase_start = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        # Track validation results\n",
    "        validation_results = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"threshold\": PIPELINE_CONFIG['dq_threshold'],\n",
    "            \"storage_format\": STORAGE_FORMAT,\n",
    "            \"files\": {},\n",
    "            \"summary\": {\"total\": 0, \"passed\": 0, \"failed\": 0, \"skipped\": 0, \"errors\": 0}\n",
    "        }\n",
    "        \n",
    "        validation_results[\"summary\"][\"total\"] = len(parquet_files)\n",
    "        print(f\"   Found {len(parquet_files)} parquet files to validate\\n\")\n",
    "        \n",
    "        # Validate each file\n",
    "        for parquet_file in sorted(parquet_files):\n",
    "            table_name = parquet_file.stem\n",
    "            config_file = CONFIG_DIR / f\"{table_name}_validation.yml\"\n",
    "            \n",
    "            if not config_file.exists():\n",
    "                print(f\"‚ö†Ô∏è SKIPPED: {table_name}.parquet (no config)\")\n",
    "                validation_results[\"summary\"][\"skipped\"] += 1\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Load and validate\n",
    "                validator = DataValidator(str(config_file))\n",
    "                result = validator.validate(str(parquet_file))\n",
    "                \n",
    "                # Store results\n",
    "                validation_results[\"files\"][table_name] = result\n",
    "                \n",
    "                # Update summary\n",
    "                if result.get(\"overall_success\", False):\n",
    "                    validation_results[\"summary\"][\"passed\"] += 1\n",
    "                    print(f\"‚úÖ PASSED: {table_name}.parquet ({result.get('success_percentage', 0):.1f}%)\")\n",
    "                    \n",
    "                    # Ingest to Silver layer (pandas + parquet works in both Local and Fabric)\n",
    "                    df = pd.read_parquet(parquet_file)\n",
    "                    silver_file = SILVER_DIR / f\"{table_name}.parquet\"\n",
    "                    df.to_parquet(silver_file, index=False)\n",
    "                    \n",
    "                else:\n",
    "                    validation_results[\"summary\"][\"failed\"] += 1\n",
    "                    print(f\"‚ùå FAILED: {table_name}.parquet ({result.get('success_percentage', 0):.1f}%)\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                validation_results[\"summary\"][\"errors\"] += 1\n",
    "                print(f\"üí• ERROR: {table_name}.parquet - {e}\")\n",
    "        \n",
    "        # Save validation results\n",
    "        results_file = RESULTS_DIR / \"validation_results.json\"\n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump(validation_results, f, indent=2)\n",
    "        \n",
    "        # Display summary\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"VALIDATION SUMMARY\")\n",
    "        print(f\"{'='*70}\")\n",
    "        summary = validation_results[\"summary\"]\n",
    "        print(f\"Total Files:  {summary['total']}\")\n",
    "        print(f\"‚úÖ Passed:     {summary['passed']}\")\n",
    "        print(f\"‚ùå Failed:     {summary['failed']}\")\n",
    "        print(f\"‚ö†Ô∏è  Skipped:    {summary['skipped']}\")\n",
    "        print(f\"üí• Errors:     {summary['errors']}\")\n",
    "        print(f\"\\nPass Rate: {(summary['passed']/summary['total']*100):.1f}%\")\n",
    "        print(f\"Results saved to: {results_file}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Log phase execution\n",
    "        execution_log[\"phases\"].append({\n",
    "            \"phase\": \"validation_ingestion\",\n",
    "            \"status\": \"success\",\n",
    "            \"duration_seconds\": (datetime.now() - phase_start).total_seconds(),\n",
    "            \"validation_summary\": validation_results[\"summary\"]\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Validation/Ingestion Failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        execution_log[\"phases\"].append({\n",
    "            \"phase\": \"validation_ingestion\",\n",
    "            \"status\": \"failed\",\n",
    "            \"error\": str(e),\n",
    "            \"duration_seconds\": (datetime.now() - phase_start).total_seconds()\n",
    "        })\n",
    "        \n",
    "        if not PIPELINE_CONFIG.get(\"continue_on_error\", False):\n",
    "            raise\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping Phase 2: Validation & Ingestion (disabled in config)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedc744b",
   "metadata": {},
   "source": [
    "## üìà Phase 3: Data Quality Monitoring\n",
    "\n",
    "**Purpose:** Generate DQ dashboards and monitoring reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e233ca06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T14:39:30.265256Z",
     "iopub.status.busy": "2025-12-10T14:39:30.265127Z",
     "iopub.status.idle": "2025-12-10T14:39:30.271767Z",
     "shell.execute_reply": "2025-12-10T14:39:30.271307Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 3: DATA QUALITY MONITORING\n",
      "================================================================================\n",
      "\n",
      "üìä Generating monitoring dashboards...\n",
      "   Data source: /home/sanmi/Documents/HS2/HS2_PROJECTS_2025/1_AIMS_LOCAL_2026/config/validation_results/validation_results.json\n",
      "‚ö†Ô∏è No file validation results available. Run validation first.\n",
      "\n",
      "üìä Summary Statistics:\n",
      "   Total Files: 68\n",
      "   Passed: 0\n",
      "   Failed: 0\n",
      "   Skipped: 0\n",
      "   Errors: 68\n"
     ]
    }
   ],
   "source": [
    "if PIPELINE_CONFIG[\"run_monitoring\"]:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PHASE 3: DATA QUALITY MONITORING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    phase_start = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        # Load validation results\n",
    "        results_file = RESULTS_DIR / \"validation_results.json\"\n",
    "        \n",
    "        if not results_file.exists():\n",
    "            print(\"‚ö†Ô∏è No validation results found. Skipping monitoring.\")\n",
    "        else:\n",
    "            with open(results_file, 'r') as f:\n",
    "                validation_data = json.load(f)\n",
    "            \n",
    "            print(f\"\\nüìä Generating monitoring dashboards...\")\n",
    "            print(f\"   Data source: {results_file}\")\n",
    "            \n",
    "            # Check if we have file results\n",
    "            files_data = validation_data.get(\"files\", {})\n",
    "            if not files_data:\n",
    "                print(\"‚ö†Ô∏è No file validation results available. Run validation first.\")\n",
    "                print(f\"\\nüìä Summary Statistics:\")\n",
    "                summary = validation_data.get(\"summary\", {})\n",
    "                print(f\"   Total Files: {summary.get('total', 0)}\")\n",
    "                print(f\"   Passed: {summary.get('passed', 0)}\")\n",
    "                print(f\"   Failed: {summary.get('failed', 0)}\")\n",
    "                print(f\"   Skipped: {summary.get('skipped', 0)}\")\n",
    "                print(f\"   Errors: {summary.get('errors', 0)}\")\n",
    "            else:\n",
    "                # Create summary DataFrame\n",
    "                summary_data = []\n",
    "                for table_name, result in files_data.items():\n",
    "                    summary_data.append({\n",
    "                        \"Table\": table_name,\n",
    "                        \"Success %\": result.get(\"success_percentage\", 0),\n",
    "                        \"Status\": \"Passed\" if result.get(\"overall_success\") else \"Failed\",\n",
    "                        \"Evaluated\": result.get(\"statistics\", {}).get(\"evaluated_expectations\", 0),\n",
    "                        \"Successful\": result.get(\"statistics\", {}).get(\"successful_expectations\", 0)\n",
    "                    })\n",
    "                \n",
    "                df_summary = pd.DataFrame(summary_data)\n",
    "                \n",
    "                print(f\"\\nüìã DQ Summary:\")\n",
    "                print(df_summary.head(10).to_string(index=False))\n",
    "                \n",
    "                # Calculate key metrics\n",
    "                avg_quality = df_summary[\"Success %\"].mean()\n",
    "                pass_rate = (df_summary[\"Status\"] == \"Passed\").sum() / len(df_summary) * 100\n",
    "                \n",
    "                print(f\"\\nüìä Key Metrics:\")\n",
    "                print(f\"   Average Quality Score: {avg_quality:.1f}%\")\n",
    "                print(f\"   Pass Rate: {pass_rate:.1f}%\")\n",
    "                print(f\"   Tables Monitored: {len(df_summary)}\")\n",
    "                \n",
    "                # Log phase execution\n",
    "                execution_log[\"phases\"].append({\n",
    "                    \"phase\": \"monitoring\",\n",
    "                    \"status\": \"success\",\n",
    "                    \"duration_seconds\": (datetime.now() - phase_start).total_seconds(),\n",
    "                    \"metrics\": {\n",
    "                        \"avg_quality_score\": float(avg_quality),\n",
    "                        \"pass_rate\": float(pass_rate),\n",
    "                        \"tables_monitored\": len(df_summary)\n",
    "                    }\n",
    "                })\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Monitoring Failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        execution_log[\"phases\"].append({\n",
    "            \"phase\": \"monitoring\",\n",
    "            \"status\": \"failed\",\n",
    "            \"error\": str(e),\n",
    "            \"duration_seconds\": (datetime.now() - phase_start).total_seconds()\n",
    "        })\n",
    "        \n",
    "        if not PIPELINE_CONFIG.get(\"continue_on_error\", False):\n",
    "            raise\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping Phase 3: Monitoring (disabled in config)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36083e7",
   "metadata": {},
   "source": [
    "## üìù Pipeline Execution Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa189291",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T14:39:30.272982Z",
     "iopub.status.busy": "2025-12-10T14:39:30.272863Z",
     "iopub.status.idle": "2025-12-10T14:39:30.276944Z",
     "shell.execute_reply": "2025-12-10T14:39:30.276463Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Overall Status:\n",
      "   Phases Completed: 2/2\n",
      "   Success Rate: 100.0%\n",
      "\n",
      "üíæ Execution log saved to: /home/sanmi/Documents/HS2/HS2_PROJECTS_2025/1_AIMS_LOCAL_2026/config/validation_results/orchestration_log_20251210_143930.json\n",
      "\n",
      "================================================================================\n",
      "üéâ ALL PHASES COMPLETED SUCCESSFULLY!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Calculate success rate\n",
    "successful_phases = sum(1 for p in execution_log[\"phases\"] if p[\"status\"] in [\"success\", \"partial\"])\n",
    "total_phases = len(execution_log[\"phases\"])\n",
    "success_rate = (successful_phases / total_phases * 100) if total_phases > 0 else 0\n",
    "\n",
    "print(f\"\\nüìä Overall Status:\")\n",
    "print(f\"   Phases Completed: {successful_phases}/{total_phases}\")\n",
    "print(f\"   Success Rate: {success_rate:.1f}%\")\n",
    "\n",
    "# Save execution log\n",
    "log_file = RESULTS_DIR / f\"orchestration_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(log_file, 'w') as f:\n",
    "    json.dump(execution_log, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ Execution log saved to: {log_file}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "if success_rate == 100:\n",
    "    print(\"üéâ ALL PHASES COMPLETED SUCCESSFULLY!\")\n",
    "elif success_rate >= 80:\n",
    "    print(\"‚ö†Ô∏è PIPELINE COMPLETED WITH WARNINGS\")\n",
    "else:\n",
    "    print(\"‚ùå PIPELINE COMPLETED WITH ERRORS\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
