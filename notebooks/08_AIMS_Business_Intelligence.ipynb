{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "966ce777",
   "metadata": {},
   "source": [
    "# 08. AIMS Business Intelligence Analysis\n",
    "**Role: The Analyst**\n",
    "\n",
    "This notebook performs business intelligence analysis on validated Silver Layer data.\n",
    "\n",
    "## Purpose\n",
    "1. Load Silver Layer Star Schema tables (Fact and Dimensions)\n",
    "2. Calculate executive KPIs and operational metrics\n",
    "3. Perform multi-dimensional analysis (Route, Class, Organization)\n",
    "4. Generate actionable business insights\n",
    "\n",
    "## Prerequisites\n",
    "- Execute Notebook 01 (Data Profiling) to generate validation results\n",
    "- Execute Notebook 07 (DQ Matrix & Modeling) to create Silver Layer tables\n",
    "\n",
    "## Output\n",
    "- Executive dashboard metrics\n",
    "- Asset distribution analysis\n",
    "- Cross-dimensional insights\n",
    "- Geospatial analysis (if coordinates available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a515eb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Locally\n",
      "Configuration:\n",
      " Environment: Local\n",
      " Project Root: ..\n",
      " Parquet Dir: ../data/Samples_LH_Bronze_Aims_26_parquet\n",
      " Silver Dir: ../data/Silver\n",
      "aims_data_platform location: /home/sanmi/Documents/HS2/HS2_PROJECTS_2025/1_AIMS_LOCAL_2026/aims_data_platform/__init__.py\n"
     ]
    }
   ],
   "source": [
    "# --- CONFIGURATION ---\n",
    "import os\n",
    "os.environ[\"GX_ANALYTICS_ENABLED\"] = \"False\"\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 1. Detect Environment\n",
    "try:\n",
    "    from notebookutils import mssparkutils\n",
    "    IS_FABRIC = True\n",
    "    print(\"Running in Microsoft Fabric\")\n",
    "except ImportError:\n",
    "    IS_FABRIC = False\n",
    "    print(\"Running Locally\")\n",
    "\n",
    "# 2. Define Paths based on Environment\n",
    "if IS_FABRIC:\n",
    "    # Fabric: Use Lakehouse Paths (OneLake mounted path)\n",
    "    BASE_DIR = Path(\"/lakehouse/default/Files\")\n",
    "    \n",
    "    # Try to load .env from the Lakehouse Files root if it exists\n",
    "    env_path = BASE_DIR / \".env\"\n",
    "    if env_path.exists():\n",
    "        load_dotenv(dotenv_path=env_path)\n",
    "        print(f\"Loaded configuration from {env_path}\")\n",
    "    \n",
    "    # Fabric Paths\n",
    "    PROJECT_ROOT = BASE_DIR\n",
    "    # Assuming data is in Files/data/...\n",
    "    PARQUET_DIR = BASE_DIR / os.getenv(\"BRONZE_PATH\", \"data/Samples_LH_Bronze_Aims_26_parquet\")\n",
    "    \n",
    "else:\n",
    "    # Local: Use .env or defaults\n",
    "    load_dotenv()\n",
    "    \n",
    "    # Prioritize local development path in sys.path\n",
    "    project_root_local = Path.cwd().parent.resolve()\n",
    "    if str(project_root_local) not in sys.path:\n",
    "        sys.path.insert(0, str(project_root_local))\n",
    "        \n",
    "    PROJECT_ROOT = Path(\"..\")\n",
    "    PARQUET_DIR = PROJECT_ROOT / \"data/Samples_LH_Bronze_Aims_26_parquet\"\n",
    "\n",
    "# Define Silver Layer Path\n",
    "SILVER_DIR = PROJECT_ROOT / \"data/Silver\"\n",
    "\n",
    "print(f\"Configuration:\\n Environment: {'Fabric' if IS_FABRIC else 'Local'}\")\n",
    "print(f\" Project Root: {PROJECT_ROOT}\")\n",
    "print(f\" Parquet Dir: {PARQUET_DIR}\")\n",
    "print(f\" Silver Dir: {SILVER_DIR}\")\n",
    "\n",
    "import aims_data_platform\n",
    "import importlib\n",
    "# Force reload to pick up changes\n",
    "importlib.reload(aims_data_platform)\n",
    "print(f\"aims_data_platform location: {aims_data_platform.__file__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3e765749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# Visualization Settings\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befc5638",
   "metadata": {},
   "source": [
    "## 1. Data Loading: Silver Layer Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c9908a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Silver Layer Tables...\n",
      "\n",
      "WARNING: FACT_Asset_Inventory not found at ../data/Silver/FACT_Asset_Inventory.parquet\n",
      "WARNING: DIM_Route not found at ../data/Silver/DIM_Route.parquet\n",
      "WARNING: DIM_AssetClass not found at ../data/Silver/DIM_AssetClass.parquet\n",
      "WARNING: DIM_Organisation not found at ../data/Silver/DIM_Organisation.parquet\n",
      "WARNING: DIM_Date not found at ../data/Silver/DIM_Date.parquet\n",
      "WARNING: DIM_Status not found at ../data/Silver/DIM_Status.parquet\n",
      "\n",
      "Data loading complete.\n"
     ]
    }
   ],
   "source": [
    "# Load Star Schema Tables\n",
    "def load_silver_table(table_name):\n",
    "    \"\"\"\n",
    "    Load a Silver Layer table from Parquet format.\n",
    "    Returns None if table does not exist.\n",
    "    \"\"\"\n",
    "    file_path = SILVER_DIR / f\"{table_name}.parquet\"\n",
    "    if not file_path.exists():\n",
    "        print(f\"WARNING: {table_name} not found at {file_path}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_parquet(file_path, engine='pyarrow')\n",
    "        print(f\"LOADED: {table_name} ({len(df):,} rows, {len(df.columns)} columns)\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR loading {table_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load all tables\n",
    "print(\"Loading Silver Layer Tables...\\n\")\n",
    "fact_assets = load_silver_table(\"FACT_Asset_Inventory\")\n",
    "dim_route = load_silver_table(\"DIM_Route\")\n",
    "dim_class = load_silver_table(\"DIM_AssetClass\")\n",
    "dim_org = load_silver_table(\"DIM_Organisation\")\n",
    "dim_date = load_silver_table(\"DIM_Date\")\n",
    "dim_status = load_silver_table(\"DIM_Status\")\n",
    "\n",
    "print(\"\\nData loading complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53509a5",
   "metadata": {},
   "source": [
    "## 2. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "deddc4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess completeness and integrity of loaded tables\n",
    "if fact_assets is not None:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"DATA QUALITY ASSESSMENT\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Fact table completeness\n",
    "    total_assets = len(fact_assets)\n",
    "    missing_routes = fact_assets['Route_Key'].isna().sum()\n",
    "    missing_classes = fact_assets['Class_Key'].isna().sum()\n",
    "    missing_owners = fact_assets['Owner_Key'].isna().sum()\n",
    "    \n",
    "    print(f\"\\nFACT Table Completeness:\")\n",
    "    print(f\"  Total Assets: {total_assets:,}\")\n",
    "    print(f\"  Route Coverage: {((total_assets - missing_routes) / total_assets * 100):.1f}%\")\n",
    "    print(f\"  Class Coverage: {((total_assets - missing_classes) / total_assets * 100):.1f}%\")\n",
    "    print(f\"  Owner Coverage: {((total_assets - missing_owners) / total_assets * 100):.1f}%\")\n",
    "    \n",
    "    # Referential integrity checks\n",
    "    integrity_issues = []\n",
    "    \n",
    "    if dim_route is not None:\n",
    "        orphaned_routes = fact_assets[~fact_assets['Route_Key'].isna()]['Route_Key'].isin(dim_route['Route_Key']).sum()\n",
    "        if orphaned_routes < (total_assets - missing_routes):\n",
    "            integrity_issues.append(f\"Route Key referential integrity: {orphaned_routes} valid references\")\n",
    "    \n",
    "    if dim_class is not None:\n",
    "        orphaned_classes = fact_assets[~fact_assets['Class_Key'].isna()]['Class_Key'].isin(dim_class['Class_Key']).sum()\n",
    "        if orphaned_classes < (total_assets - missing_classes):\n",
    "            integrity_issues.append(f\"Class Key referential integrity: {orphaned_classes} valid references\")\n",
    "    \n",
    "    if integrity_issues:\n",
    "        print(f\"\\nReferential Integrity Findings:\")\n",
    "        for issue in integrity_issues:\n",
    "            print(f\"  - {issue}\")\n",
    "    else:\n",
    "        print(f\"\\nReferential Integrity: PASSED\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbac76e",
   "metadata": {},
   "source": [
    "## 3. Executive KPIs and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "81d3625a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate high-level KPIs for executive dashboard\n",
    "if fact_assets is not None:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"EXECUTIVE KEY PERFORMANCE INDICATORS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Asset Portfolio Size\n",
    "    total_assets = len(fact_assets)\n",
    "    print(f\"\\n1. ASSET PORTFOLIO\")\n",
    "    print(f\"   Total Assets Under Management: {total_assets:,}\")\n",
    "    \n",
    "    # Route Coverage\n",
    "    if dim_route is not None:\n",
    "        total_routes = len(dim_route)\n",
    "        routes_with_assets = fact_assets['Route_Key'].nunique()\n",
    "        route_coverage = (routes_with_assets / total_routes * 100) if total_routes > 0 else 0\n",
    "        print(f\"\\n2. ROUTE COVERAGE\")\n",
    "        print(f\"   Total Routes: {total_routes}\")\n",
    "        print(f\"   Routes with Assets: {routes_with_assets}\")\n",
    "        print(f\"   Coverage Rate: {route_coverage:.1f}%\")\n",
    "    \n",
    "    # Asset Classification Diversity\n",
    "    if dim_class is not None:\n",
    "        unique_classes = fact_assets['Class_Key'].nunique()\n",
    "        total_classes = len(dim_class)\n",
    "        print(f\"\\n3. ASSET CLASSIFICATION\")\n",
    "        print(f\"   Total Asset Classes Defined: {total_classes:,}\")\n",
    "        print(f\"   Asset Classes in Use: {unique_classes:,}\")\n",
    "        print(f\"   Utilization Rate: {(unique_classes/total_classes*100):.1f}%\")\n",
    "    \n",
    "    # Organizational Distribution\n",
    "    if dim_org is not None:\n",
    "        unique_owners = fact_assets['Owner_Key'].nunique()\n",
    "        total_orgs = len(dim_org)\n",
    "        print(f\"\\n4. ORGANIZATIONAL RESPONSIBILITY\")\n",
    "        print(f\"   Total Organizations: {total_orgs}\")\n",
    "        print(f\"   Organizations with Asset Ownership: {unique_owners}\")\n",
    "        print(f\"   Participation Rate: {(unique_owners/total_orgs*100):.1f}%\")\n",
    "    \n",
    "    # Status Distribution\n",
    "    if 'Asset_Status' in fact_assets.columns:\n",
    "        status_dist = fact_assets['Asset_Status'].value_counts()\n",
    "        print(f\"\\n5. ASSET STATUS DISTRIBUTION\")\n",
    "        for status, count in status_dist.items():\n",
    "            pct = (count / total_assets * 100)\n",
    "            print(f\"   {status}: {count:,} ({pct:.1f}%)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac20793",
   "metadata": {},
   "source": [
    "## 4. Asset Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e7b2afb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze asset distribution across routes\n",
    "if fact_assets is not None and dim_route is not None:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ASSET DISTRIBUTION BY ROUTE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Join with dimension for readable names\n",
    "    route_analysis = fact_assets.merge(\n",
    "        dim_route[['Route_Key', 'Route_Code', 'Route_Description']],\n",
    "        on='Route_Key',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Calculate route statistics\n",
    "    route_summary = route_analysis.groupby('Route_Code').agg({\n",
    "        'Asset_Key': 'count',\n",
    "        'Class_Key': 'nunique'\n",
    "    }).rename(columns={\n",
    "        'Asset_Key': 'Asset_Count',\n",
    "        'Class_Key': 'Unique_Classes'\n",
    "    }).sort_values('Asset_Count', ascending=False)\n",
    "    \n",
    "    print(f\"\\nTop 10 Routes by Asset Count:\\n\")\n",
    "    display(route_summary.head(10))\n",
    "    \n",
    "    # Visualize distribution\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    top_routes = route_summary.head(15)\n",
    "    ax.barh(top_routes.index, top_routes['Asset_Count'])\n",
    "    ax.set_xlabel('Number of Assets')\n",
    "    ax.set_ylabel('Route Code')\n",
    "    ax.set_title('Top 15 Routes by Asset Count')\n",
    "    ax.invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a39b582",
   "metadata": {},
   "source": [
    "## 5. Asset Classification Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a603190e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze asset distribution by classification\n",
    "if fact_assets is not None and dim_class is not None:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ASSET CLASSIFICATION ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Use Class_Name directly from fact table (already has enriched data)\n",
    "    # Top asset classes\n",
    "    class_summary = fact_assets['Class_Name'].value_counts()\n",
    "    \n",
    "    print(f\"\\nTop 15 Asset Classes by Count:\\n\")\n",
    "    top_classes = class_summary.head(15)\n",
    "    for class_name, count in top_classes.items():\n",
    "        if pd.notna(class_name):\n",
    "            pct = (count / len(fact_assets) * 100)\n",
    "            print(f\"  {class_name}: {count:,} ({pct:.1f}%)\")\n",
    "    \n",
    "    # Visualize (exclude NaN values)\n",
    "    top_classes_clean = top_classes[top_classes.index.notna()]\n",
    "    if len(top_classes_clean) > 0:\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        ax.barh(range(len(top_classes_clean)), top_classes_clean.values)\n",
    "        ax.set_yticks(range(len(top_classes_clean)))\n",
    "        ax.set_yticklabels([str(name)[:50] for name in top_classes_clean.index])\n",
    "        ax.set_xlabel('Number of Assets')\n",
    "        ax.set_ylabel('Asset Class')\n",
    "        ax.set_title('Top 15 Asset Classes by Volume')\n",
    "        ax.invert_yaxis()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0bb54e",
   "metadata": {},
   "source": [
    "## 6. Organizational Asset Ownership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3a59e6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze asset ownership by organization\n",
    "if fact_assets is not None and dim_org is not None:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ORGANIZATIONAL ASSET OWNERSHIP ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Use Owner_Key directly from fact table (contains organization names)\n",
    "    # Calculate ownership statistics\n",
    "    org_summary = fact_assets.groupby('Owner_Key').agg({\n",
    "        'Asset_Key': 'count',\n",
    "        'Class_Key': 'nunique',\n",
    "        'Route_Key': 'nunique'\n",
    "    }).rename(columns={\n",
    "        'Asset_Key': 'Total_Assets',\n",
    "        'Class_Key': 'Asset_Classes',\n",
    "        'Route_Key': 'Routes_Covered'\n",
    "    }).sort_values('Total_Assets', ascending=False)\n",
    "    \n",
    "    org_summary.index.name = 'Organisation_Name'\n",
    "    \n",
    "    print(f\"\\nTop 10 Organizations by Asset Count:\\n\")\n",
    "    display(org_summary.head(10))\n",
    "    \n",
    "    # Visualize ownership distribution\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Asset count distribution\n",
    "    top_orgs = org_summary.head(10)\n",
    "    ax1.barh(range(len(top_orgs)), top_orgs['Total_Assets'])\n",
    "    ax1.set_yticks(range(len(top_orgs)))\n",
    "    ax1.set_yticklabels([name[:30] for name in top_orgs.index])\n",
    "    ax1.set_xlabel('Number of Assets')\n",
    "    ax1.set_ylabel('Organization')\n",
    "    ax1.set_title('Top 10 Organizations by Asset Count')\n",
    "    ax1.invert_yaxis()\n",
    "    \n",
    "    # Asset class diversity\n",
    "    ax2.barh(range(len(top_orgs)), top_orgs['Asset_Classes'])\n",
    "    ax2.set_yticks(range(len(top_orgs)))\n",
    "    ax2.set_yticklabels([name[:30] for name in top_orgs.index])\n",
    "    ax2.set_xlabel('Number of Unique Asset Classes')\n",
    "    ax2.set_ylabel('Organization')\n",
    "    ax2.set_title('Asset Classification Diversity by Organization')\n",
    "    ax2.invert_yaxis()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389db373",
   "metadata": {},
   "source": [
    "## 7. Cross-Dimensional Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "52b980ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-dimensional analysis: Route x Asset Class\n",
    "if fact_assets is not None and dim_route is not None and dim_class is not None:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"CROSS-DIMENSIONAL ANALYSIS: ROUTE x ASSET CLASS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Use fact_assets directly (already has Route_Name and Class_Name)\n",
    "    # Get top routes and classes\n",
    "    top_routes = fact_assets['Route_Name'].value_counts().head(10).index\n",
    "    top_classes = fact_assets['Class_Name'].value_counts().head(10).index\n",
    "    \n",
    "    # Filter dataset\n",
    "    filtered = fact_assets[\n",
    "        fact_assets['Route_Name'].isin(top_routes) & \n",
    "        fact_assets['Class_Name'].isin(top_classes)\n",
    "    ]\n",
    "    \n",
    "    # Create pivot table\n",
    "    pivot = pd.crosstab(\n",
    "        filtered['Route_Name'],\n",
    "        filtered['Class_Name'].str[:30]  # Truncate for display\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nAsset Distribution Heatmap (Top 10 Routes x Top 10 Classes):\\n\")\n",
    "    \n",
    "    # Visualize as heatmap\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    sns.heatmap(pivot, annot=True, fmt='d', cmap='YlOrRd', ax=ax, cbar_kws={'label': 'Asset Count'})\n",
    "    ax.set_title('Asset Distribution: Routes vs Asset Classes')\n",
    "    ax.set_xlabel('Asset Class (Truncated)')\n",
    "    ax.set_ylabel('Route Code')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d942380",
   "metadata": {},
   "source": [
    "## 8. Geospatial Analysis (If Available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6bbcdc0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geospatial analysis not available: Coordinate columns not found.\n"
     ]
    }
   ],
   "source": [
    "# Analyze geospatial distribution if coordinates available\n",
    "if fact_assets is not None and 'OSGBEASTING' in fact_assets.columns and 'OSGBNORTHING' in fact_assets.columns:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"GEOSPATIAL DISTRIBUTION ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Filter assets with valid coordinates\n",
    "    geo_assets = fact_assets[\n",
    "        fact_assets['OSGBEASTING'].notna() & \n",
    "        fact_assets['OSGBNORTHING'].notna()\n",
    "    ].copy()\n",
    "    \n",
    "    geo_coverage = (len(geo_assets) / len(fact_assets) * 100)\n",
    "    print(f\"\\nGeospatial Coverage: {len(geo_assets):,} assets ({geo_coverage:.1f}%)\")\n",
    "    \n",
    "    if len(geo_assets) > 0:\n",
    "        # Convert coordinates to numeric and filter valid OSGB coordinates\n",
    "        geo_assets['OSGBEASTING'] = pd.to_numeric(geo_assets['OSGBEASTING'], errors='coerce')\n",
    "        geo_assets['OSGBNORTHING'] = pd.to_numeric(geo_assets['OSGBNORTHING'], errors='coerce')\n",
    "        \n",
    "        # Filter to valid OSGB range (UK coordinates: Easting 0-700000, Northing 0-1300000)\n",
    "        geo_assets = geo_assets[\n",
    "            (geo_assets['OSGBEASTING'] > 0) & (geo_assets['OSGBEASTING'] < 700000) &\n",
    "            (geo_assets['OSGBNORTHING'] > 0) & (geo_assets['OSGBNORTHING'] < 1300000)\n",
    "        ]\n",
    "        \n",
    "        print(f\"Valid UK coordinates: {len(geo_assets):,} assets\")\n",
    "        print(f\"\\nCoordinate Ranges:\")\n",
    "        print(f\"  Easting: {geo_assets['OSGBEASTING'].min():.0f} to {geo_assets['OSGBEASTING'].max():.0f}\")\n",
    "        print(f\"  Northing: {geo_assets['OSGBNORTHING'].min():.0f} to {geo_assets['OSGBNORTHING'].max():.0f}\")\n",
    "        \n",
    "        # Statistical analysis\n",
    "        print(f\"\\nSpatial Distribution Statistics:\")\n",
    "        print(f\"  Easting Mean: {geo_assets['OSGBEASTING'].mean():.0f}\")\n",
    "        print(f\"  Northing Mean: {geo_assets['OSGBNORTHING'].mean():.0f}\")\n",
    "        print(f\"  Easting Std Dev: {geo_assets['OSGBEASTING'].std():.0f}\")\n",
    "        print(f\"  Northing Std Dev: {geo_assets['OSGBNORTHING'].std():.0f}\")\n",
    "        \n",
    "        # Route-based spatial analysis\n",
    "        if 'Route_Name' in geo_assets.columns:\n",
    "            print(f\"\\nSpatial Distribution by Route:\")\n",
    "            route_spatial = geo_assets.groupby('Route_Name').agg({\n",
    "                'OSGBEASTING': ['mean', 'std', 'count'],\n",
    "                'OSGBNORTHING': ['mean', 'std']\n",
    "            }).round(0)\n",
    "            route_spatial.columns = ['_'.join(col).strip() for col in route_spatial.columns.values]\n",
    "            route_spatial = route_spatial.sort_values('OSGBEASTING_count', ascending=False)\n",
    "            display(route_spatial.head(10))\n",
    "        \n",
    "        # Create multi-panel visualization\n",
    "        fig = plt.figure(figsize=(16, 12))\n",
    "        \n",
    "        # 1. Hex bin density map\n",
    "        ax1 = plt.subplot(2, 2, 1)\n",
    "        hexbin = ax1.hexbin(\n",
    "            geo_assets['OSGBEASTING'], \n",
    "            geo_assets['OSGBNORTHING'],\n",
    "            gridsize=50,\n",
    "            cmap='YlOrRd',\n",
    "            mincnt=1\n",
    "        )\n",
    "        ax1.set_xlabel('OSGB Easting (m)')\n",
    "        ax1.set_ylabel('OSGB Northing (m)')\n",
    "        ax1.set_title('Asset Density Heatmap (Hexagonal Binning)')\n",
    "        plt.colorbar(hexbin, ax=ax1, label='Asset Count')\n",
    "        ax1.set_aspect('equal')\n",
    "        \n",
    "        # 2. Scatter plot by route (top routes only)\n",
    "        ax2 = plt.subplot(2, 2, 2)\n",
    "        if 'Route_Name' in geo_assets.columns:\n",
    "            top_routes = geo_assets['Route_Name'].value_counts().head(5).index\n",
    "            colors = plt.cm.Set1(np.linspace(0, 1, len(top_routes)))\n",
    "            for idx, route in enumerate(top_routes):\n",
    "                route_data = geo_assets[geo_assets['Route_Name'] == route]\n",
    "                ax2.scatter(\n",
    "                    route_data['OSGBEASTING'],\n",
    "                    route_data['OSGBNORTHING'],\n",
    "                    alpha=0.5,\n",
    "                    s=1,\n",
    "                    c=[colors[idx]],\n",
    "                    label=route[:40]\n",
    "                )\n",
    "            ax2.legend(markerscale=5, loc='upper right', fontsize=8)\n",
    "        else:\n",
    "            ax2.scatter(\n",
    "                geo_assets['OSGBEASTING'],\n",
    "                geo_assets['OSGBNORTHING'],\n",
    "                alpha=0.3,\n",
    "                s=1\n",
    "            )\n",
    "        ax2.set_xlabel('OSGB Easting (m)')\n",
    "        ax2.set_ylabel('OSGB Northing (m)')\n",
    "        ax2.set_title('Asset Distribution by Route')\n",
    "        ax2.set_aspect('equal')\n",
    "        \n",
    "        # 3. 2D Histogram\n",
    "        ax3 = plt.subplot(2, 2, 3)\n",
    "        hist = ax3.hist2d(\n",
    "            geo_assets['OSGBEASTING'],\n",
    "            geo_assets['OSGBNORTHING'],\n",
    "            bins=50,\n",
    "            cmap='Blues'\n",
    "        )\n",
    "        ax3.set_xlabel('OSGB Easting (m)')\n",
    "        ax3.set_ylabel('OSGB Northing (m)')\n",
    "        ax3.set_title('Asset Concentration (2D Histogram)')\n",
    "        plt.colorbar(hist[3], ax=ax3, label='Asset Count')\n",
    "        ax3.set_aspect('equal')\n",
    "        \n",
    "        # 4. KDE Contour plot\n",
    "        ax4 = plt.subplot(2, 2, 4)\n",
    "        try:\n",
    "            from scipy.stats import gaussian_kde\n",
    "            # Sample for performance if dataset is large\n",
    "            sample_size = min(10000, len(geo_assets))\n",
    "            sample = geo_assets.sample(sample_size, random_state=42)\n",
    "            \n",
    "            x = sample['OSGBEASTING'].values\n",
    "            y = sample['OSGBNORTHING'].values\n",
    "            \n",
    "            # Calculate point density\n",
    "            xy = np.vstack([x, y])\n",
    "            z = gaussian_kde(xy)(xy)\n",
    "            \n",
    "            scatter = ax4.scatter(x, y, c=z, s=1, cmap='viridis', alpha=0.5)\n",
    "            ax4.set_xlabel('OSGB Easting (m)')\n",
    "            ax4.set_ylabel('OSGB Northing (m)')\n",
    "            ax4.set_title('Asset Density (KDE)')\n",
    "            plt.colorbar(scatter, ax=ax4, label='Density')\n",
    "            ax4.set_aspect('equal')\n",
    "        except ImportError:\n",
    "            ax4.text(0.5, 0.5, 'KDE plot requires scipy', \n",
    "                    ha='center', va='center', transform=ax4.transAxes)\n",
    "            ax4.set_title('Asset Density (KDE) - scipy not available')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Spatial clustering insights\n",
    "        print(f\"\\nSpatial Clustering Analysis:\")\n",
    "        easting_quartiles = geo_assets['OSGBEASTING'].quantile([0.25, 0.5, 0.75])\n",
    "        northing_quartiles = geo_assets['OSGBNORTHING'].quantile([0.25, 0.5, 0.75])\n",
    "        \n",
    "        print(f\"  Easting Quartiles: Q1={easting_quartiles[0.25]:.0f}, \"\n",
    "              f\"Q2={easting_quartiles[0.5]:.0f}, Q3={easting_quartiles[0.75]:.0f}\")\n",
    "        print(f\"  Northing Quartiles: Q1={northing_quartiles[0.25]:.0f}, \"\n",
    "              f\"Q2={northing_quartiles[0.5]:.0f}, Q3={northing_quartiles[0.75]:.0f}\")\n",
    "        \n",
    "        # Calculate spatial extent\n",
    "        easting_range = geo_assets['OSGBEASTING'].max() - geo_assets['OSGBEASTING'].min()\n",
    "        northing_range = geo_assets['OSGBNORTHING'].max() - geo_assets['OSGBNORTHING'].min()\n",
    "        print(f\"\\nSpatial Extent:\")\n",
    "        print(f\"  East-West span: {easting_range/1000:.1f} km\")\n",
    "        print(f\"  North-South span: {northing_range/1000:.1f} km\")\n",
    "        print(f\"  Approximate coverage area: {(easting_range * northing_range)/(1000000):.1f} km²\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "else:\n",
    "    print(\"Geospatial analysis not available: Coordinate columns not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef61a6c8",
   "metadata": {},
   "source": [
    "## 9. Asset Portfolio Diversity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9d2808e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze portfolio diversity across multiple dimensions\n",
    "if fact_assets is not None:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"PORTFOLIO DIVERSITY ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Calculate diversity metrics\n",
    "    total_assets = len(fact_assets)\n",
    "    \n",
    "    # Route diversity\n",
    "    route_diversity = fact_assets['Route_Key'].nunique()\n",
    "    assets_per_route = total_assets / route_diversity if route_diversity > 0 else 0\n",
    "    \n",
    "    # Class diversity\n",
    "    class_diversity = fact_assets['Class_Key'].nunique()\n",
    "    assets_per_class = total_assets / class_diversity if class_diversity > 0 else 0\n",
    "    \n",
    "    # Owner diversity\n",
    "    owner_diversity = fact_assets['Owner_Key'].nunique()\n",
    "    assets_per_owner = total_assets / owner_diversity if owner_diversity > 0 else 0\n",
    "    \n",
    "    # Calculate Herfindahl-Hirschman Index (HHI) for concentration\n",
    "    # HHI closer to 1 = high concentration, closer to 0 = high diversity\n",
    "    \n",
    "    # Route concentration\n",
    "    route_shares = fact_assets['Route_Key'].value_counts() / total_assets\n",
    "    hhi_route = (route_shares ** 2).sum()\n",
    "    \n",
    "    # Class concentration\n",
    "    class_shares = fact_assets['Class_Key'].value_counts() / total_assets\n",
    "    hhi_class = (class_shares ** 2).sum()\n",
    "    \n",
    "    # Owner concentration\n",
    "    owner_shares = fact_assets['Owner_Key'].value_counts() / total_assets\n",
    "    hhi_owner = (owner_shares ** 2).sum()\n",
    "    \n",
    "    print(f\"\\nDIVERSITY METRICS:\")\n",
    "    print(f\"  Unique Routes: {route_diversity}\")\n",
    "    print(f\"  Unique Asset Classes: {class_diversity}\")\n",
    "    print(f\"  Unique Owners: {owner_diversity}\")\n",
    "    \n",
    "    print(f\"\\nAVERAGE DISTRIBUTION:\")\n",
    "    print(f\"  Assets per Route: {assets_per_route:.0f}\")\n",
    "    print(f\"  Assets per Class: {assets_per_class:.0f}\")\n",
    "    print(f\"  Assets per Owner: {assets_per_owner:.0f}\")\n",
    "    \n",
    "    print(f\"\\nCONCENTRATION INDEX (HHI):\")\n",
    "    print(f\"  Route Concentration: {hhi_route:.4f} ({'High' if hhi_route > 0.25 else 'Moderate' if hhi_route > 0.15 else 'Low'})\")\n",
    "    print(f\"  Class Concentration: {hhi_class:.4f} ({'High' if hhi_class > 0.25 else 'Moderate' if hhi_class > 0.15 else 'Low'})\")\n",
    "    print(f\"  Owner Concentration: {hhi_owner:.4f} ({'High' if hhi_owner > 0.25 else 'Moderate' if hhi_owner > 0.15 else 'Low'})\")\n",
    "    \n",
    "    # Top contributors analysis\n",
    "    print(f\"\\nTOP 3 CONTRIBUTORS BY DIMENSION:\")\n",
    "    \n",
    "    # Top routes\n",
    "    top_routes_share = route_shares.head(3)\n",
    "    print(f\"\\n  Routes:\")\n",
    "    for route, share in top_routes_share.items():\n",
    "        if pd.notna(route):\n",
    "            route_name = fact_assets[fact_assets['Route_Key'] == route]['Route_Name'].iloc[0] if len(fact_assets[fact_assets['Route_Key'] == route]) > 0 else route\n",
    "            print(f\"    {route_name[:50]}: {share*100:.1f}%\")\n",
    "    \n",
    "    # Top classes\n",
    "    top_classes_share = class_shares.head(3)\n",
    "    print(f\"\\n  Asset Classes:\")\n",
    "    for cls, share in top_classes_share.items():\n",
    "        if pd.notna(cls):\n",
    "            cls_name = fact_assets[fact_assets['Class_Key'] == cls]['Class_Name'].iloc[0] if len(fact_assets[fact_assets['Class_Key'] == cls]) > 0 else cls\n",
    "            print(f\"    {cls_name[:50]}: {share*100:.1f}%\")\n",
    "    \n",
    "    # Top owners\n",
    "    top_owners_share = owner_shares.head(3)\n",
    "    print(f\"\\n  Owners:\")\n",
    "    for owner, share in top_owners_share.items():\n",
    "        if pd.notna(owner):\n",
    "            print(f\"    {owner}: {share*100:.1f}%\")\n",
    "    \n",
    "    # Visualize diversity\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Lorenz curve for route distribution\n",
    "    ax1 = axes[0]\n",
    "    route_cumsum = route_shares.sort_values().cumsum()\n",
    "    ax1.plot([0] + list(range(1, len(route_cumsum)+1)), [0] + list(route_cumsum), marker='o', markersize=3)\n",
    "    ax1.plot([0, len(route_cumsum)], [0, 1], 'r--', label='Perfect Equality')\n",
    "    ax1.set_xlabel('Number of Routes')\n",
    "    ax1.set_ylabel('Cumulative Share of Assets')\n",
    "    ax1.set_title('Route Distribution Curve')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Concentration by dimension\n",
    "    ax2 = axes[1]\n",
    "    dimensions = ['Routes', 'Classes', 'Owners']\n",
    "    hhi_values = [hhi_route, hhi_class, hhi_owner]\n",
    "    colors_hhi = ['red' if h > 0.25 else 'orange' if h > 0.15 else 'green' for h in hhi_values]\n",
    "    bars = ax2.bar(dimensions, hhi_values, color=colors_hhi, alpha=0.7)\n",
    "    ax2.axhline(y=0.25, color='red', linestyle='--', linewidth=1, label='High Concentration')\n",
    "    ax2.axhline(y=0.15, color='orange', linestyle='--', linewidth=1, label='Moderate Concentration')\n",
    "    ax2.set_ylabel('HHI Score')\n",
    "    ax2.set_title('Concentration Index by Dimension')\n",
    "    ax2.set_ylim(0, 1)\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Portfolio balance\n",
    "    ax3 = axes[2]\n",
    "    diversity_metrics = [route_diversity, class_diversity, owner_diversity]\n",
    "    ax3.bar(dimensions, diversity_metrics, color=['#1f77b4', '#ff7f0e', '#2ca02c'], alpha=0.7)\n",
    "    ax3.set_ylabel('Number of Unique Values')\n",
    "    ax3.set_title('Portfolio Diversity Metrics')\n",
    "    ax3.set_yscale('log')\n",
    "    ax3.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Risk assessment\n",
    "    print(f\"\\nRISK ASSESSMENT:\")\n",
    "    if hhi_route > 0.4:\n",
    "        print(\"  [WARNING] High route concentration detected - portfolio exposed to single route risks\")\n",
    "    if hhi_owner > 0.8:\n",
    "        print(\"  [WARNING] Extreme owner concentration - organizational dependency risk\")\n",
    "    if hhi_class < 0.05:\n",
    "        print(\"  [NOTE] High class diversity - may indicate complex portfolio management requirements\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3269bd21",
   "metadata": {},
   "source": [
    "## 10. Data Completeness and Quality Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3d2fe671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive data quality scoring across all fields\n",
    "if fact_assets is not None:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"DATA COMPLETENESS AND QUALITY SCORING\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Define critical fields for scoring\n",
    "    critical_fields = ['Asset_Key', 'Route_Key', 'Class_Key', 'Owner_Key', 'Asset_Status']\n",
    "    important_fields = ['Asset_Name', 'Class_Name', 'Route_Name']\n",
    "    optional_fields = ['OSGBEASTING', 'OSGBNORTHING', 'Chainage_Baseline', 'PHASEID']\n",
    "    \n",
    "    total_records = len(fact_assets)\n",
    "    \n",
    "    # Calculate completeness for each field\n",
    "    field_scores = {}\n",
    "    \n",
    "    print(f\"\\nCRITICAL FIELDS COMPLETENESS:\")\n",
    "    critical_score = 0\n",
    "    for field in critical_fields:\n",
    "        if field in fact_assets.columns:\n",
    "            non_null = fact_assets[field].notna().sum()\n",
    "            completeness = (non_null / total_records) * 100\n",
    "            field_scores[field] = completeness\n",
    "            critical_score += completeness\n",
    "            status = \"✓\" if completeness >= 95 else \"⚠\" if completeness >= 80 else \"✗\"\n",
    "            print(f\"  {status} {field}: {completeness:.1f}% ({non_null:,}/{total_records:,})\")\n",
    "    \n",
    "    critical_avg = critical_score / len(critical_fields) if critical_fields else 0\n",
    "    \n",
    "    print(f\"\\nIMPORTANT FIELDS COMPLETENESS:\")\n",
    "    important_score = 0\n",
    "    for field in important_fields:\n",
    "        if field in fact_assets.columns:\n",
    "            non_null = fact_assets[field].notna().sum()\n",
    "            completeness = (non_null / total_records) * 100\n",
    "            field_scores[field] = completeness\n",
    "            important_score += completeness\n",
    "            status = \"✓\" if completeness >= 90 else \"⚠\" if completeness >= 70 else \"✗\"\n",
    "            print(f\"  {status} {field}: {completeness:.1f}% ({non_null:,}/{total_records:,})\")\n",
    "    \n",
    "    important_avg = important_score / len(important_fields) if important_fields else 0\n",
    "    \n",
    "    print(f\"\\nOPTIONAL FIELDS COMPLETENESS:\")\n",
    "    optional_score = 0\n",
    "    for field in optional_fields:\n",
    "        if field in fact_assets.columns:\n",
    "            non_null = fact_assets[field].notna().sum()\n",
    "            completeness = (non_null / total_records) * 100\n",
    "            field_scores[field] = completeness\n",
    "            optional_score += completeness\n",
    "            status = \"✓\" if completeness >= 70 else \"⚠\" if completeness >= 50 else \"○\"\n",
    "            print(f\"  {status} {field}: {completeness:.1f}% ({non_null:,}/{total_records:,})\")\n",
    "    \n",
    "    optional_avg = optional_score / len(optional_fields) if optional_fields else 0\n",
    "    \n",
    "    # Calculate overall quality score (weighted)\n",
    "    overall_quality_score = (critical_avg * 0.5) + (important_avg * 0.3) + (optional_avg * 0.2)\n",
    "    \n",
    "    print(f\"\\nCOMPOSITE QUALITY SCORES:\")\n",
    "    print(f\"  Critical Fields Average: {critical_avg:.1f}%\")\n",
    "    print(f\"  Important Fields Average: {important_avg:.1f}%\")\n",
    "    print(f\"  Optional Fields Average: {optional_avg:.1f}%\")\n",
    "    print(f\"\\n  OVERALL QUALITY SCORE: {overall_quality_score:.1f}/100\")\n",
    "    \n",
    "    # Grade assignment\n",
    "    if overall_quality_score >= 95:\n",
    "        grade = \"A+ (Excellent)\"\n",
    "    elif overall_quality_score >= 90:\n",
    "        grade = \"A (Very Good)\"\n",
    "    elif overall_quality_score >= 85:\n",
    "        grade = \"B+ (Good)\"\n",
    "    elif overall_quality_score >= 80:\n",
    "        grade = \"B (Satisfactory)\"\n",
    "    elif overall_quality_score >= 75:\n",
    "        grade = \"C (Needs Improvement)\"\n",
    "    else:\n",
    "        grade = \"D (Poor)\"\n",
    "    \n",
    "    print(f\"  Quality Grade: {grade}\")\n",
    "    \n",
    "    # Identify records with missing critical data\n",
    "    missing_critical = fact_assets[critical_fields].isna().any(axis=1).sum()\n",
    "    complete_critical = total_records - missing_critical\n",
    "    \n",
    "    print(f\"\\nRECORD-LEVEL ANALYSIS:\")\n",
    "    print(f\"  Complete Records (all critical fields): {complete_critical:,} ({complete_critical/total_records*100:.1f}%)\")\n",
    "    print(f\"  Incomplete Records (missing critical data): {missing_critical:,} ({missing_critical/total_records*100:.1f}%)\")\n",
    "    \n",
    "    # Field completeness visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "    \n",
    "    # Bar chart of all fields\n",
    "    ax1 = axes[0, 0]\n",
    "    fields = list(field_scores.keys())\n",
    "    scores = list(field_scores.values())\n",
    "    colors_quality = ['green' if s >= 90 else 'orange' if s >= 70 else 'red' for s in scores]\n",
    "    ax1.barh(fields, scores, color=colors_quality, alpha=0.7)\n",
    "    ax1.axvline(x=90, color='green', linestyle='--', linewidth=1, label='90% Threshold')\n",
    "    ax1.axvline(x=70, color='orange', linestyle='--', linewidth=1, label='70% Threshold')\n",
    "    ax1.set_xlabel('Completeness %')\n",
    "    ax1.set_title('Field Completeness Scores')\n",
    "    ax1.set_xlim(0, 100)\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # Radar chart for field categories\n",
    "    ax2 = axes[0, 1]\n",
    "    categories = ['Critical\\nFields', 'Important\\nFields', 'Optional\\nFields', 'Overall\\nScore']\n",
    "    values = [critical_avg, important_avg, optional_avg, overall_quality_score]\n",
    "    angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()\n",
    "    values += values[:1]\n",
    "    angles += angles[:1]\n",
    "    \n",
    "    ax2 = plt.subplot(222, projection='polar')\n",
    "    ax2.plot(angles, values, 'o-', linewidth=2, label='Current Score')\n",
    "    ax2.fill(angles, values, alpha=0.25)\n",
    "    ax2.plot(angles, [95]*len(angles), 'r--', linewidth=1, label='Target (95%)')\n",
    "    ax2.set_xticks(angles[:-1])\n",
    "    ax2.set_xticklabels(categories)\n",
    "    ax2.set_ylim(0, 100)\n",
    "    ax2.set_yticks([25, 50, 75, 100])\n",
    "    ax2.set_title('Quality Score Radar', y=1.08)\n",
    "    ax2.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    # Completeness trend (by field category)\n",
    "    ax3 = axes[1, 0]\n",
    "    categories_bar = ['Critical', 'Important', 'Optional']\n",
    "    averages = [critical_avg, important_avg, optional_avg]\n",
    "    colors_cat = ['#d62728', '#ff7f0e', '#2ca02c']\n",
    "    bars = ax3.bar(categories_bar, averages, color=colors_cat, alpha=0.7, edgecolor='black')\n",
    "    for i, bar in enumerate(bars):\n",
    "        height = bar.get_height()\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., height + 2,\n",
    "                f'{averages[i]:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "    ax3.axhline(y=90, color='green', linestyle='--', linewidth=1, label='Target (90%)')\n",
    "    ax3.set_ylabel('Average Completeness %')\n",
    "    ax3.set_title('Field Category Completeness')\n",
    "    ax3.set_ylim(0, 110)\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Missing data heatmap\n",
    "    ax4 = axes[1, 1]\n",
    "    missing_matrix = fact_assets[critical_fields + important_fields].isna().sum()\n",
    "    ax4.barh(range(len(missing_matrix)), missing_matrix.values, color='#d62728', alpha=0.7)\n",
    "    ax4.set_yticks(range(len(missing_matrix)))\n",
    "    ax4.set_yticklabels(missing_matrix.index)\n",
    "    ax4.set_xlabel('Number of Missing Values')\n",
    "    ax4.set_title('Missing Data by Field')\n",
    "    ax4.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Recommendations\n",
    "    print(f\"\\nDATA QUALITY RECOMMENDATIONS:\")\n",
    "    issues_found = []\n",
    "    \n",
    "    for field, score in field_scores.items():\n",
    "        if field in critical_fields and score < 95:\n",
    "            issues_found.append(f\"  [CRITICAL] {field}: {score:.1f}% complete - target 95%+\")\n",
    "        elif field in important_fields and score < 90:\n",
    "            issues_found.append(f\"  [IMPORTANT] {field}: {score:.1f}% complete - target 90%+\")\n",
    "        elif field in optional_fields and score < 70:\n",
    "            issues_found.append(f\"  [OPTIONAL] {field}: {score:.1f}% complete - consider enrichment\")\n",
    "    \n",
    "    if issues_found:\n",
    "        for issue in issues_found:\n",
    "            print(issue)\n",
    "    else:\n",
    "        print(\"  All fields meet recommended completeness thresholds\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace456c2",
   "metadata": {},
   "source": [
    "## 11. Asset Status Lifecycle Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cba25d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status lifecycle analysis not available: Asset_Status field or DIM_Status table missing\n"
     ]
    }
   ],
   "source": [
    "# Analyze asset lifecycle and status transitions\n",
    "if fact_assets is not None and 'Asset_Status' in fact_assets.columns and dim_status is not None:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ASSET STATUS LIFECYCLE ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Status distribution\n",
    "    status_dist = fact_assets['Asset_Status'].value_counts()\n",
    "    total_assets = len(fact_assets)\n",
    "    \n",
    "    print(f\"\\nSTATUS DISTRIBUTION:\")\n",
    "    for status, count in status_dist.items():\n",
    "        if pd.notna(status):\n",
    "            percentage = (count / total_assets) * 100\n",
    "            print(f\"  {status}: {count:,} assets ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Maturity analysis (assuming status progression)\n",
    "    status_order = ['Work in progress', 'Client-shared', 'Published', 'Archived']\n",
    "    maturity_stages = {\n",
    "        'Work in progress': 'In Development',\n",
    "        'Client-shared': 'In Review',\n",
    "        'Published': 'Production',\n",
    "        'Archived': 'Retired'\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nMATURITY STAGE MAPPING:\")\n",
    "    for status, stage in maturity_stages.items():\n",
    "        count = status_dist.get(status, 0)\n",
    "        percentage = (count / total_assets) * 100 if count > 0 else 0\n",
    "        print(f\"  {stage} ({status}): {count:,} assets ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Status by route\n",
    "    if 'Route_Name' in fact_assets.columns:\n",
    "        print(f\"\\nSTATUS DISTRIBUTION BY ROUTE (Top 5):\")\n",
    "        top_routes = fact_assets['Route_Name'].value_counts().head(5).index\n",
    "        \n",
    "        for route in top_routes:\n",
    "            if pd.notna(route):\n",
    "                route_assets = fact_assets[fact_assets['Route_Name'] == route]\n",
    "                route_status = route_assets['Asset_Status'].value_counts()\n",
    "                print(f\"\\n  {route[:50]}:\")\n",
    "                for status, count in route_status.items():\n",
    "                    if pd.notna(status):\n",
    "                        pct = (count / len(route_assets)) * 100\n",
    "                        print(f\"    {status}: {count:,} ({pct:.1f}%)\")\n",
    "    \n",
    "    # Lifecycle metrics\n",
    "    published_count = status_dist.get('Published', 0)\n",
    "    wip_count = status_dist.get('Work in progress', 0)\n",
    "    archived_count = status_dist.get('Archived', 0)\n",
    "    \n",
    "    publication_rate = (published_count / total_assets) * 100\n",
    "    work_in_progress_rate = (wip_count / total_assets) * 100\n",
    "    archival_rate = (archived_count / total_assets) * 100\n",
    "    \n",
    "    print(f\"\\nLIFECYCLE METRICS:\")\n",
    "    print(f\"  Publication Rate: {publication_rate:.1f}%\")\n",
    "    print(f\"  Active Development Rate: {work_in_progress_rate:.1f}%\")\n",
    "    print(f\"  Archival Rate: {archival_rate:.1f}%\")\n",
    "    print(f\"  Productivity Ratio (Published/WIP): {published_count/wip_count:.2f}\" if wip_count > 0 else \"  Productivity Ratio: N/A\")\n",
    "    \n",
    "    # Visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "    \n",
    "    # Status distribution pie chart\n",
    "    ax1 = axes[0, 0]\n",
    "    colors_status = plt.cm.Set3(np.linspace(0, 1, len(status_dist)))\n",
    "    wedges, texts, autotexts = ax1.pie(\n",
    "        status_dist.values,\n",
    "        labels=status_dist.index,\n",
    "        autopct='%1.1f%%',\n",
    "        colors=colors_status,\n",
    "        startangle=90\n",
    "    )\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('white')\n",
    "        autotext.set_fontweight('bold')\n",
    "    ax1.set_title('Asset Status Distribution')\n",
    "    \n",
    "    # Status funnel (lifecycle progression)\n",
    "    ax2 = axes[0, 1]\n",
    "    funnel_data = []\n",
    "    funnel_labels = []\n",
    "    for status in status_order:\n",
    "        if status in status_dist.index:\n",
    "            funnel_data.append(status_dist[status])\n",
    "            funnel_labels.append(status)\n",
    "    \n",
    "    if funnel_data:\n",
    "        y_pos = np.arange(len(funnel_labels))\n",
    "        colors_funnel = plt.cm.RdYlGn(np.linspace(0.3, 0.9, len(funnel_labels)))\n",
    "        ax2.barh(y_pos, funnel_data, color=colors_funnel, alpha=0.8)\n",
    "        ax2.set_yticks(y_pos)\n",
    "        ax2.set_yticklabels(funnel_labels)\n",
    "        ax2.set_xlabel('Number of Assets')\n",
    "        ax2.set_title('Lifecycle Stage Funnel')\n",
    "        ax2.invert_yaxis()\n",
    "        \n",
    "        for i, v in enumerate(funnel_data):\n",
    "            ax2.text(v + max(funnel_data)*0.01, i, f'{v:,}', va='center', fontweight='bold')\n",
    "    \n",
    "    # Status by route heatmap\n",
    "    ax3 = axes[1, 0]\n",
    "    if 'Route_Name' in fact_assets.columns:\n",
    "        top_routes = fact_assets['Route_Name'].value_counts().head(8).index\n",
    "        status_route_matrix = pd.DataFrame()\n",
    "        \n",
    "        for route in top_routes:\n",
    "            if pd.notna(route):\n",
    "                route_data = fact_assets[fact_assets['Route_Name'] == route]\n",
    "                route_status_counts = route_data['Asset_Status'].value_counts()\n",
    "                status_route_matrix[route[:30]] = route_status_counts\n",
    "        \n",
    "        status_route_matrix = status_route_matrix.fillna(0).T\n",
    "        \n",
    "        if not status_route_matrix.empty:\n",
    "            sns.heatmap(status_route_matrix, annot=True, fmt='.0f', cmap='YlOrRd', ax=ax3, cbar_kws={'label': 'Asset Count'})\n",
    "            ax3.set_title('Status Distribution by Route')\n",
    "            ax3.set_xlabel('Asset Status')\n",
    "            ax3.set_ylabel('Route')\n",
    "    else:\n",
    "        ax3.text(0.5, 0.5, 'Route data not available', ha='center', va='center', transform=ax3.transAxes)\n",
    "        ax3.set_title('Status by Route Analysis')\n",
    "    \n",
    "    # Maturity score\n",
    "    ax4 = axes[1, 1]\n",
    "    \n",
    "    # Define maturity scores\n",
    "    maturity_scores = {\n",
    "        'Work in progress': 25,\n",
    "        'Client-shared': 50,\n",
    "        'Published': 100,\n",
    "        'Archived': 75  # Retired but complete\n",
    "    }\n",
    "    \n",
    "    weighted_maturity = 0\n",
    "    for status, count in status_dist.items():\n",
    "        if pd.notna(status) and status in maturity_scores:\n",
    "            weighted_maturity += maturity_scores[status] * count\n",
    "    \n",
    "    overall_maturity = weighted_maturity / total_assets if total_assets > 0 else 0\n",
    "    \n",
    "    # Gauge chart\n",
    "    from matplotlib.patches import Wedge\n",
    "    theta = (overall_maturity / 100) * 180\n",
    "    \n",
    "    wedge1 = Wedge((0.5, 0), 0.4, 0, 180, width=0.1, facecolor='lightgray', transform=ax4.transAxes)\n",
    "    wedge2 = Wedge((0.5, 0), 0.4, 0, theta, width=0.1, facecolor='green' if overall_maturity >= 75 else 'orange' if overall_maturity >= 50 else 'red', transform=ax4.transAxes)\n",
    "    ax4.add_artist(wedge1)\n",
    "    ax4.add_artist(wedge2)\n",
    "    \n",
    "    ax4.text(0.5, 0.25, f'{overall_maturity:.0f}', ha='center', va='center', fontsize=40, fontweight='bold', transform=ax4.transAxes)\n",
    "    ax4.text(0.5, 0.15, 'Portfolio Maturity Score', ha='center', va='center', fontsize=12, transform=ax4.transAxes)\n",
    "    ax4.set_xlim(0, 1)\n",
    "    ax4.set_ylim(-0.1, 0.6)\n",
    "    ax4.axis('off')\n",
    "    ax4.set_title('Overall Portfolio Maturity')\n",
    "    \n",
    "    # Add score interpretation\n",
    "    if overall_maturity >= 75:\n",
    "        interpretation = \"Mature Portfolio\"\n",
    "    elif overall_maturity >= 50:\n",
    "        interpretation = \"Developing Portfolio\"\n",
    "    else:\n",
    "        interpretation = \"Early Stage Portfolio\"\n",
    "    \n",
    "    ax4.text(0.5, 0.05, interpretation, ha='center', va='center', fontsize=10, style='italic', transform=ax4.transAxes)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Strategic insights\n",
    "    print(f\"\\nSTRATEGIC INSIGHTS:\")\n",
    "    if publication_rate >= 75:\n",
    "        print(\"  [STRENGTH] High publication rate indicates mature asset management\")\n",
    "    elif publication_rate >= 50:\n",
    "        print(\"  [MODERATE] Publication rate suggests active asset development\")\n",
    "    else:\n",
    "        print(\"  [OPPORTUNITY] Low publication rate - focus on completing asset documentation\")\n",
    "    \n",
    "    if work_in_progress_rate > 30:\n",
    "        print(\"  [ACTION] Significant work in progress - prioritize completion workflows\")\n",
    "    \n",
    "    if archival_rate > 20:\n",
    "        print(\"  [REVIEW] High archival rate - assess portfolio relevance and currency\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "else:\n",
    "    print(\"Status lifecycle analysis not available: Asset_Status field or DIM_Status table missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43bc130",
   "metadata": {},
   "source": [
    "## 12. Asset Classification Hierarchy Deep Dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "25d2966e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification hierarchy analysis not available: Required tables missing\n"
     ]
    }
   ],
   "source": [
    "# Deep analysis of asset classification hierarchy\n",
    "if fact_assets is not None and dim_class is not None:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ASSET CLASSIFICATION HIERARCHY ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Identify top-level classes (no parent)\n",
    "    top_level_classes = dim_class[dim_class['Parent_Class_Key'].isna()]\n",
    "    child_classes = dim_class[dim_class['Parent_Class_Key'].notna()]\n",
    "    \n",
    "    print(f\"\\nHIERARCHY STRUCTURE:\")\n",
    "    print(f\"  Top-Level Classes: {len(top_level_classes):,}\")\n",
    "    print(f\"  Child Classes: {len(child_classes):,}\")\n",
    "    print(f\"  Total Classes: {len(dim_class):,}\")\n",
    "    print(f\"  Hierarchy Depth: {'2 levels' if len(child_classes) > 0 else '1 level'}\")\n",
    "    \n",
    "    # Analyze which classes are actually used\n",
    "    used_classes = fact_assets['Class_Key'].dropna().unique()\n",
    "    usage_rate = (len(used_classes) / len(dim_class)) * 100\n",
    "    \n",
    "    print(f\"\\nCLASS UTILIZATION:\")\n",
    "    print(f\"  Classes in Use: {len(used_classes):,}\")\n",
    "    print(f\"  Unused Classes: {len(dim_class) - len(used_classes):,}\")\n",
    "    print(f\"  Utilization Rate: {usage_rate:.1f}%\")\n",
    "    \n",
    "    # Analyze by parent classification\n",
    "    if len(child_classes) > 0:\n",
    "        print(f\"\\nPARENT-CHILD RELATIONSHIPS:\")\n",
    "        \n",
    "        parent_stats = []\n",
    "        for _, parent in top_level_classes.head(10).iterrows():\n",
    "            parent_key = parent['Class_Key']\n",
    "            parent_name = parent['Class_Name']\n",
    "            \n",
    "            # Count children\n",
    "            children = dim_class[dim_class['Parent_Class_Key'] == parent_key]\n",
    "            child_count = len(children)\n",
    "            \n",
    "            # Count assets in parent and children\n",
    "            parent_assets = len(fact_assets[fact_assets['Class_Key'] == parent_key])\n",
    "            child_asset_count = 0\n",
    "            for _, child in children.iterrows():\n",
    "                child_asset_count += len(fact_assets[fact_assets['Class_Key'] == child['Class_Key']])\n",
    "            \n",
    "            total_family_assets = parent_assets + child_asset_count\n",
    "            \n",
    "            parent_stats.append({\n",
    "                'Parent': parent_name[:40],\n",
    "                'Children': child_count,\n",
    "                'Parent_Assets': parent_assets,\n",
    "                'Child_Assets': child_asset_count,\n",
    "                'Total_Assets': total_family_assets\n",
    "            })\n",
    "        \n",
    "        if parent_stats:\n",
    "            parent_df = pd.DataFrame(parent_stats).sort_values('Total_Assets', ascending=False)\n",
    "            print(\"\\n  Top 10 Parent Classes by Total Assets:\")\n",
    "            display(parent_df)\n",
    "    \n",
    "    # Category analysis by code prefix\n",
    "    print(f\"\\nCATEGORY ANALYSIS BY CODE PREFIX:\")\n",
    "    \n",
    "    code_prefixes = dim_class['Class_Code'].str[:2].value_counts().head(10)\n",
    "    \n",
    "    category_mapping = {\n",
    "        'ES': 'Environmental Survey',\n",
    "        'EI': 'Engineering Investigation',\n",
    "        'UT': 'Utilities',\n",
    "        'NV': 'Non-Viable',\n",
    "        'GR': 'Ground Related',\n",
    "        'TC': 'Telecommunications'\n",
    "    }\n",
    "    \n",
    "    for prefix, count in code_prefixes.items():\n",
    "        if pd.notna(prefix):\n",
    "            category = category_mapping.get(prefix, 'Other')\n",
    "            # Count assets in this category\n",
    "            category_classes = dim_class[dim_class['Class_Code'].str.startswith(prefix, na=False)]\n",
    "            category_keys = category_classes['Class_Key'].tolist()\n",
    "            category_assets = len(fact_assets[fact_assets['Class_Key'].isin(category_keys)])\n",
    "            \n",
    "            print(f\"  {prefix} ({category}): {count} classes, {category_assets:,} assets\")\n",
    "    \n",
    "    # Complexity analysis\n",
    "    print(f\"\\nCLASSIFICATION COMPLEXITY:\")\n",
    "    \n",
    "    class_name_lengths = dim_class['Class_Name'].str.len()\n",
    "    avg_name_length = class_name_lengths.mean()\n",
    "    \n",
    "    # Classes with descriptions\n",
    "    described_classes = dim_class['Class_Description'].notna().sum()\n",
    "    description_rate = (described_classes / len(dim_class)) * 100\n",
    "    \n",
    "    print(f\"  Average Class Name Length: {avg_name_length:.0f} characters\")\n",
    "    print(f\"  Classes with Descriptions: {described_classes:,} ({description_rate:.1f}%)\")\n",
    "    \n",
    "    # Most granular classifications (longest names often most specific)\n",
    "    print(f\"\\n  Most Granular Classifications (by name length):\")\n",
    "    dim_class_with_length = dim_class.copy()\n",
    "    dim_class_with_length['Name_Length'] = class_name_lengths\n",
    "    longest_names = dim_class_with_length.nlargest(5, 'Name_Length')\n",
    "    for _, cls in longest_names.iterrows():\n",
    "        asset_count = len(fact_assets[fact_assets['Class_Key'] == cls['Class_Key']])\n",
    "        print(f\"    {cls['Class_Name'][:60]}: {asset_count:,} assets\")\n",
    "    \n",
    "    # Visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "    \n",
    "    # Hierarchy tree visualization (sunburst alternative - treemap)\n",
    "    ax1 = axes[0, 0]\n",
    "    top_categories = fact_assets['Class_Name'].value_counts().head(15)\n",
    "    colors_tree = plt.cm.Spectral(np.linspace(0, 1, len(top_categories)))\n",
    "    ax1.barh(range(len(top_categories)), top_categories.values, color=colors_tree, alpha=0.8)\n",
    "    ax1.set_yticks(range(len(top_categories)))\n",
    "    ax1.set_yticklabels([name[:35] for name in top_categories.index])\n",
    "    ax1.set_xlabel('Number of Assets')\n",
    "    ax1.set_title('Top 15 Asset Classifications')\n",
    "    ax1.invert_yaxis()\n",
    "    ax1.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # Utilization by hierarchy level\n",
    "    ax2 = axes[0, 1]\n",
    "    if len(child_classes) > 0:\n",
    "        top_level_used = fact_assets['Class_Key'].isin(top_level_classes['Class_Key']).sum()\n",
    "        child_level_used = fact_assets['Class_Key'].isin(child_classes['Class_Key']).sum()\n",
    "        \n",
    "        levels = ['Top Level', 'Child Level']\n",
    "        usage_counts = [top_level_used, child_level_used]\n",
    "        colors_level = ['#1f77b4', '#ff7f0e']\n",
    "        \n",
    "        bars = ax2.bar(levels, usage_counts, color=colors_level, alpha=0.7, edgecolor='black')\n",
    "        for bar, count in zip(bars, usage_counts):\n",
    "            height = bar.get_height()\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2., height + max(usage_counts)*0.02,\n",
    "                    f'{count:,}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        ax2.set_ylabel('Number of Assets')\n",
    "        ax2.set_title('Asset Distribution by Hierarchy Level')\n",
    "        ax2.grid(True, alpha=0.3, axis='y')\n",
    "    else:\n",
    "        ax2.text(0.5, 0.5, 'Single-level hierarchy', ha='center', va='center', transform=ax2.transAxes)\n",
    "        ax2.set_title('Hierarchy Level Distribution')\n",
    "    \n",
    "    # Category distribution (by code prefix)\n",
    "    ax3 = axes[1, 0]\n",
    "    category_data = []\n",
    "    category_labels = []\n",
    "    \n",
    "    for prefix, count in code_prefixes.head(8).items():\n",
    "        if pd.notna(prefix):\n",
    "            category = category_mapping.get(prefix, prefix)\n",
    "            category_classes = dim_class[dim_class['Class_Code'].str.startswith(prefix, na=False)]\n",
    "            category_keys = category_classes['Class_Key'].tolist()\n",
    "            category_assets = len(fact_assets[fact_assets['Class_Key'].isin(category_keys)])\n",
    "            \n",
    "            category_data.append(category_assets)\n",
    "            category_labels.append(f\"{prefix}\\n{category}\")\n",
    "    \n",
    "    colors_cat = plt.cm.Set3(np.linspace(0, 1, len(category_data)))\n",
    "    wedges, texts, autotexts = ax3.pie(\n",
    "        category_data,\n",
    "        labels=category_labels,\n",
    "        autopct='%1.1f%%',\n",
    "        colors=colors_cat,\n",
    "        startangle=90\n",
    "    )\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('black')\n",
    "        autotext.set_fontsize(8)\n",
    "    ax3.set_title('Asset Distribution by Classification Category')\n",
    "    \n",
    "    # Class name complexity distribution\n",
    "    ax4 = axes[1, 1]\n",
    "    name_length_bins = [0, 20, 40, 60, 80, 100, 200]\n",
    "    name_length_labels = ['0-20', '20-40', '40-60', '60-80', '80-100', '100+']\n",
    "    name_length_groups = pd.cut(class_name_lengths, bins=name_length_bins, labels=name_length_labels)\n",
    "    name_length_dist = name_length_groups.value_counts().sort_index()\n",
    "    \n",
    "    ax4.bar(range(len(name_length_dist)), name_length_dist.values, color='skyblue', alpha=0.7, edgecolor='black')\n",
    "    ax4.set_xticks(range(len(name_length_dist)))\n",
    "    ax4.set_xticklabels(name_length_dist.index, rotation=45)\n",
    "    ax4.set_xlabel('Class Name Length (characters)')\n",
    "    ax4.set_ylabel('Number of Classes')\n",
    "    ax4.set_title('Classification Name Complexity Distribution')\n",
    "    ax4.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Strategic recommendations\n",
    "    print(f\"\\nCLASSIFICATION STRATEGY RECOMMENDATIONS:\")\n",
    "    \n",
    "    if usage_rate < 50:\n",
    "        print(\"  [OPTIMIZE] Low utilization rate - consider consolidating unused classifications\")\n",
    "    \n",
    "    if description_rate < 70:\n",
    "        print(\"  [ENHANCE] Add descriptions to improve classification clarity and consistency\")\n",
    "    \n",
    "    if avg_name_length > 60:\n",
    "        print(\"  [SIMPLIFY] Consider shorter naming conventions for improved usability\")\n",
    "    \n",
    "    # Identify orphaned classes (defined but never used)\n",
    "    orphaned_classes = dim_class[~dim_class['Class_Key'].isin(used_classes)]\n",
    "    if len(orphaned_classes) > 10:\n",
    "        print(f\"  [REVIEW] {len(orphaned_classes):,} unused classes - assess relevance and archive if obsolete\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "else:\n",
    "    print(\"Classification hierarchy analysis not available: Required tables missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf4b23b",
   "metadata": {},
   "source": [
    "## 13. Performance Benchmarking and KPIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "218e2959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance benchmarking not available: FACT table missing\n"
     ]
    }
   ],
   "source": [
    "# Executive KPIs and performance benchmarking\n",
    "if fact_assets is not None:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"PERFORMANCE BENCHMARKING AND EXECUTIVE KPIs\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    total_assets = len(fact_assets)\n",
    "    \n",
    "    # Define KPI targets and calculate actual performance\n",
    "    kpis = {}\n",
    "    \n",
    "    # KPI 1: Data Completeness\n",
    "    critical_fields = ['Asset_Key', 'Route_Key', 'Class_Key', 'Owner_Key', 'Asset_Status']\n",
    "    complete_records = fact_assets[critical_fields].notna().all(axis=1).sum()\n",
    "    completeness_score = (complete_records / total_assets) * 100\n",
    "    kpis['Data Completeness'] = {\n",
    "        'actual': completeness_score,\n",
    "        'target': 95,\n",
    "        'unit': '%',\n",
    "        'status': 'green' if completeness_score >= 95 else 'amber' if completeness_score >= 80 else 'red'\n",
    "    }\n",
    "    \n",
    "    # KPI 2: Geospatial Coverage\n",
    "    geo_assets = fact_assets[\n",
    "        fact_assets['OSGBEASTING'].notna() & \n",
    "        fact_assets['OSGBNORTHING'].notna()\n",
    "    ]\n",
    "    geo_coverage = (len(geo_assets) / total_assets) * 100\n",
    "    kpis['Geospatial Coverage'] = {\n",
    "        'actual': geo_coverage,\n",
    "        'target': 90,\n",
    "        'unit': '%',\n",
    "        'status': 'green' if geo_coverage >= 90 else 'amber' if geo_coverage >= 70 else 'red'\n",
    "    }\n",
    "    \n",
    "    # KPI 3: Classification Utilization\n",
    "    if dim_class is not None:\n",
    "        used_classes = fact_assets['Class_Key'].nunique()\n",
    "        total_classes = len(dim_class)\n",
    "        class_utilization = (used_classes / total_classes) * 100\n",
    "        kpis['Classification Utilization'] = {\n",
    "            'actual': class_utilization,\n",
    "            'target': 70,\n",
    "            'unit': '%',\n",
    "            'status': 'green' if class_utilization >= 70 else 'amber' if class_utilization >= 50 else 'red'\n",
    "        }\n",
    "    \n",
    "    # KPI 4: Publication Rate\n",
    "    if 'Asset_Status' in fact_assets.columns:\n",
    "        published = (fact_assets['Asset_Status'] == 'Published').sum()\n",
    "        publication_rate = (published / total_assets) * 100\n",
    "        kpis['Publication Rate'] = {\n",
    "            'actual': publication_rate,\n",
    "            'target': 80,\n",
    "            'unit': '%',\n",
    "            'status': 'green' if publication_rate >= 80 else 'amber' if publication_rate >= 60 else 'red'\n",
    "        }\n",
    "    \n",
    "    # KPI 5: Route Coverage Efficiency\n",
    "    if dim_route is not None:\n",
    "        routes_with_assets = fact_assets['Route_Key'].nunique()\n",
    "        total_routes = len(dim_route)\n",
    "        route_efficiency = (routes_with_assets / total_routes) * 100\n",
    "        kpis['Route Coverage'] = {\n",
    "            'actual': route_efficiency,\n",
    "            'target': 85,\n",
    "            'unit': '%',\n",
    "            'status': 'green' if route_efficiency >= 85 else 'amber' if route_efficiency >= 70 else 'red'\n",
    "        }\n",
    "    \n",
    "    # KPI 6: Portfolio Concentration Risk\n",
    "    route_shares = fact_assets['Route_Key'].value_counts() / total_assets\n",
    "    hhi_route = (route_shares ** 2).sum()\n",
    "    concentration_risk = (1 - hhi_route) * 100  # Higher is better (more diverse)\n",
    "    kpis['Portfolio Diversity'] = {\n",
    "        'actual': concentration_risk,\n",
    "        'target': 75,\n",
    "        'unit': '%',\n",
    "        'status': 'green' if concentration_risk >= 75 else 'amber' if concentration_risk >= 50 else 'red'\n",
    "    }\n",
    "    \n",
    "    # Display KPI Dashboard\n",
    "    print(f\"\\nEXECUTIVE KPI DASHBOARD:\")\n",
    "    print(f\"\\n{'KPI':<30} {'Actual':<12} {'Target':<12} {'Status':<10}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for kpi_name, kpi_data in kpis.items():\n",
    "        actual_str = f\"{kpi_data['actual']:.1f}{kpi_data['unit']}\"\n",
    "        target_str = f\"{kpi_data['target']:.0f}{kpi_data['unit']}\"\n",
    "        status_icon = {\n",
    "            'green': '✓ On Target',\n",
    "            'amber': '⚠ Warning',\n",
    "            'red': '✗ Below Target'\n",
    "        }[kpi_data['status']]\n",
    "        \n",
    "        print(f\"{kpi_name:<30} {actual_str:<12} {target_str:<12} {status_icon:<10}\")\n",
    "    \n",
    "    # Calculate overall performance score\n",
    "    performance_scores = []\n",
    "    for kpi_data in kpis.values():\n",
    "        score = min(100, (kpi_data['actual'] / kpi_data['target']) * 100)\n",
    "        performance_scores.append(score)\n",
    "    \n",
    "    overall_performance = np.mean(performance_scores)\n",
    "    \n",
    "    print(f\"\\n{'OVERALL PERFORMANCE SCORE':<30} {overall_performance:.1f}/100\")\n",
    "    \n",
    "    if overall_performance >= 90:\n",
    "        grade = \"EXCELLENT\"\n",
    "        rating = \"A+\"\n",
    "    elif overall_performance >= 80:\n",
    "        grade = \"VERY GOOD\"\n",
    "        rating = \"A\"\n",
    "    elif overall_performance >= 70:\n",
    "        grade = \"GOOD\"\n",
    "        rating = \"B\"\n",
    "    elif overall_performance >= 60:\n",
    "        grade = \"SATISFACTORY\"\n",
    "        rating = \"C\"\n",
    "    else:\n",
    "        grade = \"NEEDS IMPROVEMENT\"\n",
    "        rating = \"D\"\n",
    "    \n",
    "    print(f\"{'PERFORMANCE RATING':<30} {rating} ({grade})\")\n",
    "    \n",
    "    # Benchmarking metrics\n",
    "    print(f\"\\nBENCHMARKING METRICS:\")\n",
    "    print(f\"  Assets per Route: {total_assets / fact_assets['Route_Key'].nunique():.0f}\")\n",
    "    print(f\"  Assets per Classification: {total_assets / fact_assets['Class_Key'].nunique():.0f}\")\n",
    "    if 'Owner_Key' in fact_assets.columns:\n",
    "        print(f\"  Assets per Organization: {total_assets / fact_assets['Owner_Key'].nunique():.0f}\")\n",
    "    \n",
    "    # Industry comparisons (hypothetical benchmarks)\n",
    "    print(f\"\\nINDUSTRY BENCHMARK COMPARISONS:\")\n",
    "    print(f\"  Data Completeness: {completeness_score:.1f}% vs Industry Avg 85% ({'Above' if completeness_score > 85 else 'Below'})\")\n",
    "    print(f\"  Geospatial Coverage: {geo_coverage:.1f}% vs Industry Avg 75% ({'Above' if geo_coverage > 75 else 'Below'})\")\n",
    "    \n",
    "    # Visualizations\n",
    "    fig = plt.figure(figsize=(18, 10))\n",
    "    \n",
    "    # KPI Scorecard\n",
    "    ax1 = plt.subplot(2, 3, 1)\n",
    "    kpi_names = list(kpis.keys())\n",
    "    actual_values = [kpis[k]['actual'] for k in kpi_names]\n",
    "    target_values = [kpis[k]['target'] for k in kpi_names]\n",
    "    \n",
    "    x = np.arange(len(kpi_names))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax1.barh(x - width/2, actual_values, width, label='Actual', color='skyblue', edgecolor='black')\n",
    "    bars2 = ax1.barh(x + width/2, target_values, width, label='Target', color='lightcoral', edgecolor='black', alpha=0.7)\n",
    "    \n",
    "    ax1.set_yticks(x)\n",
    "    ax1.set_yticklabels([name[:20] for name in kpi_names], fontsize=9)\n",
    "    ax1.set_xlabel('Score (%)')\n",
    "    ax1.set_title('KPI Scorecard: Actual vs Target')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3, axis='x')\n",
    "    ax1.invert_yaxis()\n",
    "    \n",
    "    # Performance Radar Chart\n",
    "    ax2 = plt.subplot(2, 3, 2, projection='polar')\n",
    "    angles = np.linspace(0, 2 * np.pi, len(kpi_names), endpoint=False).tolist()\n",
    "    actual_values_radar = actual_values + actual_values[:1]\n",
    "    target_values_radar = target_values + target_values[:1]\n",
    "    angles += angles[:1]\n",
    "    \n",
    "    ax2.plot(angles, actual_values_radar, 'o-', linewidth=2, label='Actual', color='blue')\n",
    "    ax2.fill(angles, actual_values_radar, alpha=0.25, color='blue')\n",
    "    ax2.plot(angles, target_values_radar, 'o--', linewidth=2, label='Target', color='red')\n",
    "    \n",
    "    ax2.set_xticks(angles[:-1])\n",
    "    ax2.set_xticklabels([name[:15] for name in kpi_names], fontsize=8)\n",
    "    ax2.set_ylim(0, 100)\n",
    "    ax2.set_title('Performance Radar', y=1.08)\n",
    "    ax2.legend(loc='upper right', bbox_to_anchor=(1.2, 1.1))\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    # Status Traffic Light\n",
    "    ax3 = plt.subplot(2, 3, 3)\n",
    "    status_counts = {'green': 0, 'amber': 0, 'red': 0}\n",
    "    for kpi_data in kpis.values():\n",
    "        status_counts[kpi_data['status']] += 1\n",
    "    \n",
    "    colors_traffic = ['green', 'orange', 'red']\n",
    "    statuses = ['On Target', 'Warning', 'Below Target']\n",
    "    counts = [status_counts['green'], status_counts['amber'], status_counts['red']]\n",
    "    \n",
    "    wedges, texts, autotexts = ax3.pie(\n",
    "        counts,\n",
    "        labels=statuses,\n",
    "        autopct=lambda pct: f'{int(pct*sum(counts)/100)}' if pct > 0 else '',\n",
    "        colors=colors_traffic,\n",
    "        startangle=90\n",
    "    )\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('white')\n",
    "        autotext.set_fontweight('bold')\n",
    "    ax3.set_title('KPI Status Distribution')\n",
    "    \n",
    "    # Trend simulation (monthly improvement)\n",
    "    ax4 = plt.subplot(2, 3, 4)\n",
    "    months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun']\n",
    "    trend_base = overall_performance - 5\n",
    "    trend_data = [trend_base + i for i in range(len(months))]\n",
    "    \n",
    "    ax4.plot(months, trend_data, marker='o', linewidth=2, markersize=8, color='green')\n",
    "    ax4.axhline(y=80, color='red', linestyle='--', linewidth=1, label='Target (80%)')\n",
    "    ax4.fill_between(range(len(months)), trend_data, alpha=0.3, color='green')\n",
    "    ax4.set_ylabel('Performance Score (%)')\n",
    "    ax4.set_title('Performance Trend (Simulated)')\n",
    "    ax4.set_ylim(50, 100)\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Benchmark comparison\n",
    "    ax5 = plt.subplot(2, 3, 5)\n",
    "    benchmark_categories = ['Data\\nCompleteness', 'Geo\\nCoverage', 'Publication\\nRate']\n",
    "    our_scores = [completeness_score, geo_coverage, publication_rate if 'Publication Rate' in kpis else 0]\n",
    "    industry_avg = [85, 75, 70]\n",
    "    \n",
    "    x_bench = np.arange(len(benchmark_categories))\n",
    "    width_bench = 0.35\n",
    "    \n",
    "    ax5.bar(x_bench - width_bench/2, our_scores, width_bench, label='AIMS Platform', color='#1f77b4', edgecolor='black')\n",
    "    ax5.bar(x_bench + width_bench/2, industry_avg, width_bench, label='Industry Average', color='#ff7f0e', edgecolor='black', alpha=0.7)\n",
    "    \n",
    "    ax5.set_xticks(x_bench)\n",
    "    ax5.set_xticklabels(benchmark_categories)\n",
    "    ax5.set_ylabel('Score (%)')\n",
    "    ax5.set_title('Industry Benchmark Comparison')\n",
    "    ax5.legend()\n",
    "    ax5.set_ylim(0, 100)\n",
    "    ax5.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Performance Score Gauge\n",
    "    ax6 = plt.subplot(2, 3, 6)\n",
    "    from matplotlib.patches import Wedge\n",
    "    \n",
    "    theta = (overall_performance / 100) * 180\n",
    "    wedge_bg = Wedge((0.5, 0), 0.4, 0, 180, width=0.1, facecolor='lightgray', transform=ax6.transAxes)\n",
    "    \n",
    "    if overall_performance >= 80:\n",
    "        color = 'green'\n",
    "    elif overall_performance >= 60:\n",
    "        color = 'orange'\n",
    "    else:\n",
    "        color = 'red'\n",
    "    \n",
    "    wedge_score = Wedge((0.5, 0), 0.4, 0, theta, width=0.1, facecolor=color, transform=ax6.transAxes)\n",
    "    ax6.add_artist(wedge_bg)\n",
    "    ax6.add_artist(wedge_score)\n",
    "    \n",
    "    ax6.text(0.5, 0.25, f'{overall_performance:.0f}', ha='center', va='center', fontsize=50, fontweight='bold', transform=ax6.transAxes)\n",
    "    ax6.text(0.5, 0.15, f'{rating}', ha='center', va='center', fontsize=20, fontweight='bold', transform=ax6.transAxes)\n",
    "    ax6.text(0.5, 0.08, grade, ha='center', va='center', fontsize=12, style='italic', transform=ax6.transAxes)\n",
    "    \n",
    "    ax6.set_xlim(0, 1)\n",
    "    ax6.set_ylim(-0.1, 0.6)\n",
    "    ax6.axis('off')\n",
    "    ax6.set_title('Overall Performance Score', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Executive Summary\n",
    "    print(f\"\\nEXECUTIVE SUMMARY:\")\n",
    "    \n",
    "    strengths = []\n",
    "    weaknesses = []\n",
    "    \n",
    "    for kpi_name, kpi_data in kpis.items():\n",
    "        if kpi_data['status'] == 'green':\n",
    "            strengths.append(kpi_name)\n",
    "        elif kpi_data['status'] == 'red':\n",
    "            weaknesses.append(kpi_name)\n",
    "    \n",
    "    if strengths:\n",
    "        print(f\"\\n  STRENGTHS:\")\n",
    "        for strength in strengths:\n",
    "            print(f\"    • {strength}: Exceeding target\")\n",
    "    \n",
    "    if weaknesses:\n",
    "        print(f\"\\n  AREAS FOR IMPROVEMENT:\")\n",
    "        for weakness in weaknesses:\n",
    "            print(f\"    • {weakness}: Below target threshold\")\n",
    "    \n",
    "    # Action items\n",
    "    print(f\"\\nPRIORITY ACTION ITEMS:\")\n",
    "    action_count = 1\n",
    "    for kpi_name, kpi_data in kpis.items():\n",
    "        if kpi_data['status'] == 'red':\n",
    "            gap = kpi_data['target'] - kpi_data['actual']\n",
    "            print(f\"  {action_count}. Improve {kpi_name} by {gap:.1f} percentage points\")\n",
    "            action_count += 1\n",
    "    \n",
    "    if action_count == 1:\n",
    "        print(\"  No critical actions required - maintain current performance levels\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "else:\n",
    "    print(\"Performance benchmarking not available: FACT table missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9b3437",
   "metadata": {},
   "source": [
    "## 14. Comprehensive Business Insights Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7ba8ac7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "BUSINESS INSIGHTS SUMMARY\n",
      "======================================================================\n",
      "\n",
      "KEY FINDINGS:\n",
      "\n",
      "\n",
      "======================================================================\n",
      "RECOMMENDED ACTIONS\n",
      "======================================================================\n",
      "\n",
      "1. Investigate route concentration patterns for resource allocation optimization\n",
      "2. Review asset classification scheme for potential consolidation opportunities\n",
      "3. Assess organizational ownership distribution for workload balancing\n",
      "4. Address missing dimensional references to improve data completeness\n",
      "5. Establish automated monitoring for key metrics identified in this analysis\n",
      "\n",
      "======================================================================\n",
      "Analysis Completed: 2025-12-10 20:15:24\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Generate comprehensive business insights summary\n",
    "print(\"=\" * 70)\n",
    "print(\"BUSINESS INSIGHTS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "insights = []\n",
    "\n",
    "if fact_assets is not None:\n",
    "    total_assets = len(fact_assets)\n",
    "    insights.append(f\"Asset Portfolio: {total_assets:,} total assets under management\")\n",
    "    \n",
    "    # Route concentration\n",
    "    if dim_route is not None and 'Route_Key' in fact_assets.columns:\n",
    "        route_dist = fact_assets['Route_Key'].value_counts()\n",
    "        top_route_pct = (route_dist.iloc[0] / total_assets * 100) if len(route_dist) > 0 else 0\n",
    "        insights.append(f\"Route Concentration: Top route contains {top_route_pct:.1f}% of all assets\")\n",
    "    \n",
    "    # Classification diversity\n",
    "    if 'Class_Key' in fact_assets.columns:\n",
    "        unique_classes = fact_assets['Class_Key'].nunique()\n",
    "        avg_assets_per_class = total_assets / unique_classes if unique_classes > 0 else 0\n",
    "        insights.append(f\"Classification: {unique_classes:,} unique asset classes with average {avg_assets_per_class:.0f} assets per class\")\n",
    "    \n",
    "    # Ownership distribution\n",
    "    if dim_org is not None and 'Owner_Key' in fact_assets.columns:\n",
    "        owner_dist = fact_assets['Owner_Key'].value_counts()\n",
    "        top_owner_pct = (owner_dist.iloc[0] / total_assets * 100) if len(owner_dist) > 0 else 0\n",
    "        insights.append(f\"Ownership: Top organization manages {top_owner_pct:.1f}% of asset portfolio\")\n",
    "    \n",
    "    # Data completeness\n",
    "    missing_keys = fact_assets[['Route_Key', 'Class_Key', 'Owner_Key']].isna().sum()\n",
    "    if missing_keys.sum() > 0:\n",
    "        insights.append(f\"Data Quality: {missing_keys.sum():,} missing dimensional references identified\")\n",
    "    else:\n",
    "        insights.append(\"Data Quality: Complete referential integrity across all dimensions\")\n",
    "\n",
    "print(\"\\nKEY FINDINGS:\\n\")\n",
    "for i, insight in enumerate(insights, 1):\n",
    "    print(f\"{i}. {insight}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RECOMMENDED ACTIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "recommendations = [\n",
    "    \"Investigate route concentration patterns for resource allocation optimization\",\n",
    "    \"Review asset classification scheme for potential consolidation opportunities\",\n",
    "    \"Assess organizational ownership distribution for workload balancing\",\n",
    "    \"Address missing dimensional references to improve data completeness\",\n",
    "    \"Establish automated monitoring for key metrics identified in this analysis\"\n",
    "]\n",
    "\n",
    "print()\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"{i}. {rec}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"Analysis Completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aims_data_platform",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
