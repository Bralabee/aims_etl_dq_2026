{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96471623",
   "metadata": {},
   "source": [
    "# AIMS Data Ingestion Pipeline with DQ Gatekeeping\n",
    "\n",
    "This notebook executes the data ingestion process with Data Quality checks.\n",
    "\n",
    "## Purpose\n",
    "- Load data files from local source directory\n",
    "- Validate each file against its DQ config\n",
    "- If PASSED: Process and mark as complete\n",
    "- If FAILED: Quarantine and log for review\n",
    "- Track processed files using watermarks\n",
    "\n",
    "## Local Execution\n",
    "This notebook runs entirely locally using the sample data in `data/Samples_LH_Bronze_Aims_26_parquet/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6640b8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Install the library (if not already installed in the environment)\n",
    "from pathlib import Path\n",
    "\n",
    "# Detect Environment (avoid slow/hanging imports locally)\n",
    "IS_FABRIC = Path(\"/lakehouse/default/Files\").exists()\n",
    "if IS_FABRIC:\n",
    "    try:\n",
    "        from notebookutils import mssparkutils  # noqa: F401\n",
    "    except Exception:\n",
    "        mssparkutils = None\n",
    "    print(\"Running in Microsoft Fabric\")\n",
    "else:\n",
    "    print(\"Running Locally\")\n",
    "\n",
    "if not IS_FABRIC:\n",
    "    wheel_candidates = []\n",
    "    wheel_candidates += sorted(Path(\"../dq_great_expectations/dq_package_dist\").glob(\"fabric_data_quality-*.whl\"))\n",
    "    wheel_candidates += sorted(Path(\"../dist\").glob(\"fabric_data_quality-*.whl\"))\n",
    "    wheel_candidates += sorted(Path(\"../../2_DATA_QUALITY_LIBRARY/dist\").glob(\"fabric_data_quality-*.whl\"))\n",
    "\n",
    "    if not wheel_candidates:\n",
    "        raise FileNotFoundError(\n",
    "            \"No fabric_data_quality wheel found. Build one in 2_DATA_QUALITY_LIBRARY/dist or place it in dq_great_expectations/dq_package_dist.\"\n",
    "        )\n",
    "\n",
    "    wheel_path = wheel_candidates[-1]\n",
    "    print(f\"Installing: {wheel_path}\")\n",
    "    %pip install {wheel_path} --force-reinstall\n",
    "    # %pip install adlfs\n",
    "else:\n",
    "    # Fabric: We assume the library is installed via the Fabric Environment (Workspace Settings).\n",
    "    # If you need to install it manually from Files, uncomment the line below:\n",
    "    # %pip install /lakehouse/default/Files/libs/fabric_data_quality-*.whl --force-reinstall\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647385da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "   Environment: Local\n",
      "   Bronze (Source): /home/sanmi/Documents/HS2/HS2_PROJECTS_2025/AIMS_LOCAL/data/Samples_LH_Bronze_Aims_26_parquet\n",
      "   Silver (Target): /home/sanmi/Documents/HS2/HS2_PROJECTS_2025/AIMS_LOCAL/data/Silver\n",
      "   Gold (Target):   /home/sanmi/Documents/HS2/HS2_PROJECTS_2025/AIMS_LOCAL/data/Gold\n",
      "   DQ Configs: /home/sanmi/Documents/HS2/HS2_PROJECTS_2025/AIMS_LOCAL/dq_great_expectations/generated_configs\n",
      "   Watermarks: /home/sanmi/Documents/HS2/HS2_PROJECTS_2025/AIMS_LOCAL/data/state/watermarks.json\n",
      "   DQ Logs: /home/sanmi/Documents/HS2/HS2_PROJECTS_2025/AIMS_LOCAL/data/state/dq_logs.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Configuration & Setup\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import pyarrow.parquet as pq  # Added for empty file detection\n",
    "\n",
    "# Define Paths based on Environment\n",
    "if IS_FABRIC:\n",
    "    # Fabric Paths\n",
    "    BASE_DIR = Path(\"/lakehouse/default/Files\")\n",
    "    \n",
    "    # Try to load .env from the Lakehouse Files root\n",
    "    env_path = BASE_DIR / \".env\"\n",
    "    if env_path.exists():\n",
    "        load_dotenv(dotenv_path=env_path)\n",
    "        print(f\"Loaded configuration from {env_path}\")\n",
    "\n",
    "outputs\n",
    "# Step 3: Import DQ Libraries\n",
    "from dq_framework import DataLoader, DataQualityValidator\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76b44d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Watermark & Logging Helper Functions\n",
    "\n",
    "def load_watermarks():\n",
    "    \"\"\"Load watermarks from JSON file.\"\"\"\n",
    "    if WATERMARK_FILE.exists():\n",
    "        with open(WATERMARK_FILE, 'r') as f:\n",
    "            return json.load(f)\n",
    "    return {}\n",
    "\n",
    "def save_watermark(file_name):\n",
    "    \"\"\"Mark a file as processed.\"\"\"\n",
    "    watermarks = load_watermarks()\n",
    "    watermarks[file_name] = datetime.now().isoformat()\n",
    "    with open(WATERMARK_FILE, 'w') as f:\n",
    "        json.dump(watermarks, f, indent=2)\n",
    "\n",
    "def is_processed(file_name):\n",
    "    \"\"\"Check if file has already been processed.\"\"\"\n",
    "    watermarks = load_watermarks()\n",
    "    return file_name in watermarks\n",
    "\n",
    "def log_dq_result(file_name, status, score, details=None):\n",
    "    \"\"\"Append validation result to JSONL log file.\"\"\"\n",
    "    entry = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"file\": file_name,\n",
    "        \"status\": status,\n",
    "        \"score\": score,\n",
    "        \"details\": details or {}\n",
    "    }\n",
    "    with open(DQ_LOG_FILE, \"a\") as f:\n",
    "        f.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "def quarantine_file(file_path, reason):\n",
    "    \"\"\"Simulate moving file to quarantine.\"\"\"\n",
    "    print(f\"   QUARANTINED: {os.path.basename(file_path)} -> Reason: {reason}\")\n",
    "\n",
    "print(\"Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98b6585e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Ingestion Pipeline for 0 files...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Pipeline Execution Complete\n",
      "============================================================\n",
      "Total Files: 0\n",
      "Processed: 0\n",
      "Passed DQ: 0\n",
      "Failed DQ: 0\n",
      "Skipped (Already Processed or Empty): 0\n",
      "\n",
      "View results in Notebook 03 (Monitoring Dashboard)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Execute Ingestion Pipeline with DQ Gatekeeping\n",
    "\n",
    "# Discover all parquet files in source directory (Bronze)\n",
    "source_files = list(DATA_PATH.glob(\"*.parquet\"))\n",
    "print(f\"Starting Ingestion Pipeline for {len(source_files)} files...\\n\")\n",
    "\n",
    "processed_count = 0\n",
    "passed_count = 0\n",
    "failed_count = 0\n",
    "skipped_count = 0\n",
    "\n",
    "# Use tqdm for progress bar\n",
    "for file_path in tqdm(source_files, desc=\"Processing Files\"):\n",
    "    file_name = file_path.name\n",
    "    \n",
    "    # Check Watermark (Skip if already processed)\n",
    "    if is_processed(file_name):\n",
    "        print(f\"Skipping {file_name} (Already Processed)\")\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "\n",
    "    # Check for Empty File (0 rows)\n",
    "    # We skip these to prevent skewing DQ metrics\n",
    "    try:\n",
    "        if pq.read_metadata(file_path).num_rows == 0:\n",
    "            print(f\"Skipping {file_name} (Empty File - 0 rows)\")\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "    except Exception:\n",
    "        pass # Proceed if check fails (let the pipeline handle it)\n",
    "\n",
    "    print(f\"Processing: {file_name}...\")\n",
    "    \n",
    "    try:\n",
    "        # --- PHASE 1: DQ GATEKEEPING (Validation) ---\n",
    "        # Look for corresponding validation config\n",
    "        config_name = file_name.replace('.parquet', '_validation.yml')\n",
    "        config_path = CONFIG_DIR / config_name\n",
    "        \n",
    "        if not config_path.exists():\n",
    "            print(f\"   Warning: No validation config found. Skipping DQ check.\")\n",
    "            validation_passed = True  # or False for strict mode\n",
    "            score = 0.0\n",
    "            failures = []\n",
    "        else:\n",
    "            # Load data and validate\n",
    "            validator = DataQualityValidator(config_path=str(config_path))\n",
    "            df_batch = DataLoader.load_data(str(file_path), sample_size=100000)\n",
    "            result = validator.validate(df_batch)\n",
    "            \n",
    "            validation_passed = result['success']\n",
    "            score = result['success_rate']\n",
    "            failures = result.get('failed_expectations', [])\n",
    "            \n",
    "            # Log the result\n",
    "            log_dq_result(\n",
    "                file_name, \n",
    "                \"PASSED\" if validation_passed else \"FAILED\", \n",
    "                score, \n",
    "                {\"failed_count\": len(failures), \"failures\": failures[:5]}  # Log first 5 failures\n",
    "            )\n",
    "\n",
    "        # --- PHASE 2: ACTION (Ingest to Silver) ---\n",
    "        if validation_passed:\n",
    "            print(f\"   DQ Passed (Score: {score:.1f}%). Ingesting to Silver...\")\n",
    "            \n",
    "            # Define Silver Path\n",
    "            silver_path = SILVER_DIR / file_name\n",
    "            \n",
    "            # In a real Spark/Fabric scenario, you would read into a DataFrame and write to Delta\n",
    "            # Example:\n",
    "            # df = spark.read.parquet(str(file_path))\n",
    "            # df.write.format(\"delta\").mode(\"overwrite\").save(str(silver_path))\n",
    "            \n",
    "            # For this local/demo simulation, we'll just copy the file if it doesn't exist\n",
    "            if not silver_path.exists():\n",
    "                import shutil\n",
    "                shutil.copy2(file_path, silver_path)\n",
    "                print(f\"   -> Copied to {silver_path}\")\n",
    "            else:\n",
    "                print(f\"   -> File already exists in Silver\")\n",
    "            \n",
    "            # Mark as processed\n",
    "            save_watermark(file_name)\n",
    "            print(f\"   Marked as processed.\")\n",
    "            passed_count += 1\n",
    "            \n",
    "        else:\n",
    "            print(f\"   DQ FAILED (Score: {score:.1f}%). Blocked from ingestion.\")\n",
    "            quarantine_file(file_path, f\"Failed {len(failures)} checks\")\n",
    "            failed_count += 1\n",
    "            \n",
    "        processed_count += 1\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   Pipeline Error: {e}\")\n",
    "        log_dq_result(file_name, \"ERROR\", 0.0, {\"error\": str(e)})\n",
    "        failed_count += 1\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Pipeline Execution Complete\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total Files: {len(source_files)}\")\n",
    "print(f\"Processed: {processed_count}\")\n",
    "print(f\"Passed DQ: {passed_count}\")\n",
    "print(f\"Failed DQ: {failed_count}\")\n",
    "print(f\"Skipped (Already Processed or Empty): {skipped_count}\")\n",
    "print(f\"\\nView results in Notebook 03 (Monitoring Dashboard)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aims_data_platform",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
