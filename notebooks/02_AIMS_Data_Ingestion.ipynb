{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96471623",
   "metadata": {},
   "source": [
    "# AIMS Data Ingestion Pipeline with DQ Gatekeeping\n",
    "\n",
    "This notebook executes the data ingestion process with Data Quality checks.\n",
    "\n",
    "## Purpose\n",
    "- Load data files from local source directory\n",
    "- Validate each file against its DQ config\n",
    "- If PASSED: Process and mark as complete\n",
    "- If FAILED: Quarantine and log for review\n",
    "- Track processed files using watermarks\n",
    "\n",
    "## Local Execution\n",
    "This notebook runs entirely locally using the sample data in `data/Samples_LH_Bronze_Aims_26_parquet/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6640b8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Configuration & Environment Setup\n",
    "# Uses centralized settings - no wheel installation needed\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Import shared utilities\n",
    "from notebooks.config import settings\n",
    "from notebooks.lib import platform_utils\n",
    "from notebooks.lib.storage import StorageManager\n",
    "\n",
    "# Environment info\n",
    "IS_FABRIC = platform_utils.IS_FABRIC\n",
    "print(f\"Running in {'Microsoft Fabric' if IS_FABRIC else 'Local Development'}\")\n",
    "print(f\"Environment: {settings.environment}\")\n",
    "print(f\"Storage Format: {settings.storage_format}\")\n",
    "\n",
    "# Initialize StorageManager for cross-platform file operations\n",
    "storage_manager = StorageManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647385da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "   Environment: Local\n",
      "   Bronze (Source): /home/sanmi/Documents/HS2/HS2_PROJECTS_2025/AIMS_LOCAL/data/Samples_LH_Bronze_Aims_26_parquet\n",
      "   Silver (Target): /home/sanmi/Documents/HS2/HS2_PROJECTS_2025/AIMS_LOCAL/data/Silver\n",
      "   Gold (Target):   /home/sanmi/Documents/HS2/HS2_PROJECTS_2025/AIMS_LOCAL/data/Gold\n",
      "   DQ Configs: /home/sanmi/Documents/HS2/HS2_PROJECTS_2025/AIMS_LOCAL/dq_great_expectations/generated_configs\n",
      "   Watermarks: /home/sanmi/Documents/HS2/HS2_PROJECTS_2025/AIMS_LOCAL/data/state/watermarks.json\n",
      "   DQ Logs: /home/sanmi/Documents/HS2/HS2_PROJECTS_2025/AIMS_LOCAL/data/state/dq_logs.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Configuration & Setup\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import pyarrow.parquet as pq  # For empty file detection\n",
    "\n",
    "# Use centralized paths from settings\n",
    "DATA_PATH = settings.bronze_dir        # Source: Bronze layer\n",
    "SILVER_DIR = settings.silver_dir       # Target: Silver layer\n",
    "CONFIG_DIR = settings.config_dir       # DQ validation configs\n",
    "STATE_DIR = settings.state_dir         # Watermarks and logs\n",
    "QUARANTINE_DIR = settings.quarantine_dir  # Quarantined files\n",
    "\n",
    "# Ensure directories exist\n",
    "platform_utils.ensure_directory(SILVER_DIR)\n",
    "platform_utils.ensure_directory(STATE_DIR)\n",
    "platform_utils.ensure_directory(QUARANTINE_DIR)\n",
    "\n",
    "# Define state files\n",
    "WATERMARK_FILE = STATE_DIR / \"watermarks.json\"\n",
    "DQ_LOG_FILE = STATE_DIR / \"dq_results.jsonl\"\n",
    "\n",
    "# DQ Settings from centralized config\n",
    "DQ_THRESHOLD = settings.dq_threshold  # Pass/fail threshold from settings\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Bronze (Source): {DATA_PATH}\")\n",
    "print(f\"  Silver (Target): {SILVER_DIR}\")\n",
    "print(f\"  Config Dir: {CONFIG_DIR}\")\n",
    "print(f\"  State Dir: {STATE_DIR}\")\n",
    "print(f\"  DQ Threshold: {DQ_THRESHOLD}%\")\n",
    "\n",
    "# Step 3: Import DQ Libraries\n",
    "try:\n",
    "    from aims_data_platform.dq_framework import DataLoader, DataQualityValidator\n",
    "    print(\"✅ Libraries imported from aims_data_platform\")\n",
    "except ImportError:\n",
    "    from dq_framework import DataLoader, DataQualityValidator\n",
    "    print(\"✅ Libraries imported from dq_framework (fallback)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b44d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Watermark & Logging Helper Functions\n",
    "# Uses platform_utils for cross-platform file operations\n",
    "\n",
    "def load_watermarks():\n",
    "    \"\"\"Load watermarks from JSON file.\"\"\"\n",
    "    if platform_utils.file_exists(WATERMARK_FILE):\n",
    "        with open(WATERMARK_FILE, 'r') as f:\n",
    "            return json.load(f)\n",
    "    return {}\n",
    "\n",
    "def save_watermark(file_name):\n",
    "    \"\"\"Mark a file as processed.\"\"\"\n",
    "    watermarks = load_watermarks()\n",
    "    watermarks[file_name] = datetime.now().isoformat()\n",
    "    with open(WATERMARK_FILE, 'w') as f:\n",
    "        json.dump(watermarks, f, indent=2)\n",
    "\n",
    "def is_processed(file_name):\n",
    "    \"\"\"Check if file has already been processed.\"\"\"\n",
    "    watermarks = load_watermarks()\n",
    "    return file_name in watermarks\n",
    "\n",
    "def log_dq_result(file_name, status, score, details=None):\n",
    "    \"\"\"Append validation result to JSONL log file.\"\"\"\n",
    "    entry = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"file\": file_name,\n",
    "        \"status\": status,\n",
    "        \"score\": score,\n",
    "        \"details\": details or {}\n",
    "    }\n",
    "    with open(DQ_LOG_FILE, \"a\") as f:\n",
    "        f.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "def quarantine_file(file_path, reason, df=None):\n",
    "    \"\"\"\n",
    "    Move/copy failed file to quarantine using StorageManager.\n",
    "    If df is provided, writes the DataFrame instead of copying.\n",
    "    \"\"\"\n",
    "    file_name = Path(file_path).name\n",
    "    print(f\"   QUARANTINED: {file_name} -> Reason: {reason}\")\n",
    "    \n",
    "    if df is not None:\n",
    "        # Use StorageManager to quarantine the data\n",
    "        quarantine_path = storage_manager.quarantine_data(\n",
    "            df=df,\n",
    "            table_name=file_name.replace('.parquet', ''),\n",
    "            reason=reason\n",
    "        )\n",
    "        print(f\"   -> Quarantined to: {quarantine_path}\")\n",
    "    else:\n",
    "        # Just log the quarantine action (file stays in bronze)\n",
    "        log_entry = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"file\": file_name,\n",
    "            \"reason\": reason,\n",
    "            \"source_path\": str(file_path)\n",
    "        }\n",
    "        quarantine_log = QUARANTINE_DIR / \"quarantine_log.jsonl\"\n",
    "        with open(quarantine_log, \"a\") as f:\n",
    "            f.write(json.dumps(log_entry) + \"\\n\")\n",
    "\n",
    "print(\"✅ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b6585e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Ingestion Pipeline for 0 files...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Pipeline Execution Complete\n",
      "============================================================\n",
      "Total Files: 0\n",
      "Processed: 0\n",
      "Passed DQ: 0\n",
      "Failed DQ: 0\n",
      "Skipped (Already Processed or Empty): 0\n",
      "\n",
      "View results in Notebook 03 (Monitoring Dashboard)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Execute Ingestion Pipeline with DQ Gatekeeping\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Discover all parquet files in source directory (Bronze)\n",
    "source_files = list(DATA_PATH.glob(\"*.parquet\"))\n",
    "print(f\"Starting Ingestion Pipeline for {len(source_files)} files...\\n\")\n",
    "\n",
    "processed_count = 0\n",
    "passed_count = 0\n",
    "failed_count = 0\n",
    "skipped_count = 0\n",
    "\n",
    "# Use tqdm for progress bar\n",
    "for file_path in tqdm(source_files, desc=\"Processing Files\"):\n",
    "    file_name = file_path.name\n",
    "    \n",
    "    # Check Watermark (Skip if already processed)\n",
    "    if is_processed(file_name):\n",
    "        print(f\"Skipping {file_name} (Already Processed)\")\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "\n",
    "    # Check for Empty File (0 rows) using platform_utils\n",
    "    try:\n",
    "        if platform_utils.file_exists(file_path):\n",
    "            if pq.read_metadata(file_path).num_rows == 0:\n",
    "                print(f\"Skipping {file_name} (Empty File - 0 rows)\")\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "    except Exception:\n",
    "        pass  # Proceed if check fails (let the pipeline handle it)\n",
    "\n",
    "    print(f\"Processing: {file_name}...\")\n",
    "    \n",
    "    try:\n",
    "        # --- PHASE 1: DQ GATEKEEPING (Validation) ---\n",
    "        config_name = file_name.replace('.parquet', '_validation.yml')\n",
    "        config_path = CONFIG_DIR / config_name\n",
    "        \n",
    "        if not platform_utils.file_exists(config_path):\n",
    "            print(f\"   Warning: No validation config found. Skipping DQ check.\")\n",
    "            validation_passed = True\n",
    "            score = 0.0\n",
    "            failures = []\n",
    "            df_batch = None\n",
    "        else:\n",
    "            # Load data and validate\n",
    "            validator = DataQualityValidator(config_path=str(config_path))\n",
    "            df_batch = DataLoader.load_data(str(file_path), sample_size=settings.sample_size or 100000)\n",
    "            result = validator.validate(df_batch)\n",
    "            \n",
    "            validation_passed = result['success']\n",
    "            score = result['success_rate']\n",
    "            failures = result.get('failed_expectations', [])\n",
    "            \n",
    "            # Use centralized DQ threshold\n",
    "            if score < DQ_THRESHOLD:\n",
    "                validation_passed = False\n",
    "            \n",
    "            # Log the result\n",
    "            log_dq_result(\n",
    "                file_name, \n",
    "                \"PASSED\" if validation_passed else \"FAILED\", \n",
    "                score, \n",
    "                {\"failed_count\": len(failures), \"failures\": failures[:5]}\n",
    "            )\n",
    "\n",
    "        # --- PHASE 2: ACTION (Ingest to Silver) ---\n",
    "        if validation_passed:\n",
    "            print(f\"   DQ Passed (Score: {score:.1f}%). Ingesting to Silver...\")\n",
    "            \n",
    "            table_name = file_name.replace('.parquet', '')\n",
    "            \n",
    "            # Load data if not already loaded\n",
    "            if df_batch is None:\n",
    "                df_batch = pd.read_parquet(file_path)\n",
    "            \n",
    "            # Use StorageManager for cross-platform write (works in both local and Fabric)\n",
    "            output_path = storage_manager.write_to_silver(\n",
    "                df=df_batch,\n",
    "                table_name=table_name\n",
    "            )\n",
    "            print(f\"   -> Written to: {output_path}\")\n",
    "            \n",
    "            # Mark as processed\n",
    "            save_watermark(file_name)\n",
    "            print(f\"   Marked as processed.\")\n",
    "            passed_count += 1\n",
    "            \n",
    "        else:\n",
    "            print(f\"   DQ FAILED (Score: {score:.1f}%). Blocked from ingestion.\")\n",
    "            # Quarantine with data if available\n",
    "            quarantine_file(file_path, f\"Failed {len(failures)} checks (Score: {score:.1f}%)\", df_batch)\n",
    "            failed_count += 1\n",
    "            \n",
    "        processed_count += 1\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   Pipeline Error: {e}\")\n",
    "        log_dq_result(file_name, \"ERROR\", 0.0, {\"error\": str(e)})\n",
    "        failed_count += 1\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Pipeline Execution Complete\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total Files: {len(source_files)}\")\n",
    "print(f\"Processed: {processed_count}\")\n",
    "print(f\"Passed DQ: {passed_count}\")\n",
    "print(f\"Failed DQ: {failed_count}\")\n",
    "print(f\"Skipped (Already Processed or Empty): {skipped_count}\")\n",
    "print(f\"\\nView results in Notebook 03 (Monitoring Dashboard)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aims_data_platform",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
