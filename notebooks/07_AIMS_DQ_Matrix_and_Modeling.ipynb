{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c328507a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Running Locally\n",
      "Configuration:\n",
      " Environment: Local\n",
      " Project Root: ..\n",
      " Parquet Dir: ../data/Samples_LH_Bronze_Aims_26_parquet\n",
      " DQ Results Dir: ../data/dq_results\n",
      "aims_data_platform location: /home/sanmi/Documents/HS2/HS2_PROJECTS_2025/1_AIMS_LOCAL_2026/aims_data_platform/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ[\"GX_ANALYTICS_ENABLED\"] = \"False\"\n",
    "import sys\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "import importlib\n",
    "\n",
    "# 1. Detect Environment & Setup Paths\n",
    "try:\n",
    "    from notebookutils import mssparkutils\n",
    "    IS_FABRIC = True\n",
    "    print(\"Running in Microsoft Fabric\")\n",
    "    BASE_DIR = Path(\"/lakehouse/default/Files\")\n",
    "    \n",
    "    # Try to load .env from the Lakehouse Files root if it exists\n",
    "    env_path = BASE_DIR / \".env\"\n",
    "    if env_path.exists():\n",
    "        load_dotenv(dotenv_path=env_path)\n",
    "        print(f\"Loaded configuration from {env_path}\")\n",
    "        \n",
    "    PROJECT_ROOT = BASE_DIR\n",
    "    \n",
    "except ImportError:\n",
    "    IS_FABRIC = False\n",
    "    print(\"Running Locally\")\n",
    "    load_dotenv()\n",
    "    \n",
    "    # Prioritize local development path in sys.path\n",
    "    project_root_local = Path.cwd().parent.resolve()\n",
    "    if str(project_root_local) not in sys.path:\n",
    "        sys.path.insert(0, str(project_root_local))\n",
    "        \n",
    "    PROJECT_ROOT = Path(\"..\")\n",
    "\n",
    "# Define Data Paths\n",
    "PARQUET_DIR = PROJECT_ROOT / \"data/Samples_LH_Bronze_Aims_26_parquet\"\n",
    "DATA_DIR = PARQUET_DIR  # Alias for consistency with other notebooks\n",
    "DQ_RESULTS_DIR = PROJECT_ROOT / \"data/dq_results\"\n",
    "CONFIG_DIR = PROJECT_ROOT / \"dq_great_expectations/generated_configs\"\n",
    "\n",
    "print(f\"Configuration:\\n Environment: {'Fabric' if IS_FABRIC else 'Local'}\")\n",
    "print(f\" Project Root: {PROJECT_ROOT}\")\n",
    "print(f\" Parquet Dir: {PARQUET_DIR}\")\n",
    "print(f\" DQ Results Dir: {DQ_RESULTS_DIR}\")\n",
    "\n",
    "import aims_data_platform\n",
    "importlib.reload(aims_data_platform)\n",
    "print(f\"aims_data_platform location: {aims_data_platform.__file__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3845b42b",
   "metadata": {},
   "source": [
    "## 1. Data Quality Matrix Generation\n",
    "\n",
    "We will parse the YAML configuration files to map technical validation rules to business Data Quality dimensions:\n",
    "- **Completeness:** Null checks (`expect_column_values_to_not_be_null`)\n",
    "- **Uniqueness:** Duplication checks (`expect_column_values_to_be_unique`)\n",
    "- **Validity:** Schema/Type checks (`expect_column_to_exist`, `expect_column_values_to_be_in_set`)\n",
    "- **Consistency:** Row counts and structural integrity (`expect_table_row_count...`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68dd3e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ No DQ rules found. Check if validation config files exist in: ../dq_great_expectations/generated_configs\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "211c3a68-1239-4c3b-96ad-b250415906ee",
       "rows": [],
       "shape": {
        "columns": 0,
        "rows": 0
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def map_expectation_to_dimension(expectation_type):\n",
    "    if \"not_be_null\" in expectation_type:\n",
    "        return \"Completeness\"\n",
    "    elif \"unique\" in expectation_type:\n",
    "        return \"Uniqueness\"\n",
    "    elif \"exist\" in expectation_type or \"in_set\" in expectation_type or \"type\" in expectation_type:\n",
    "        return \"Validity\"\n",
    "    elif \"row_count\" in expectation_type or \"column_count\" in expectation_type:\n",
    "        return \"Consistency\"\n",
    "    else:\n",
    "        return \"Accuracy\" # Default/Other\n",
    "\n",
    "dq_rules = []\n",
    "\n",
    "# Parse all YAML files (use *_validation.yml pattern)\n",
    "for yaml_file in glob.glob(str(CONFIG_DIR / \"*_validation.yml\")):\n",
    "    with open(yaml_file, 'r') as f:\n",
    "        try:\n",
    "            config = yaml.safe_load(f)\n",
    "            table_name = config.get('validation_name', 'Unknown').replace('aims_', '')\n",
    "            \n",
    "            for exp in config.get('expectations', []):\n",
    "                exp_type = exp['expectation_type']\n",
    "                dimension = map_expectation_to_dimension(exp_type)\n",
    "                column = exp.get('kwargs', {}).get('column', 'Table Level')\n",
    "                severity = exp.get('meta', {}).get('severity', 'Low')\n",
    "                \n",
    "                dq_rules.append({\n",
    "                    'Table': table_name,\n",
    "                    'Column': column,\n",
    "                    'Dimension': dimension,\n",
    "                    'Rule': exp_type,\n",
    "                    'Severity': severity,\n",
    "                    'Count': 1\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing {yaml_file}: {e}\")\n",
    "\n",
    "df_dq = pd.DataFrame(dq_rules)\n",
    "\n",
    "if len(df_dq) == 0:\n",
    "    print(\"⚠️ No DQ rules found. Check if validation config files exist in:\", CONFIG_DIR)\n",
    "    dq_matrix = pd.DataFrame()\n",
    "else:\n",
    "    # Create Matrix View (Pivot)\n",
    "    dq_matrix = df_dq.pivot_table(\n",
    "        index='Table', \n",
    "        columns='Dimension', \n",
    "        values='Count', \n",
    "        aggfunc='count', \n",
    "        fill_value=0\n",
    "    )\n",
    "    \n",
    "    # Sort by total rules\n",
    "    dq_matrix['Total Rules'] = dq_matrix.sum(axis=1)\n",
    "    dq_matrix = dq_matrix.sort_values('Total Rules', ascending=False).head(20) # Top 20 tables\n",
    "    \n",
    "    print(\"Top 20 Tables by Rule Coverage:\")\n",
    "display(dq_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06a58153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ No data to visualize. Run data profiling notebooks (01-05) first to generate DQ configs.\n"
     ]
    }
   ],
   "source": [
    "# Visual Heatmap of the Matrix\n",
    "if len(dq_matrix) > 0:\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(dq_matrix.drop('Total Rules', axis=1), annot=True, cmap=\"YlGnBu\", fmt='g')\n",
    "    plt.title(\"Data Quality Coverage Matrix (Rules per Dimension)\")\n",
    "    plt.ylabel(\"Business Entity (Table)\")\n",
    "    plt.xlabel(\"DQ Dimension\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"⚠️ No data to visualize. Run data profiling notebooks (01-05) first to generate DQ configs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bd80fe",
   "metadata": {},
   "source": [
    "### Key Insights from the Data Quality Matrix\n",
    "\n",
    "**What the Matrix Tells Us:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6922c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: DQ Matrix not available. Execute previous cells first.\n"
     ]
    }
   ],
   "source": [
    "# Generate dynamic commentary based on actual data\n",
    "if 'dq_matrix' in locals() and dq_matrix is not None and len(dq_matrix) > 0:\n",
    "    # Calculate Overall DQ KPI\n",
    "    total_checks = dq_matrix['Total Rules'].sum()\n",
    "    avg_coverage = dq_matrix['Total Rules'].mean()\n",
    "    tables_covered = len(dq_matrix)\n",
    "    \n",
    "    # Scoring: Normalize based on breadth and depth\n",
    "    # Breadth: How many tables have coverage (out of expected 30-40 tables)\n",
    "    # Depth: Average rules per table (target: 50+ rules per critical table)\n",
    "    breadth_score = min(100, (tables_covered / 35) * 100)  # Expect ~35 tables\n",
    "    depth_score = min(100, (avg_coverage / 50) * 100)  # Target 50 rules/table\n",
    "    overall_kpi = (breadth_score * 0.4) + (depth_score * 0.6)  # Weight depth higher\n",
    "    \n",
    "    # Find table with highest total rules\n",
    "    top_table = dq_matrix.sort_values('Total Rules', ascending=False).index[0]\n",
    "    top_rules = int(dq_matrix.loc[top_table, 'Total Rules'])\n",
    "    \n",
    "    # Count high-risk tables (>35 rules)\n",
    "    high_risk_count = len(dq_matrix[dq_matrix['Total Rules'] > 35])\n",
    "    \n",
    "    # Get dominant dimension\n",
    "    dimension_totals = dq_matrix.drop('Total Rules', axis=1).sum()\n",
    "    dominant_dim = dimension_totals.idxmax()\n",
    "    dominant_count = int(dimension_totals.max())\n",
    "    \n",
    "    # Calculate completeness percentage\n",
    "    if 'Completeness' in dimension_totals.index:\n",
    "        completeness_pct = (dimension_totals['Completeness'] / dimension_totals.sum()) * 100\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"DATA QUALITY MATRIX ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"\\nOVERALL DATA QUALITY KPI: {overall_kpi:.1f}/100\")\n",
    "    print(f\"  Coverage Breadth: {breadth_score:.1f}/100 ({tables_covered} tables)\")\n",
    "    print(f\"  Validation Depth: {depth_score:.1f}/100 ({avg_coverage:.1f} avg rules/table)\")\n",
    "    print(f\"  Total Validation Rules: {total_checks:,}\")\n",
    "    \n",
    "    print(f\"\\n1. COVERAGE ASSESSMENT\")\n",
    "    print(f\"   Highest validation coverage: '{top_table}' ({top_rules} rules)\")\n",
    "    print(f\"   Critical business entity requiring comprehensive validation\")\n",
    "    \n",
    "    print(f\"\\n2. VALIDATION DISTRIBUTION\")\n",
    "    print(f\"   Dominant dimension: {dominant_dim} ({dominant_count} rules)\")\n",
    "    print(f\"   Average rules per table: {avg_coverage:.1f}\")\n",
    "    if 'Completeness' in dimension_totals.index:\n",
    "        print(f\"   Completeness validation: {completeness_pct:.1f}% of total rules\")\n",
    "        print(f\"   Primary focus: Null/missing data detection\")\n",
    "    \n",
    "    print(f\"\\n3. RISK CLASSIFICATION\")\n",
    "    print(f\"   High-risk tables (35+ rules): {high_risk_count}\")\n",
    "    print(f\"   Priority remediation targets identified\")\n",
    "    \n",
    "    # List top 5 high-risk tables\n",
    "    top_5 = dq_matrix.nlargest(5, 'Total Rules')\n",
    "    print(f\"\\n   Critical Tables Requiring Attention:\")\n",
    "    for idx, (table, row) in enumerate(top_5.iterrows(), 1):\n",
    "        print(f\"      {idx}. {table}: {int(row['Total Rules'])} rules\")\n",
    "    \n",
    "    print(f\"\\n4. OPERATIONAL APPLICATION\")\n",
    "    print(f\"   Prioritized remediation sequence (highest coverage first)\")\n",
    "    print(f\"   Compliance tracking framework (rules passed vs total)\")\n",
    "    print(f\"   Validation gap analysis (tables with low coverage)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "else:\n",
    "    print(\"WARNING: DQ Matrix not available. Execute previous cells first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63269357",
   "metadata": {},
   "source": [
    "## 2. Reporting Data Model (Snowflake Schema)\n",
    "\n",
    "To support the reporting requirements, we need to transform the flat `aims_*` tables into a structured **Star/Snowflake Schema**.\n",
    "\n",
    "### Proposed Architecture\n",
    "1.  **Fact Table:** `FACT_Asset_Inventory` (Central table containing Asset ID, Status, and Foreign Keys)\n",
    "2.  **Dimension Tables:**\n",
    "    -   `DIM_Route` (Route details, Linear referencing info)\n",
    "    -   `DIM_AssetClass` (Hierarchy, Class Names)\n",
    "    -   `DIM_Organisation` (Owner, Maintainer info)\n",
    "    -   `DIM_Location` (Geospatial coordinates)\n",
    "\n",
    "This structure allows BI tools (PowerBI, Tableau) to slice and dice metrics efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95c6f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load validation results to identify passed tables only\n",
    "validation_results_path = DATA_DIR.parent / \"config\" / \"validation_results.json\"\n",
    "passed_tables = []\n",
    "\n",
    "if validation_results_path.exists():\n",
    "    import json\n",
    "    with open(validation_results_path, 'r') as f:\n",
    "        validation_data = json.load(f)\n",
    "    \n",
    "    # Extract tables that passed validation\n",
    "    for table_name, result in validation_data.items():\n",
    "        if isinstance(result, dict) and result.get('success', False):\n",
    "            passed_tables.append(table_name.replace('aims_', '').lower())\n",
    "    \n",
    "    print(f\"Validation results loaded: {len(passed_tables)} tables passed DQ checks\")\n",
    "    print(f\"Passed tables: {', '.join(passed_tables)}\")\n",
    "else:\n",
    "    print(\"WARNING: Validation results not found. Loading all available tables.\")\n",
    "    print(\"Run Notebook 01 (Data Profiling) to generate validation results.\")\n",
    "    # Fallback: Load core tables needed for Silver layer\n",
    "    passed_tables = ['assets', 'assetlocations', 'routes', 'assetclasses', 'organisations']\n",
    "\n",
    "# Load Data for Prototyping (Memory Safe) - Only Passed Tables\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "def load_parquet(name, limit=100000):\n",
    "    \"\"\"\n",
    "    Loads a Parquet table with PyArrow, normalizes columns, and performs basic health check.\n",
    "    Includes safety limit to prevent memory issues on large datasets.\n",
    "    Only loads tables that passed validation.\n",
    "    \"\"\"\n",
    "    # Check if table is in passed list\n",
    "    if name not in passed_tables:\n",
    "        print(f\"SKIPPED: {name} (not in passed validation list)\")\n",
    "        return None\n",
    "    \n",
    "    path = DATA_DIR / f\"aims_{name}.parquet\"\n",
    "    if not path.exists():\n",
    "        print(f\"WARNING: Table not found: {name}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Check file size metadata first\n",
    "        parquet_file = pq.ParquetFile(path)\n",
    "        total_rows = parquet_file.metadata.num_rows\n",
    "        \n",
    "        if total_rows > limit:\n",
    "            print(f\"NOTICE: Large file detected: {name} ({total_rows:,} rows). Loading sample of {limit:,} rows.\")\n",
    "            table = parquet_file.read_row_groups([0]) \n",
    "            if table.num_rows > limit:\n",
    "                 df = table.slice(0, limit).to_pandas()\n",
    "            else:\n",
    "                 df = table.to_pandas()\n",
    "        else:\n",
    "            df = pd.read_parquet(path, engine='pyarrow')\n",
    "\n",
    "        df.columns = [c.upper() for c in df.columns]\n",
    "        print(f\"LOADED: {name} ({len(df):,} rows)\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR loading {name}: {e}\")\n",
    "        return None\n",
    "\n",
    "assets = load_parquet(\"assets\")\n",
    "locations = load_parquet(\"assetlocations\")\n",
    "routes = load_parquet(\"routes\")\n",
    "classes = load_parquet(\"assetclasses\")\n",
    "orgs = load_parquet(\"organisations\")\n",
    "\n",
    "print(\"\\nData loading complete. Only validated tables loaded for Silver layer transformation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1124b8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Star Schema Modeling Logic\n",
    "\n",
    "if assets is not None and locations is not None:\n",
    "    # 1. Create FACT_Asset_Inventory\n",
    "    # Join Assets with Locations to establish primary spatial/linear context\n",
    "    # Note: ASSETCLASS information resides in locations table\n",
    "    # LEFT JOIN ensures all Assets retained even without Location record\n",
    "    \n",
    "    # Identify available columns\n",
    "    available_cols = locations.columns.tolist()\n",
    "    \n",
    "    # Select required columns\n",
    "    location_cols = ['ASSETID', 'ASSET', 'ASSETCLASSID', 'ASSETCLASS', 'ROUTEID', 'ROUTE']\n",
    "    \n",
    "    # Include geospatial columns if present\n",
    "    for col in ['OSGBEASTING', 'OSGBNORTHING', 'STARTOSGBEASTING', 'STARTOSGBNORTHING']:\n",
    "        if col in available_cols:\n",
    "            location_cols.append(col)\n",
    "    \n",
    "    # Include chainage baseline if present\n",
    "    if 'CHAINAGEBASELINE' in available_cols:\n",
    "        location_cols.append('CHAINAGEBASELINE')\n",
    "    \n",
    "    fact_assets = pd.merge(\n",
    "        assets[['ID', 'OWNER', 'PHASEID', 'STATUS', 'NAME']],\n",
    "        locations[location_cols],\n",
    "        left_on='ID', \n",
    "        right_on='ASSETID',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Rename for business clarity\n",
    "    rename_map = {\n",
    "        'ID': 'Asset_Key',\n",
    "        'ASSETCLASSID': 'Class_Key',\n",
    "        'ASSETCLASS': 'Class_Name',\n",
    "        'OWNER': 'Owner_Key',\n",
    "        'ROUTEID': 'Route_Key',\n",
    "        'ROUTE': 'Route_Name',\n",
    "        'STATUS': 'Asset_Status',\n",
    "        'NAME': 'Asset_Name',\n",
    "        'CHAINAGEBASELINE': 'Chainage_Baseline'\n",
    "    }\n",
    "    \n",
    "    fact_assets = fact_assets.rename(columns={k: v for k, v in rename_map.items() if k in fact_assets.columns})\n",
    "    \n",
    "    # Remove duplicate ASSET column\n",
    "    if 'ASSET' in fact_assets.columns:\n",
    "        fact_assets = fact_assets.drop('ASSET', axis=1)\n",
    "    \n",
    "    print(\"--- FACT_Asset_Inventory (Preview) ---\")\n",
    "    print(f\"Shape: {fact_assets.shape}\")\n",
    "    print(f\"Columns: {list(fact_assets.columns)}\")\n",
    "    display(fact_assets.head(10))\n",
    "    \n",
    "    # 2. Create DIM_Route\n",
    "    if routes is not None:\n",
    "        route_cols = ['ID', 'CODE', 'DESCRIPTION']\n",
    "        dim_route = routes[route_cols].rename(columns={\n",
    "            'ID': 'Route_Key',\n",
    "            'CODE': 'Route_Code',\n",
    "            'DESCRIPTION': 'Route_Description'\n",
    "        })\n",
    "        print(\"\\n--- DIM_Route (Preview) ---\")\n",
    "        print(f\"Shape: {dim_route.shape}\")\n",
    "        display(dim_route.head(10))\n",
    "\n",
    "    # 3. Create DIM_AssetClass\n",
    "    if classes is not None:\n",
    "        class_cols = ['ID', 'CODE', 'NAME', 'PARENTID', 'DESCRIPTION']\n",
    "        dim_class = classes[class_cols].rename(columns={\n",
    "            'ID': 'Class_Key',\n",
    "            'CODE': 'Class_Code',\n",
    "            'NAME': 'Class_Name',\n",
    "            'PARENTID': 'Parent_Class_Key',\n",
    "            'DESCRIPTION': 'Class_Description'\n",
    "        })\n",
    "        print(\"\\n--- DIM_AssetClass (Preview) ---\")\n",
    "        print(f\"Shape: {dim_class.shape}\")\n",
    "        display(dim_class.head(10))\n",
    "        \n",
    "    # 4. Create DIM_Organisation\n",
    "    if orgs is not None:\n",
    "        # Identify available columns\n",
    "        org_cols = ['ID', 'NAME']\n",
    "        if 'TYPE' in orgs.columns:\n",
    "            org_cols.append('TYPE')\n",
    "        if 'PARENTID' in orgs.columns:\n",
    "            org_cols.append('PARENTID')\n",
    "            \n",
    "        dim_org = orgs[org_cols].rename(columns={\n",
    "            'ID': 'Owner_Key',\n",
    "            'NAME': 'Organisation_Name',\n",
    "            'TYPE': 'Org_Type',\n",
    "            'PARENTID': 'Parent_Organisation_Key'\n",
    "        })\n",
    "        print(\"\\n--- DIM_Organisation (Preview) ---\")\n",
    "        print(f\"Shape: {dim_org.shape}\")\n",
    "        display(dim_org.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d398fff",
   "metadata": {},
   "source": [
    "## 3. Implementation: Creating Silver Layer Tables\n",
    "\n",
    "Below, we'll implement the actual transformation logic to persist these models as **Delta/Parquet tables** in the Silver Layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac3a42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist Silver Layer Tables to Parquet\n",
    "\n",
    "SILVER_DIR = PROJECT_ROOT / \"data\" / \"silver_layer\"\n",
    "SILVER_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Save Fact Table\n",
    "if 'fact_assets' in locals() and fact_assets is not None:\n",
    "    fact_output = SILVER_DIR / \"FACT_Asset_Inventory.parquet\"\n",
    "    fact_assets.to_parquet(fact_output, index=False, engine='pyarrow')\n",
    "    print(f\"Persisted FACT_Asset_Inventory: {fact_output} ({len(fact_assets):,} rows)\")\n",
    "\n",
    "# Save Dimension Tables\n",
    "dim_tables = {\n",
    "    'DIM_Route': 'dim_route',\n",
    "    'DIM_AssetClass': 'dim_class',\n",
    "    'DIM_Organisation': 'dim_org'\n",
    "}\n",
    "\n",
    "for table_name, var_name in dim_tables.items():\n",
    "    if var_name in locals() and locals()[var_name] is not None:\n",
    "        dim_df = locals()[var_name]\n",
    "        dim_output = SILVER_DIR / f\"{table_name}.parquet\"\n",
    "        dim_df.to_parquet(dim_output, index=False, engine='pyarrow')\n",
    "        print(f\"Persisted {table_name}: {dim_output} ({len(dim_df):,} rows)\")\n",
    "\n",
    "print(f\"\\nSilver layer tables persisted to: {SILVER_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6373d6",
   "metadata": {},
   "source": [
    "### 3.4: Expand Dimensions - DIM_Date & DIM_Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bf7b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Additional Dimension Tables\n",
    "\n",
    "# DIM_Date: Extract all unique dates from asset-related columns\n",
    "if 'assets' in locals() and assets is not None:\n",
    "    date_cols = [col for col in assets.columns if 'DATE' in col.upper() or 'TIME' in col.upper()]\n",
    "    \n",
    "    if date_cols:\n",
    "        all_dates = pd.Series(dtype='datetime64[ns]')\n",
    "        for col in date_cols:\n",
    "            try:\n",
    "                dates = pd.to_datetime(assets[col], errors='coerce')\n",
    "                all_dates = pd.concat([all_dates, dates])\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Create dimension\n",
    "        unique_dates = all_dates.dropna().unique()\n",
    "        dim_date = pd.DataFrame({'Date': unique_dates})\n",
    "        dim_date['Date'] = pd.to_datetime(dim_date['Date'])\n",
    "        dim_date = dim_date.sort_values('Date').reset_index(drop=True)\n",
    "        \n",
    "        # Add derived attributes\n",
    "        dim_date['Date_Key'] = dim_date['Date'].dt.strftime('%Y%m%d').astype(int)\n",
    "        dim_date['Year'] = dim_date['Date'].dt.year\n",
    "        dim_date['Quarter'] = dim_date['Date'].dt.quarter\n",
    "        dim_date['Month'] = dim_date['Date'].dt.month\n",
    "        dim_date['Month_Name'] = dim_date['Date'].dt.month_name()\n",
    "        dim_date['Week'] = dim_date['Date'].dt.isocalendar().week\n",
    "        dim_date['Day'] = dim_date['Date'].dt.day\n",
    "        dim_date['Day_Name'] = dim_date['Date'].dt.day_name()\n",
    "        dim_date['Is_Weekend'] = dim_date['Date'].dt.dayofweek.isin([5, 6])\n",
    "        \n",
    "        # Persist\n",
    "        date_output = SILVER_DIR / \"DIM_Date.parquet\"\n",
    "        dim_date.to_parquet(date_output, index=False, engine='pyarrow')\n",
    "        print(f\"Persisted DIM_Date: {date_output} ({len(dim_date):,} rows)\")\n",
    "        print(f\"   Date Range: {dim_date['Date'].min()} to {dim_date['Date'].max()}\\n\")\n",
    "        display(dim_date.head(10))\n",
    "\n",
    "# DIM_Status: Extract unique statuses from fact table\n",
    "if 'fact_assets' in locals() and fact_assets is not None and 'Asset_Status' in fact_assets.columns:\n",
    "    statuses = fact_assets['Asset_Status'].dropna().unique()\n",
    "    \n",
    "    status_descriptions = {\n",
    "        'Published': 'Asset record published',\n",
    "        'Client-shared': 'Asset information shared with client',\n",
    "        'Archived': 'Asset record archived',\n",
    "        'Work in progress': 'Asset record under development'\n",
    "    }\n",
    "    \n",
    "    dim_status = pd.DataFrame({\n",
    "        'Status_Key': statuses,\n",
    "        'Status_Name': statuses,\n",
    "        'Status_Description': [status_descriptions.get(s, 'Status definition pending') for s in statuses]\n",
    "    })\n",
    "    \n",
    "    # Persist\n",
    "    status_output = SILVER_DIR / \"DIM_Status.parquet\"\n",
    "    dim_status.to_parquet(status_output, index=False, engine='pyarrow')\n",
    "    print(f\"\\nPersisted DIM_Status: {status_output} ({len(dim_status):,} rows)\\n\")\n",
    "    display(dim_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0fdeeb",
   "metadata": {},
   "source": [
    "### 3.5: Summary & Next Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73371f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dynamic summary based on created tables\n",
    "print(\"=\" * 70)\n",
    "print(\"SILVER LAYER TRANSFORMATION SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "created_tables = []\n",
    "total_rows = 0\n",
    "\n",
    "# Check Fact Table\n",
    "if 'fact_assets' in locals() and fact_assets is not None:\n",
    "    fact_rows = len(fact_assets)\n",
    "    created_tables.append(f\"FACT_Asset_Inventory ({fact_rows:,} rows)\")\n",
    "    total_rows += fact_rows\n",
    "\n",
    "# Check Dimension Tables\n",
    "dim_tables = {\n",
    "    'DIM_Route': 'dim_route',\n",
    "    'DIM_AssetClass': 'dim_class',\n",
    "    'DIM_Organisation': 'dim_org',\n",
    "    'DIM_Date': 'dim_date',\n",
    "    'DIM_Status': 'dim_status'\n",
    "}\n",
    "\n",
    "for table_name, var_name in dim_tables.items():\n",
    "    if var_name in locals() and locals()[var_name] is not None:\n",
    "        rows = len(locals()[var_name])\n",
    "        created_tables.append(f\"{table_name} ({rows:,} rows)\")\n",
    "        total_rows += rows\n",
    "\n",
    "print(f\"\\nTABLES CREATED: {len(created_tables)}\")\n",
    "for i, table in enumerate(created_tables, 1):\n",
    "    print(f\"   {i}. {table}\")\n",
    "\n",
    "print(f\"\\nTOTAL DATA VOLUME: {total_rows:,} rows across all tables\")\n",
    "\n",
    "if 'SILVER_DIR' in locals():\n",
    "    print(f\"\\nOUTPUT LOCATION: {SILVER_DIR}\")\n",
    "    \n",
    "    # List actual files created\n",
    "    import os\n",
    "    if os.path.exists(SILVER_DIR):\n",
    "        files = [f for f in os.listdir(SILVER_DIR) if f.endswith('.parquet')]\n",
    "        if files:\n",
    "            print(f\"   Files on disk: {len(files)}\")\n",
    "            for f in sorted(files):\n",
    "                file_path = SILVER_DIR / f\n",
    "                size_bytes = os.path.getsize(file_path)\n",
    "                size_mb = size_bytes / (1024 * 1024)\n",
    "                \n",
    "                # Display KB for small files, MB for larger files\n",
    "                if size_mb < 0.01:\n",
    "                    size_kb = size_bytes / 1024\n",
    "                    print(f\"   - {f} ({size_kb:.1f} KB)\")\n",
    "                else:\n",
    "                    print(f\"   - {f} ({size_mb:.2f} MB)\")\n",
    "            \n",
    "            print(\"\\n   NOTE: Small dimension table sizes (KB range) are expected.\")\n",
    "            print(\"   Dimension tables contain reference data (routes, statuses, etc.)\")\n",
    "            print(\"   Fact tables contain transactional data (larger MB range)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"NEXT ACTIONS FOR REPORTING TEAM\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\"\"\n",
    "1. POWERBI/TABLEAU CONNECTION\n",
    "   Connect data source to Silver Layer directory specified above\n",
    "\n",
    "2. CONFIGURE RELATIONSHIPS\n",
    "   - FACT_Asset_Inventory.Route_Key -> DIM_Route.Route_Key\n",
    "   - FACT_Asset_Inventory.Class_Key -> DIM_AssetClass.Class_Key\n",
    "   - FACT_Asset_Inventory.Owner_Key -> DIM_Organisation.Owner_Key\"\"\")\n",
    "\n",
    "if 'dim_status' in locals() and locals()['dim_status'] is not None:\n",
    "    print(\"   - FACT_Asset_Inventory.Asset_Status -> DIM_Status.Status_Key\")\n",
    "\n",
    "if 'dim_date' in locals() and locals()['dim_date'] is not None:\n",
    "    print(\"   - Join date fields using DIM_Date for temporal analysis\")\n",
    "\n",
    "print(\"\"\"\n",
    "3. RECOMMENDED REPORTS\n",
    "   - Asset Inventory by Route and Asset Class\n",
    "   - Status Distribution Over Time\n",
    "   - Organisational Asset Ownership Analysis\"\"\")\n",
    "\n",
    "if 'fact_assets' in locals() and 'OSGBEASTING' in fact_assets.columns:\n",
    "    print(\"   - Geospatial Asset Map (OSGB coordinates)\")\n",
    "\n",
    "print(\"\"\"\n",
    "4. AUTOMATION OPTIONS\n",
    "   - Schedule notebook execution in Microsoft Fabric (daily refresh)\n",
    "   - Integrate into Airflow/cron pipeline for automated processing\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aims_data_platform",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
