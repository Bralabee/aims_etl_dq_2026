{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c328507a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Locally\n",
      "Configuration:\n",
      " Environment: Local\n",
      " Project Root: ..\n",
      " Parquet Dir: ../data/Samples_LH_Bronze_Aims_26_parquet\n",
      " DQ Results Dir: ../data/dq_results\n",
      "aims_data_platform location: /home/sanmi/Documents/HS2/HS2_PROJECTS_2025/1_AIMS_LOCAL_2026/aims_data_platform/__init__.py\n",
      "aims_data_platform location: /home/sanmi/Documents/HS2/HS2_PROJECTS_2025/1_AIMS_LOCAL_2026/aims_data_platform/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ[\"GX_ANALYTICS_ENABLED\"] = \"False\"\n",
    "import sys\n",
    "import time\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "import importlib\n",
    "\n",
    "print(\"Starting configuration...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# 1. Detect Environment\n",
    "print(\"Detecting environment...\")\n",
    "IS_FABRIC = False\n",
    "\n",
    "# Fast check: Look for Fabric-specific paths to avoid slow imports\n",
    "if Path(\"/lakehouse/default/Files\").exists():\n",
    "    try:\n",
    "        from notebookutils import mssparkutils\n",
    "        IS_FABRIC = True\n",
    "        print(\"Running in Microsoft Fabric\")\n",
    "    except ImportError:\n",
    "        print(\"Running in Fabric environment (path detected) but mssparkutils missing\")\n",
    "        IS_FABRIC = True\n",
    "else:\n",
    "    print(\"Running Locally (Fabric path not found)\")\n",
    "\n",
    "print(f\"Environment detection took {time.time() - start_time:.2f}s\")\n",
    "\n",
    "# 2. Define Paths based on Environment\n",
    "if IS_FABRIC:\n",
    "    BASE_DIR = Path(\"/lakehouse/default/Files\")\n",
    "    \n",
    "    # Try to load .env from the Lakehouse Files root if it exists\n",
    "    env_path = BASE_DIR / \".env\"\n",
    "    if env_path.exists():\n",
    "        load_dotenv(dotenv_path=env_path)\n",
    "        print(f\"Loaded configuration from {env_path}\")\n",
    "        \n",
    "    PROJECT_ROOT = BASE_DIR\n",
    "    \n",
    "else:\n",
    "    load_dotenv()\n",
    "    \n",
    "    # Prioritize local development path in sys.path\n",
    "    project_root_local = Path.cwd().parent.resolve()\n",
    "    if str(project_root_local) not in sys.path:\n",
    "        sys.path.insert(0, str(project_root_local))\n",
    "        \n",
    "    PROJECT_ROOT = Path(\"..\")\n",
    "\n",
    "# Define Data Paths\n",
    "PARQUET_DIR = PROJECT_ROOT / \"data/Samples_LH_Bronze_Aims_26_parquet\"\n",
    "DATA_DIR = PARQUET_DIR  # Alias for consistency with other notebooks\n",
    "DQ_RESULTS_DIR = PROJECT_ROOT / \"config/validation_results\"\n",
    "CONFIG_DIR = PROJECT_ROOT / \"config/data_quality\"\n",
    "\n",
    "print(f\"Configuration:\\n Environment: {'Fabric' if IS_FABRIC else 'Local'}\")\n",
    "print(f\" Project Root: {PROJECT_ROOT}\")\n",
    "print(f\" Parquet Dir: {PARQUET_DIR}\")\n",
    "print(f\" DQ Results Dir: {DQ_RESULTS_DIR}\")\n",
    "print(f\" Config Dir: {CONFIG_DIR}\")\n",
    "\n",
    "import aims_data_platform\n",
    "importlib.reload(aims_data_platform)\n",
    "print(f\"aims_data_platform location: {aims_data_platform.__file__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3845b42b",
   "metadata": {},
   "source": [
    "## 1. Data Quality Matrix Generation\n",
    "\n",
    "We will parse the YAML configuration files to map technical validation rules to business Data Quality dimensions:\n",
    "- **Completeness:** Null checks (`expect_column_values_to_not_be_null`)\n",
    "- **Uniqueness:** Duplication checks (`expect_column_values_to_be_unique`)\n",
    "- **Validity:** Schema/Type checks (`expect_column_to_exist`, `expect_column_values_to_be_in_set`)\n",
    "- **Consistency:** Row counts and structural integrity (`expect_table_row_count...`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68dd3e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ No DQ rules found. Check if validation config files exist in: ../dq_great_expectations/generated_configs\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "2a38ba4c-7895-4ad6-a242-d4b9ab910641",
       "rows": [],
       "shape": {
        "columns": 0,
        "rows": 0
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def map_expectation_to_dimension(expectation_type):\n",
    "    if \"not_be_null\" in expectation_type:\n",
    "        return \"Completeness\"\n",
    "    elif \"unique\" in expectation_type:\n",
    "        return \"Uniqueness\"\n",
    "    elif \"exist\" in expectation_type or \"in_set\" in expectation_type or \"type\" in expectation_type:\n",
    "        return \"Validity\"\n",
    "    elif \"row_count\" in expectation_type or \"column_count\" in expectation_type:\n",
    "        return \"Consistency\"\n",
    "    else:\n",
    "        return \"Accuracy\" # Default/Other\n",
    "\n",
    "dq_rules = []\n",
    "\n",
    "# Parse all YAML files (use *_validation.yml pattern)\n",
    "for yaml_file in glob.glob(str(CONFIG_DIR / \"*_validation.yml\")):\n",
    "    with open(yaml_file, 'r') as f:\n",
    "        try:\n",
    "            config = yaml.safe_load(f)\n",
    "            table_name = config.get('validation_name', 'Unknown').replace('aims_', '')\n",
    "            \n",
    "            for exp in config.get('expectations', []):\n",
    "                exp_type = exp['expectation_type']\n",
    "                dimension = map_expectation_to_dimension(exp_type)\n",
    "                column = exp.get('kwargs', {}).get('column', 'Table Level')\n",
    "                severity = exp.get('meta', {}).get('severity', 'Low')\n",
    "                \n",
    "                dq_rules.append({\n",
    "                    'Table': table_name,\n",
    "                    'Column': column,\n",
    "                    'Dimension': dimension,\n",
    "                    'Rule': exp_type,\n",
    "                    'Severity': severity,\n",
    "                    'Count': 1\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing {yaml_file}: {e}\")\n",
    "\n",
    "df_dq = pd.DataFrame(dq_rules)\n",
    "\n",
    "if len(df_dq) == 0:\n",
    "    print(\"⚠️ No DQ rules found. Check if validation config files exist in:\", CONFIG_DIR)\n",
    "    dq_matrix = pd.DataFrame()\n",
    "else:\n",
    "    # Create Matrix View (Pivot)\n",
    "    dq_matrix = df_dq.pivot_table(\n",
    "        index='Table', \n",
    "        columns='Dimension', \n",
    "        values='Count', \n",
    "        aggfunc='count', \n",
    "        fill_value=0\n",
    "    )\n",
    "    \n",
    "    # Sort by total rules\n",
    "    dq_matrix['Total Rules'] = dq_matrix.sum(axis=1)\n",
    "    dq_matrix = dq_matrix.sort_values('Total Rules', ascending=False).head(20) # Top 20 tables\n",
    "    \n",
    "    print(\"Top 20 Tables by Rule Coverage:\")\n",
    "display(dq_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06a58153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ No data to visualize. Run data profiling notebooks (01-05) first to generate DQ configs.\n"
     ]
    }
   ],
   "source": [
    "# Visual Heatmap of the Matrix\n",
    "if len(dq_matrix) > 0:\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(dq_matrix.drop('Total Rules', axis=1), annot=True, cmap=\"YlGnBu\", fmt='g')\n",
    "    plt.title(\"Data Quality Coverage Matrix (Rules per Dimension)\")\n",
    "    plt.ylabel(\"Business Entity (Table)\")\n",
    "    plt.xlabel(\"DQ Dimension\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"⚠️ No data to visualize. Run data profiling notebooks (01-05) first to generate DQ configs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bd80fe",
   "metadata": {},
   "source": [
    "### Key Insights from the Data Quality Matrix\n",
    "\n",
    "**What the Matrix Tells Us:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6922c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: DQ Matrix not available. Execute previous cells first.\n"
     ]
    }
   ],
   "source": [
    "# Generate dynamic commentary based on actual data\n",
    "if 'dq_matrix' in locals() and dq_matrix is not None and len(dq_matrix) > 0:\n",
    "    # Calculate Overall DQ KPI\n",
    "    total_checks = dq_matrix['Total Rules'].sum()\n",
    "    avg_coverage = dq_matrix['Total Rules'].mean()\n",
    "    tables_covered = len(dq_matrix)\n",
    "    \n",
    "    # Scoring: Normalize based on breadth and depth\n",
    "    # Breadth: How many tables have coverage (out of expected 30-40 tables)\n",
    "    # Depth: Average rules per table (target: 50+ rules per critical table)\n",
    "    breadth_score = min(100, (tables_covered / 35) * 100)  # Expect ~35 tables\n",
    "    depth_score = min(100, (avg_coverage / 50) * 100)  # Target 50 rules/table\n",
    "    overall_kpi = (breadth_score * 0.4) + (depth_score * 0.6)  # Weight depth higher\n",
    "    \n",
    "    # Find table with highest total rules\n",
    "    top_table = dq_matrix.sort_values('Total Rules', ascending=False).index[0]\n",
    "    top_rules = int(dq_matrix.loc[top_table, 'Total Rules'])\n",
    "    \n",
    "    # Count high-risk tables (>35 rules)\n",
    "    high_risk_count = len(dq_matrix[dq_matrix['Total Rules'] > 35])\n",
    "    \n",
    "    # Get dominant dimension\n",
    "    dimension_totals = dq_matrix.drop('Total Rules', axis=1).sum()\n",
    "    dominant_dim = dimension_totals.idxmax()\n",
    "    dominant_count = int(dimension_totals.max())\n",
    "    \n",
    "    # Calculate completeness percentage\n",
    "    if 'Completeness' in dimension_totals.index:\n",
    "        completeness_pct = (dimension_totals['Completeness'] / dimension_totals.sum()) * 100\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"DATA QUALITY MATRIX ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"\\nOVERALL DATA QUALITY KPI: {overall_kpi:.1f}/100\")\n",
    "    print(f\"  Coverage Breadth: {breadth_score:.1f}/100 ({tables_covered} tables)\")\n",
    "    print(f\"  Validation Depth: {depth_score:.1f}/100 ({avg_coverage:.1f} avg rules/table)\")\n",
    "    print(f\"  Total Validation Rules: {total_checks:,}\")\n",
    "    \n",
    "    print(f\"\\n1. COVERAGE ASSESSMENT\")\n",
    "    print(f\"   Highest validation coverage: '{top_table}' ({top_rules} rules)\")\n",
    "    print(f\"   Critical business entity requiring comprehensive validation\")\n",
    "    \n",
    "    print(f\"\\n2. VALIDATION DISTRIBUTION\")\n",
    "    print(f\"   Dominant dimension: {dominant_dim} ({dominant_count} rules)\")\n",
    "    print(f\"   Average rules per table: {avg_coverage:.1f}\")\n",
    "    if 'Completeness' in dimension_totals.index:\n",
    "        print(f\"   Completeness validation: {completeness_pct:.1f}% of total rules\")\n",
    "        print(f\"   Primary focus: Null/missing data detection\")\n",
    "    \n",
    "    print(f\"\\n3. RISK CLASSIFICATION\")\n",
    "    print(f\"   High-risk tables (35+ rules): {high_risk_count}\")\n",
    "    print(f\"   Priority remediation targets identified\")\n",
    "    \n",
    "    # List top 5 high-risk tables\n",
    "    top_5 = dq_matrix.nlargest(5, 'Total Rules')\n",
    "    print(f\"\\n   Critical Tables Requiring Attention:\")\n",
    "    for idx, (table, row) in enumerate(top_5.iterrows(), 1):\n",
    "        print(f\"      {idx}. {table}: {int(row['Total Rules'])} rules\")\n",
    "    \n",
    "    print(f\"\\n4. OPERATIONAL APPLICATION\")\n",
    "    print(f\"   Prioritized remediation sequence (highest coverage first)\")\n",
    "    print(f\"   Compliance tracking framework (rules passed vs total)\")\n",
    "    print(f\"   Validation gap analysis (tables with low coverage)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "else:\n",
    "    print(\"WARNING: DQ Matrix not available. Execute previous cells first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63269357",
   "metadata": {},
   "source": [
    "## 2. Reporting Data Model (Snowflake Schema)\n",
    "\n",
    "To support the reporting requirements, we need to transform the flat `aims_*` tables into a structured **Star/Snowflake Schema**.\n",
    "\n",
    "### Proposed Architecture\n",
    "1.  **Fact Table:** `FACT_Asset_Inventory` (Central table containing Asset ID, Status, and Foreign Keys)\n",
    "2.  **Dimension Tables:**\n",
    "    -   `DIM_Route` (Route details, Linear referencing info)\n",
    "    -   `DIM_AssetClass` (Hierarchy, Class Names)\n",
    "    -   `DIM_Organisation` (Owner, Maintainer info)\n",
    "    -   `DIM_Location` (Geospatial coordinates)\n",
    "\n",
    "This structure allows BI tools (PowerBI, Tableau) to slice and dice metrics efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95c6f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Validation results not found. Loading all available tables.\n",
      "Run Notebook 01 (Data Profiling) to generate validation results.\n",
      "NOTICE: Large file detected: assets (2,217,599 rows). Loading sample of 100,000 rows.\n",
      "LOADED: assets (100,000 rows)\n",
      "NOTICE: Large file detected: assetlocations (2,235,262 rows). Loading sample of 100,000 rows.\n",
      "NOTICE: Large file detected: assets (2,217,599 rows). Loading sample of 100,000 rows.\n",
      "LOADED: assets (100,000 rows)\n",
      "NOTICE: Large file detected: assetlocations (2,235,262 rows). Loading sample of 100,000 rows.\n",
      "LOADED: assetlocations (100,000 rows)\n",
      "LOADED: routes (33 rows)\n",
      "LOADED: assetclasses (5,644 rows)\n",
      "LOADED: organisations (28 rows)\n",
      "\n",
      "Data loading complete. Only validated tables loaded for Silver layer transformation.\n",
      "LOADED: assetlocations (100,000 rows)\n",
      "LOADED: routes (33 rows)\n",
      "LOADED: assetclasses (5,644 rows)\n",
      "LOADED: organisations (28 rows)\n",
      "\n",
      "Data loading complete. Only validated tables loaded for Silver layer transformation.\n"
     ]
    }
   ],
   "source": [
    "# Load validation results to identify passed tables only\n",
    "validation_results_path = DQ_RESULTS_DIR / \"validation_results.json\"\n",
    "passed_tables = []\n",
    "\n",
    "if validation_results_path.exists():\n",
    "    import json\n",
    "    with open(validation_results_path, 'r') as f:\n",
    "        validation_data = json.load(f)\n",
    "    \n",
    "    # Extract tables that passed validation\n",
    "    for table_name, result in validation_data.items():\n",
    "        # Handle both old and new schema if necessary, but primarily new schema\n",
    "        # New schema: validation_data['files'][table_name]['overall_success']\n",
    "        # The code below assumes validation_data is a dict of results, which matches the 'files' key in the new schema\n",
    "        # But validation_results.json structure is:\n",
    "        # { \"timestamp\": ..., \"files\": { \"table1\": { \"overall_success\": ... } }, \"summary\": ... }\n",
    "        \n",
    "        # We need to check if we are iterating over the root or the 'files' key\n",
    "        pass\n",
    "    \n",
    "    # Correctly access the files dictionary\n",
    "    files_data = validation_data.get(\"files\", {})\n",
    "    if not files_data and \"files\" not in validation_data:\n",
    "        # Fallback for older structure if it exists (unlikely given recent fixes)\n",
    "        files_data = validation_data\n",
    "        \n",
    "    for table_name, result in files_data.items():\n",
    "        if isinstance(result, dict) and result.get('overall_success', False):\n",
    "            passed_tables.append(table_name.replace('aims_', '').lower())\n",
    "    \n",
    "    print(f\"Validation results loaded: {len(passed_tables)} tables passed DQ checks\")\n",
    "    print(f\"Passed tables: {', '.join(passed_tables)}\")\n",
    "else:\n",
    "    print(f\"WARNING: Validation results not found at {validation_results_path}. Loading all available tables.\")\n",
    "    print(\"Run Notebook 01 (Data Profiling) or 00 (Orchestration) to generate validation results.\")\n",
    "    # Fallback: Load core tables needed for Silver layer\n",
    "    passed_tables = ['assets', 'assetlocations', 'routes', 'assetclasses', 'organisations']\n",
    "\n",
    "# Load Data for Prototyping (Memory Safe) - Only Passed Tables\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "def load_parquet(name, limit=100000):\n",
    "    \"\"\"\n",
    "    Loads a Parquet table with PyArrow, normalizes columns, and performs basic health check.\n",
    "    Includes safety limit to prevent memory issues on large datasets.\n",
    "    Only loads tables that passed validation.\n",
    "    \"\"\"\n",
    "    # Check if table is in passed list\n",
    "    if name not in passed_tables:\n",
    "        print(f\"SKIPPED: {name} (not in passed validation list)\")\n",
    "        return None\n",
    "    \n",
    "    path = DATA_DIR / f\"aims_{name}.parquet\"\n",
    "    if not path.exists():\n",
    "        print(f\"WARNING: Table not found: {name}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Check file size metadata first\n",
    "        parquet_file = pq.ParquetFile(path)\n",
    "        total_rows = parquet_file.metadata.num_rows\n",
    "        \n",
    "        if total_rows > limit:\n",
    "            print(f\"NOTICE: Large file detected: {name} ({total_rows:,} rows). Loading sample of {limit:,} rows.\")\n",
    "            table = parquet_file.read_row_groups([0]) \n",
    "            if table.num_rows > limit:\n",
    "                 df = table.slice(0, limit).to_pandas()\n",
    "            else:\n",
    "                 df = table.to_pandas()\n",
    "        else:\n",
    "            df = pd.read_parquet(path, engine='pyarrow')\n",
    "\n",
    "        df.columns = [c.upper() for c in df.columns]\n",
    "        print(f\"LOADED: {name} ({len(df):,} rows)\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR loading {name}: {e}\")\n",
    "        return None\n",
    "\n",
    "assets = load_parquet(\"assets\")\n",
    "locations = load_parquet(\"assetlocations\")\n",
    "routes = load_parquet(\"routes\")\n",
    "classes = load_parquet(\"assetclasses\")\n",
    "orgs = load_parquet(\"organisations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1124b8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- FACT_Asset_Inventory (Preview) ---\n",
      "Shape: (100057, 15)\n",
      "Columns: ['Asset_Key', 'Owner_Key', 'PHASEID', 'Asset_Status', 'Asset_Name', 'ASSETID', 'Class_Key', 'Class_Name', 'Route_Key', 'Route_Name', 'OSGBEASTING', 'OSGBNORTHING', 'STARTOSGBEASTING', 'STARTOSGBNORTHING', 'Chainage_Baseline']\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Asset_Key",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Owner_Key",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "PHASEID",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Asset_Status",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Asset_Name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "ASSETID",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Class_Key",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Class_Name",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Route_Key",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Route_Name",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "OSGBEASTING",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "OSGBNORTHING",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "STARTOSGBEASTING",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "STARTOSGBNORTHING",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Chainage_Baseline",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "6a0f26c9-86b6-4be3-b488-f0b5c20dc862",
       "rows": [
        [
         "0",
         "13254212",
         "HS2",
         "13000015",
         "Published",
         "MS187-CT035",
         "13254212",
         "13002124",
         "CT - Cone penetration testing (EI-70-25-05)",
         "13000187",
         "H2ML - Main Line  (London Euston – Wigan WCML Connection)",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "Hybrid Bill"
        ],
        [
         "1",
         "13254213",
         "HS2",
         "13000015",
         "Published",
         "ML094-WS037",
         "13254213",
         "13002289",
         "WS - Windowless sampling (EI-70-05-20)",
         "13000187",
         "H2ML - Main Line  (London Euston – Wigan WCML Connection)",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "Hybrid Bill"
        ],
        [
         "2",
         "13254214",
         "HS2",
         "13000015",
         "Published",
         "ML000-RM169",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "3",
         "13254215",
         "HS2",
         "13000015",
         "Published",
         "ML152-TP006",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "4",
         "13254209",
         "HS2",
         "13000015",
         "Published",
         "ML183-TP007",
         "13254209",
         "13002298",
         "TP - Trial pit (mechanically dug) (EI-70-10-05)",
         "13000187",
         "H2ML - Main Line  (London Euston – Wigan WCML Connection)",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "Hybrid Bill"
        ],
        [
         "5",
         "13254216",
         "Phase 2a",
         "13000016",
         "Client-shared",
         "Hamley (North) Drop Inlet Culvert",
         "13254216",
         "13002259",
         "Culvert (CV-BR-CV)",
         "13000187",
         "H2ML - Main Line  (London Euston – Wigan WCML Connection)",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "ERD 1 Snakegrid"
        ],
        [
         "6",
         "13254217",
         "HS2",
         "13000015",
         "Published",
         "ML049-WS009",
         "13254217",
         "13002289",
         "WS - Windowless sampling (EI-70-05-20)",
         "13000187",
         "H2ML - Main Line  (London Euston – Wigan WCML Connection)",
         "489069.5",
         "204068.39",
         "0.0",
         "0.0",
         "Hybrid Bill"
        ],
        [
         "7",
         "13254218",
         "HS2",
         "13000015",
         "Published",
         "ML159-SW001",
         "13254218",
         "13002049",
         "SW - Surface water sampling point (EI-70-20-30)",
         "13000187",
         "H2ML - Main Line  (London Euston – Wigan WCML Connection)",
         "419533.21",
         "286076.5",
         "0.0",
         "0.0",
         "Hybrid Bill"
        ],
        [
         "8",
         "13254219",
         "EK Joint Venture",
         "13000015",
         "Archived",
         "Doddershall No. 2 Culvert",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "9",
         "13254220",
         "HS2",
         "13000015",
         "Published",
         "ML171-TP020",
         "13254220",
         "13002298",
         "TP - Trial pit (mechanically dug) (EI-70-10-05)",
         "13000187",
         "H2ML - Main Line  (London Euston – Wigan WCML Connection)",
         "418466.79",
         "298358.82",
         "0.0",
         "0.0",
         "Hybrid Bill"
        ]
       ],
       "shape": {
        "columns": 15,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Asset_Key</th>\n",
       "      <th>Owner_Key</th>\n",
       "      <th>PHASEID</th>\n",
       "      <th>Asset_Status</th>\n",
       "      <th>Asset_Name</th>\n",
       "      <th>ASSETID</th>\n",
       "      <th>Class_Key</th>\n",
       "      <th>Class_Name</th>\n",
       "      <th>Route_Key</th>\n",
       "      <th>Route_Name</th>\n",
       "      <th>OSGBEASTING</th>\n",
       "      <th>OSGBNORTHING</th>\n",
       "      <th>STARTOSGBEASTING</th>\n",
       "      <th>STARTOSGBNORTHING</th>\n",
       "      <th>Chainage_Baseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13254212</td>\n",
       "      <td>HS2</td>\n",
       "      <td>13000015</td>\n",
       "      <td>Published</td>\n",
       "      <td>MS187-CT035</td>\n",
       "      <td>13254212</td>\n",
       "      <td>13002124</td>\n",
       "      <td>CT - Cone penetration testing (EI-70-25-05)</td>\n",
       "      <td>13000187</td>\n",
       "      <td>H2ML - Main Line  (London Euston – Wigan WCML ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Hybrid Bill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13254213</td>\n",
       "      <td>HS2</td>\n",
       "      <td>13000015</td>\n",
       "      <td>Published</td>\n",
       "      <td>ML094-WS037</td>\n",
       "      <td>13254213</td>\n",
       "      <td>13002289</td>\n",
       "      <td>WS - Windowless sampling (EI-70-05-20)</td>\n",
       "      <td>13000187</td>\n",
       "      <td>H2ML - Main Line  (London Euston – Wigan WCML ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Hybrid Bill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13254214</td>\n",
       "      <td>HS2</td>\n",
       "      <td>13000015</td>\n",
       "      <td>Published</td>\n",
       "      <td>ML000-RM169</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13254215</td>\n",
       "      <td>HS2</td>\n",
       "      <td>13000015</td>\n",
       "      <td>Published</td>\n",
       "      <td>ML152-TP006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13254209</td>\n",
       "      <td>HS2</td>\n",
       "      <td>13000015</td>\n",
       "      <td>Published</td>\n",
       "      <td>ML183-TP007</td>\n",
       "      <td>13254209</td>\n",
       "      <td>13002298</td>\n",
       "      <td>TP - Trial pit (mechanically dug) (EI-70-10-05)</td>\n",
       "      <td>13000187</td>\n",
       "      <td>H2ML - Main Line  (London Euston – Wigan WCML ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Hybrid Bill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13254216</td>\n",
       "      <td>Phase 2a</td>\n",
       "      <td>13000016</td>\n",
       "      <td>Client-shared</td>\n",
       "      <td>Hamley (North) Drop Inlet Culvert</td>\n",
       "      <td>13254216</td>\n",
       "      <td>13002259</td>\n",
       "      <td>Culvert (CV-BR-CV)</td>\n",
       "      <td>13000187</td>\n",
       "      <td>H2ML - Main Line  (London Euston – Wigan WCML ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ERD 1 Snakegrid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13254217</td>\n",
       "      <td>HS2</td>\n",
       "      <td>13000015</td>\n",
       "      <td>Published</td>\n",
       "      <td>ML049-WS009</td>\n",
       "      <td>13254217</td>\n",
       "      <td>13002289</td>\n",
       "      <td>WS - Windowless sampling (EI-70-05-20)</td>\n",
       "      <td>13000187</td>\n",
       "      <td>H2ML - Main Line  (London Euston – Wigan WCML ...</td>\n",
       "      <td>489069.5</td>\n",
       "      <td>204068.39</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Hybrid Bill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13254218</td>\n",
       "      <td>HS2</td>\n",
       "      <td>13000015</td>\n",
       "      <td>Published</td>\n",
       "      <td>ML159-SW001</td>\n",
       "      <td>13254218</td>\n",
       "      <td>13002049</td>\n",
       "      <td>SW - Surface water sampling point (EI-70-20-30)</td>\n",
       "      <td>13000187</td>\n",
       "      <td>H2ML - Main Line  (London Euston – Wigan WCML ...</td>\n",
       "      <td>419533.21</td>\n",
       "      <td>286076.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Hybrid Bill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13254219</td>\n",
       "      <td>EK Joint Venture</td>\n",
       "      <td>13000015</td>\n",
       "      <td>Archived</td>\n",
       "      <td>Doddershall No. 2 Culvert</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>13254220</td>\n",
       "      <td>HS2</td>\n",
       "      <td>13000015</td>\n",
       "      <td>Published</td>\n",
       "      <td>ML171-TP020</td>\n",
       "      <td>13254220</td>\n",
       "      <td>13002298</td>\n",
       "      <td>TP - Trial pit (mechanically dug) (EI-70-10-05)</td>\n",
       "      <td>13000187</td>\n",
       "      <td>H2ML - Main Line  (London Euston – Wigan WCML ...</td>\n",
       "      <td>418466.79</td>\n",
       "      <td>298358.82</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Hybrid Bill</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Asset_Key         Owner_Key   PHASEID   Asset_Status  \\\n",
       "0  13254212               HS2  13000015      Published   \n",
       "1  13254213               HS2  13000015      Published   \n",
       "2  13254214               HS2  13000015      Published   \n",
       "3  13254215               HS2  13000015      Published   \n",
       "4  13254209               HS2  13000015      Published   \n",
       "5  13254216          Phase 2a  13000016  Client-shared   \n",
       "6  13254217               HS2  13000015      Published   \n",
       "7  13254218               HS2  13000015      Published   \n",
       "8  13254219  EK Joint Venture  13000015       Archived   \n",
       "9  13254220               HS2  13000015      Published   \n",
       "\n",
       "                          Asset_Name   ASSETID Class_Key  \\\n",
       "0                        MS187-CT035  13254212  13002124   \n",
       "1                        ML094-WS037  13254213  13002289   \n",
       "2                        ML000-RM169       NaN       NaN   \n",
       "3                        ML152-TP006       NaN       NaN   \n",
       "4                        ML183-TP007  13254209  13002298   \n",
       "5  Hamley (North) Drop Inlet Culvert  13254216  13002259   \n",
       "6                        ML049-WS009  13254217  13002289   \n",
       "7                        ML159-SW001  13254218  13002049   \n",
       "8          Doddershall No. 2 Culvert       NaN       NaN   \n",
       "9                        ML171-TP020  13254220  13002298   \n",
       "\n",
       "                                        Class_Name Route_Key  \\\n",
       "0      CT - Cone penetration testing (EI-70-25-05)  13000187   \n",
       "1           WS - Windowless sampling (EI-70-05-20)  13000187   \n",
       "2                                              NaN       NaN   \n",
       "3                                              NaN       NaN   \n",
       "4  TP - Trial pit (mechanically dug) (EI-70-10-05)  13000187   \n",
       "5                               Culvert (CV-BR-CV)  13000187   \n",
       "6           WS - Windowless sampling (EI-70-05-20)  13000187   \n",
       "7  SW - Surface water sampling point (EI-70-20-30)  13000187   \n",
       "8                                              NaN       NaN   \n",
       "9  TP - Trial pit (mechanically dug) (EI-70-10-05)  13000187   \n",
       "\n",
       "                                          Route_Name OSGBEASTING OSGBNORTHING  \\\n",
       "0  H2ML - Main Line  (London Euston – Wigan WCML ...         0.0          0.0   \n",
       "1  H2ML - Main Line  (London Euston – Wigan WCML ...         0.0          0.0   \n",
       "2                                                NaN         NaN          NaN   \n",
       "3                                                NaN         NaN          NaN   \n",
       "4  H2ML - Main Line  (London Euston – Wigan WCML ...         0.0          0.0   \n",
       "5  H2ML - Main Line  (London Euston – Wigan WCML ...         0.0          0.0   \n",
       "6  H2ML - Main Line  (London Euston – Wigan WCML ...    489069.5    204068.39   \n",
       "7  H2ML - Main Line  (London Euston – Wigan WCML ...   419533.21     286076.5   \n",
       "8                                                NaN         NaN          NaN   \n",
       "9  H2ML - Main Line  (London Euston – Wigan WCML ...   418466.79    298358.82   \n",
       "\n",
       "  STARTOSGBEASTING STARTOSGBNORTHING Chainage_Baseline  \n",
       "0              0.0               0.0       Hybrid Bill  \n",
       "1              0.0               0.0       Hybrid Bill  \n",
       "2              NaN               NaN               NaN  \n",
       "3              NaN               NaN               NaN  \n",
       "4              0.0               0.0       Hybrid Bill  \n",
       "5              0.0               0.0   ERD 1 Snakegrid  \n",
       "6              0.0               0.0       Hybrid Bill  \n",
       "7              0.0               0.0       Hybrid Bill  \n",
       "8              NaN               NaN               NaN  \n",
       "9              0.0               0.0       Hybrid Bill  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- DIM_Route (Preview) ---\n",
      "Shape: (33, 3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Route_Key",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Route_Code",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Route_Description",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "a5032eaa-8606-4f93-b594-f01d05e250c6",
       "rows": [
        [
         "0",
         "13000184",
         "H2HC",
         "Handsacre, WCML Connecting Line (Streethay Jn. – Handsacre Jn.)"
        ],
        [
         "1",
         "13000185",
         "H2NC",
         "Attleboro’  Jn. – Curdworth Jn. (North Chord)"
        ],
        [
         "2",
         "13000187",
         "H2ML",
         "Main Line  (London Euston – Wigan WCML Connection)"
        ],
        [
         "3",
         "13000191",
         "H2BS",
         "Birmingham Spur (Coleshill Jn. – Birmingham Curzon St.)"
        ],
        [
         "4",
         "13000197",
         "H2EL",
         "Eastern Leg (Curdworth Jn. – Ulleskelf Jn.)"
        ],
        [
         "5",
         "13000603",
         "H2RW",
         "Route Wide Systems"
        ],
        [
         "6",
         "13000803",
         "H2NB",
         "Norton Bridge Connecting Line & Headshunt"
        ],
        [
         "7",
         "13000804",
         "H2SF",
         "Stone IMB-R Facility"
        ],
        [
         "8",
         "13000805",
         "H2SO",
         "Stone Passenger Loop & IMB-R Connection with Headshunt"
        ],
        [
         "9",
         "13000183",
         "CAW",
         "NR: Cricklewood - Acton"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Route_Key</th>\n",
       "      <th>Route_Code</th>\n",
       "      <th>Route_Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13000184</td>\n",
       "      <td>H2HC</td>\n",
       "      <td>Handsacre, WCML Connecting Line (Streethay Jn....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13000185</td>\n",
       "      <td>H2NC</td>\n",
       "      <td>Attleboro’  Jn. – Curdworth Jn. (North Chord)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13000187</td>\n",
       "      <td>H2ML</td>\n",
       "      <td>Main Line  (London Euston – Wigan WCML Connect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13000191</td>\n",
       "      <td>H2BS</td>\n",
       "      <td>Birmingham Spur (Coleshill Jn. – Birmingham Cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13000197</td>\n",
       "      <td>H2EL</td>\n",
       "      <td>Eastern Leg (Curdworth Jn. – Ulleskelf Jn.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13000603</td>\n",
       "      <td>H2RW</td>\n",
       "      <td>Route Wide Systems</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13000803</td>\n",
       "      <td>H2NB</td>\n",
       "      <td>Norton Bridge Connecting Line &amp; Headshunt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13000804</td>\n",
       "      <td>H2SF</td>\n",
       "      <td>Stone IMB-R Facility</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13000805</td>\n",
       "      <td>H2SO</td>\n",
       "      <td>Stone Passenger Loop &amp; IMB-R Connection with H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>13000183</td>\n",
       "      <td>CAW</td>\n",
       "      <td>NR: Cricklewood - Acton</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Route_Key Route_Code                                  Route_Description\n",
       "0  13000184       H2HC  Handsacre, WCML Connecting Line (Streethay Jn....\n",
       "1  13000185       H2NC      Attleboro’  Jn. – Curdworth Jn. (North Chord)\n",
       "2  13000187       H2ML  Main Line  (London Euston – Wigan WCML Connect...\n",
       "3  13000191       H2BS  Birmingham Spur (Coleshill Jn. – Birmingham Cu...\n",
       "4  13000197       H2EL        Eastern Leg (Curdworth Jn. – Ulleskelf Jn.)\n",
       "5  13000603       H2RW                                 Route Wide Systems\n",
       "6  13000803       H2NB          Norton Bridge Connecting Line & Headshunt\n",
       "7  13000804       H2SF                               Stone IMB-R Facility\n",
       "8  13000805       H2SO  Stone Passenger Loop & IMB-R Connection with H...\n",
       "9  13000183        CAW                            NR: Cricklewood - Acton"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- DIM_AssetClass (Preview) ---\n",
      "Shape: (5644, 5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Class_Key",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Class_Code",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Class_Name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Parent_Class_Key",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Class_Description",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "93803fb2-9391-4af6-861f-fe970bebbe79",
       "rows": [
        [
         "0",
         "13002122",
         "TC",
         "Telecommunications",
         null,
         "None"
        ],
        [
         "1",
         "13002123",
         "GR-EP-02",
         "EPS Phase",
         "13002187.0",
         "None"
        ],
        [
         "2",
         "13002124",
         "EI-70-25-05",
         "CT - Cone penetration testing",
         "13002173.0",
         "[Non_Asset] A small diameter (typically 35.7mm) exploratory hole formed by a cone penetrometer machine pushing a 60 degree cone into the ground at a constant rate.\nCone penetration testing is a method used to determine the geotechnical engineering properties of subsurface materials and to delineate stratigraphy. CPTs may also be equipped with a porous filter permitting measurement of excess pore water pressures and permitting dissipation tests to be undertaken (piezocone).\nIf seismic testing is added to the normal CPT test and a seismic cone is used, CS-Seismic cone penetration testing class shall be used.\nA CPT hole is generally not backfilled except its inspection pit."
        ],
        [
         "3",
         "13002128",
         "UT-ET-OH-CN",
         "Overhead Electricity Conductor",
         "13002135.0",
         "None"
        ],
        [
         "4",
         "13002129",
         "NV-CM-PA",
         "Play Area",
         "13001991.0",
         "A place with specific design to allow children to play there"
        ],
        [
         "5",
         "13002130",
         "GR-DP",
         "Depot",
         "13002071.0",
         "None"
        ],
        [
         "6",
         "13002131",
         "TC-TR",
         "Transmission Network",
         "13002122.0",
         "None"
        ],
        [
         "7",
         "13002133",
         "TP-DS-IS",
         "Isolator (Do not use - Withdrawn - Replaced by PC-PS-LV-IS)",
         "13002051.0",
         "An isolator is an electrical device that enables electrical circuits to be safely de-energised, or isolated, for maintenance purposes. An isolator may vary significantly in type and size e.g. as part of a HV system such as overhead line, or within a building to isolate the lighting circuit. These assets should only be identified when they are stand alone items."
        ],
        [
         "8",
         "13002134",
         "EI-70-20-35",
         "UI - Non-intrusive utilities investigations",
         "13001989.0",
         "A test completed by either GPR or observation to identify whether there are any utilities within the proximity of the HS2 route"
        ],
        [
         "9",
         "13002136",
         "UT",
         "Utilities",
         null,
         "None"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class_Key</th>\n",
       "      <th>Class_Code</th>\n",
       "      <th>Class_Name</th>\n",
       "      <th>Parent_Class_Key</th>\n",
       "      <th>Class_Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13002122</td>\n",
       "      <td>TC</td>\n",
       "      <td>Telecommunications</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13002123</td>\n",
       "      <td>GR-EP-02</td>\n",
       "      <td>EPS Phase</td>\n",
       "      <td>13002187.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13002124</td>\n",
       "      <td>EI-70-25-05</td>\n",
       "      <td>CT - Cone penetration testing</td>\n",
       "      <td>13002173.0</td>\n",
       "      <td>[Non_Asset] A small diameter (typically 35.7mm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13002128</td>\n",
       "      <td>UT-ET-OH-CN</td>\n",
       "      <td>Overhead Electricity Conductor</td>\n",
       "      <td>13002135.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13002129</td>\n",
       "      <td>NV-CM-PA</td>\n",
       "      <td>Play Area</td>\n",
       "      <td>13001991.0</td>\n",
       "      <td>A place with specific design to allow children...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13002130</td>\n",
       "      <td>GR-DP</td>\n",
       "      <td>Depot</td>\n",
       "      <td>13002071.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13002131</td>\n",
       "      <td>TC-TR</td>\n",
       "      <td>Transmission Network</td>\n",
       "      <td>13002122.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13002133</td>\n",
       "      <td>TP-DS-IS</td>\n",
       "      <td>Isolator (Do not use - Withdrawn - Replaced by...</td>\n",
       "      <td>13002051.0</td>\n",
       "      <td>An isolator is an electrical device that enabl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13002134</td>\n",
       "      <td>EI-70-20-35</td>\n",
       "      <td>UI - Non-intrusive utilities investigations</td>\n",
       "      <td>13001989.0</td>\n",
       "      <td>A test completed by either GPR or observation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>13002136</td>\n",
       "      <td>UT</td>\n",
       "      <td>Utilities</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Class_Key   Class_Code                                         Class_Name  \\\n",
       "0  13002122           TC                                 Telecommunications   \n",
       "1  13002123     GR-EP-02                                          EPS Phase   \n",
       "2  13002124  EI-70-25-05                      CT - Cone penetration testing   \n",
       "3  13002128  UT-ET-OH-CN                     Overhead Electricity Conductor   \n",
       "4  13002129     NV-CM-PA                                          Play Area   \n",
       "5  13002130        GR-DP                                              Depot   \n",
       "6  13002131        TC-TR                               Transmission Network   \n",
       "7  13002133     TP-DS-IS  Isolator (Do not use - Withdrawn - Replaced by...   \n",
       "8  13002134  EI-70-20-35        UI - Non-intrusive utilities investigations   \n",
       "9  13002136           UT                                          Utilities   \n",
       "\n",
       "  Parent_Class_Key                                  Class_Description  \n",
       "0             None                                               None  \n",
       "1       13002187.0                                               None  \n",
       "2       13002173.0  [Non_Asset] A small diameter (typically 35.7mm...  \n",
       "3       13002135.0                                               None  \n",
       "4       13001991.0  A place with specific design to allow children...  \n",
       "5       13002071.0                                               None  \n",
       "6       13002122.0                                               None  \n",
       "7       13002051.0  An isolator is an electrical device that enabl...  \n",
       "8       13001989.0  A test completed by either GPR or observation ...  \n",
       "9             None                                               None  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- DIM_Organisation (Preview) ---\n",
      "Shape: (28, 3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Owner_Key",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Organisation_Name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Parent_Organisation_Key",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "3ceb8c72-8117-4ee2-847e-d33069be00ef",
       "rows": [
        [
         "0",
         "1",
         "HS2",
         null
        ],
        [
         "1",
         "158",
         "datb",
         null
        ],
        [
         "2",
         "566",
         "SCSJV-MWCC",
         null
        ],
        [
         "3",
         "562",
         "AlignJV-MWCC",
         null
        ],
        [
         "4",
         "565",
         "EKJV-MWCC",
         null
        ],
        [
         "5",
         "564",
         "BBVJV-MWCC",
         null
        ],
        [
         "6",
         "559",
         "Phase 2B",
         null
        ],
        [
         "7",
         "709",
         "EWC-CSJV",
         null
        ],
        [
         "8",
         "908",
         "Mace-Dragados JV",
         null
        ],
        [
         "9",
         "362",
         "Fusion JV",
         null
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Owner_Key</th>\n",
       "      <th>Organisation_Name</th>\n",
       "      <th>Parent_Organisation_Key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>HS2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>158</td>\n",
       "      <td>datb</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>566</td>\n",
       "      <td>SCSJV-MWCC</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>562</td>\n",
       "      <td>AlignJV-MWCC</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>565</td>\n",
       "      <td>EKJV-MWCC</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>564</td>\n",
       "      <td>BBVJV-MWCC</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>559</td>\n",
       "      <td>Phase 2B</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>709</td>\n",
       "      <td>EWC-CSJV</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>908</td>\n",
       "      <td>Mace-Dragados JV</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>362</td>\n",
       "      <td>Fusion JV</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Owner_Key Organisation_Name Parent_Organisation_Key\n",
       "0         1               HS2                    None\n",
       "1       158              datb                    None\n",
       "2       566        SCSJV-MWCC                    None\n",
       "3       562      AlignJV-MWCC                    None\n",
       "4       565         EKJV-MWCC                    None\n",
       "5       564        BBVJV-MWCC                    None\n",
       "6       559          Phase 2B                    None\n",
       "7       709          EWC-CSJV                    None\n",
       "8       908  Mace-Dragados JV                    None\n",
       "9       362         Fusion JV                    None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Star Schema Modeling Logic\n",
    "\n",
    "if assets is not None and locations is not None:\n",
    "    # 1. Create FACT_Asset_Inventory\n",
    "    # Join Assets with Locations to establish primary spatial/linear context\n",
    "    # Note: ASSETCLASS information resides in locations table\n",
    "    # LEFT JOIN ensures all Assets retained even without Location record\n",
    "    \n",
    "    # Identify available columns\n",
    "    available_cols = locations.columns.tolist()\n",
    "    \n",
    "    # Select required columns\n",
    "    location_cols = ['ASSETID', 'ASSET', 'ASSETCLASSID', 'ASSETCLASS', 'ROUTEID', 'ROUTE']\n",
    "    \n",
    "    # Include geospatial columns if present\n",
    "    for col in ['OSGBEASTING', 'OSGBNORTHING', 'STARTOSGBEASTING', 'STARTOSGBNORTHING']:\n",
    "        if col in available_cols:\n",
    "            location_cols.append(col)\n",
    "    \n",
    "    # Include chainage baseline if present\n",
    "    if 'CHAINAGEBASELINE' in available_cols:\n",
    "        location_cols.append('CHAINAGEBASELINE')\n",
    "    \n",
    "    fact_assets = pd.merge(\n",
    "        assets[['ID', 'OWNER', 'PHASEID', 'STATUS', 'NAME']],\n",
    "        locations[location_cols],\n",
    "        left_on='ID', \n",
    "        right_on='ASSETID',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Rename for business clarity\n",
    "    rename_map = {\n",
    "        'ID': 'Asset_Key',\n",
    "        'ASSETCLASSID': 'Class_Key',\n",
    "        'ASSETCLASS': 'Class_Name',\n",
    "        'OWNER': 'Owner_Key',\n",
    "        'ROUTEID': 'Route_Key',\n",
    "        'ROUTE': 'Route_Name',\n",
    "        'STATUS': 'Asset_Status',\n",
    "        'NAME': 'Asset_Name',\n",
    "        'CHAINAGEBASELINE': 'Chainage_Baseline'\n",
    "    }\n",
    "    \n",
    "    fact_assets = fact_assets.rename(columns={k: v for k, v in rename_map.items() if k in fact_assets.columns})\n",
    "    \n",
    "    # Remove duplicate ASSET column\n",
    "    if 'ASSET' in fact_assets.columns:\n",
    "        fact_assets = fact_assets.drop('ASSET', axis=1)\n",
    "    \n",
    "    print(\"--- FACT_Asset_Inventory (Preview) ---\")\n",
    "    print(f\"Shape: {fact_assets.shape}\")\n",
    "    print(f\"Columns: {list(fact_assets.columns)}\")\n",
    "    display(fact_assets.head(10))\n",
    "    \n",
    "    # 2. Create DIM_Route\n",
    "    if routes is not None:\n",
    "        route_cols = ['ID', 'CODE', 'DESCRIPTION']\n",
    "        dim_route = routes[route_cols].rename(columns={\n",
    "            'ID': 'Route_Key',\n",
    "            'CODE': 'Route_Code',\n",
    "            'DESCRIPTION': 'Route_Description'\n",
    "        })\n",
    "        print(\"\\n--- DIM_Route (Preview) ---\")\n",
    "        print(f\"Shape: {dim_route.shape}\")\n",
    "        display(dim_route.head(10))\n",
    "\n",
    "    # 3. Create DIM_AssetClass\n",
    "    if classes is not None:\n",
    "        class_cols = ['ID', 'CODE', 'NAME', 'PARENTID', 'DESCRIPTION']\n",
    "        dim_class = classes[class_cols].rename(columns={\n",
    "            'ID': 'Class_Key',\n",
    "            'CODE': 'Class_Code',\n",
    "            'NAME': 'Class_Name',\n",
    "            'PARENTID': 'Parent_Class_Key',\n",
    "            'DESCRIPTION': 'Class_Description'\n",
    "        })\n",
    "        print(\"\\n--- DIM_AssetClass (Preview) ---\")\n",
    "        print(f\"Shape: {dim_class.shape}\")\n",
    "        display(dim_class.head(10))\n",
    "        \n",
    "    # 4. Create DIM_Organisation\n",
    "    if orgs is not None:\n",
    "        # Identify available columns\n",
    "        org_cols = ['ID', 'NAME']\n",
    "        if 'TYPE' in orgs.columns:\n",
    "            org_cols.append('TYPE')\n",
    "        if 'PARENTID' in orgs.columns:\n",
    "            org_cols.append('PARENTID')\n",
    "            \n",
    "        dim_org = orgs[org_cols].rename(columns={\n",
    "            'ID': 'Owner_Key',\n",
    "            'NAME': 'Organisation_Name',\n",
    "            'TYPE': 'Org_Type',\n",
    "            'PARENTID': 'Parent_Organisation_Key'\n",
    "        })\n",
    "        print(\"\\n--- DIM_Organisation (Preview) ---\")\n",
    "        print(f\"Shape: {dim_org.shape}\")\n",
    "        display(dim_org.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d398fff",
   "metadata": {},
   "source": [
    "## 3. Implementation: Creating Silver Layer Tables\n",
    "\n",
    "Below, we'll implement the actual transformation logic to persist these models as **Delta/Parquet tables** in the Silver Layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ac3a42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persisted FACT_Asset_Inventory: ../data/silver_layer/FACT_Asset_Inventory.parquet (100,057 rows)\n",
      "Persisted DIM_Route: ../data/silver_layer/DIM_Route.parquet (33 rows)\n",
      "Persisted DIM_AssetClass: ../data/silver_layer/DIM_AssetClass.parquet (5,644 rows)\n",
      "Persisted DIM_Organisation: ../data/silver_layer/DIM_Organisation.parquet (28 rows)\n",
      "\n",
      "Silver layer tables persisted to: ../data/silver_layer\n"
     ]
    }
   ],
   "source": [
    "# Persist Silver Layer Tables to Parquet\n",
    "\n",
    "SILVER_DIR = PROJECT_ROOT / \"data\" / \"silver_layer\"\n",
    "SILVER_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Save Fact Table\n",
    "if 'fact_assets' in locals() and fact_assets is not None:\n",
    "    fact_output = SILVER_DIR / \"FACT_Asset_Inventory.parquet\"\n",
    "    fact_assets.to_parquet(fact_output, index=False, engine='pyarrow')\n",
    "    print(f\"Persisted FACT_Asset_Inventory: {fact_output} ({len(fact_assets):,} rows)\")\n",
    "\n",
    "# Save Dimension Tables\n",
    "dim_tables = {\n",
    "    'DIM_Route': 'dim_route',\n",
    "    'DIM_AssetClass': 'dim_class',\n",
    "    'DIM_Organisation': 'dim_org'\n",
    "}\n",
    "\n",
    "for table_name, var_name in dim_tables.items():\n",
    "    if var_name in locals() and locals()[var_name] is not None:\n",
    "        dim_df = locals()[var_name]\n",
    "        dim_output = SILVER_DIR / f\"{table_name}.parquet\"\n",
    "        dim_df.to_parquet(dim_output, index=False, engine='pyarrow')\n",
    "        print(f\"Persisted {table_name}: {dim_output} ({len(dim_df):,} rows)\")\n",
    "\n",
    "print(f\"\\nSilver layer tables persisted to: {SILVER_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6373d6",
   "metadata": {},
   "source": [
    "### 3.4: Expand Dimensions - DIM_Date & DIM_Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3bf7b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persisted DIM_Date: ../data/silver_layer/DIM_Date.parquet (84,961 rows)\n",
      "   Date Range: 2016-02-09 21:25:01 to 2025-11-10 13:49:55\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Date",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "Date_Key",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Year",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "Quarter",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "Month",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "Month_Name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Week",
         "rawType": "UInt32",
         "type": "integer"
        },
        {
         "name": "Day",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "Day_Name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Is_Weekend",
         "rawType": "bool",
         "type": "boolean"
        }
       ],
       "ref": "361b1291-2d6c-4616-8c6b-862c507b4694",
       "rows": [
        [
         "0",
         "2016-02-09 21:25:01",
         "20160209",
         "2016",
         "1",
         "2",
         "February",
         "6",
         "9",
         "Tuesday",
         "False"
        ],
        [
         "1",
         "2016-02-09 21:25:02",
         "20160209",
         "2016",
         "1",
         "2",
         "February",
         "6",
         "9",
         "Tuesday",
         "False"
        ],
        [
         "2",
         "2016-02-09 21:25:03",
         "20160209",
         "2016",
         "1",
         "2",
         "February",
         "6",
         "9",
         "Tuesday",
         "False"
        ],
        [
         "3",
         "2016-02-09 21:25:04",
         "20160209",
         "2016",
         "1",
         "2",
         "February",
         "6",
         "9",
         "Tuesday",
         "False"
        ],
        [
         "4",
         "2016-02-09 21:25:05",
         "20160209",
         "2016",
         "1",
         "2",
         "February",
         "6",
         "9",
         "Tuesday",
         "False"
        ],
        [
         "5",
         "2016-02-09 21:25:06",
         "20160209",
         "2016",
         "1",
         "2",
         "February",
         "6",
         "9",
         "Tuesday",
         "False"
        ],
        [
         "6",
         "2016-02-09 21:25:07",
         "20160209",
         "2016",
         "1",
         "2",
         "February",
         "6",
         "9",
         "Tuesday",
         "False"
        ],
        [
         "7",
         "2016-02-09 21:25:08",
         "20160209",
         "2016",
         "1",
         "2",
         "February",
         "6",
         "9",
         "Tuesday",
         "False"
        ],
        [
         "8",
         "2016-02-09 21:25:09",
         "20160209",
         "2016",
         "1",
         "2",
         "February",
         "6",
         "9",
         "Tuesday",
         "False"
        ],
        [
         "9",
         "2016-02-09 21:25:10",
         "20160209",
         "2016",
         "1",
         "2",
         "February",
         "6",
         "9",
         "Tuesday",
         "False"
        ]
       ],
       "shape": {
        "columns": 10,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Date_Key</th>\n",
       "      <th>Year</th>\n",
       "      <th>Quarter</th>\n",
       "      <th>Month</th>\n",
       "      <th>Month_Name</th>\n",
       "      <th>Week</th>\n",
       "      <th>Day</th>\n",
       "      <th>Day_Name</th>\n",
       "      <th>Is_Weekend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-02-09 21:25:01</td>\n",
       "      <td>20160209</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>February</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-02-09 21:25:02</td>\n",
       "      <td>20160209</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>February</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-02-09 21:25:03</td>\n",
       "      <td>20160209</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>February</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-02-09 21:25:04</td>\n",
       "      <td>20160209</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>February</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-02-09 21:25:05</td>\n",
       "      <td>20160209</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>February</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2016-02-09 21:25:06</td>\n",
       "      <td>20160209</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>February</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2016-02-09 21:25:07</td>\n",
       "      <td>20160209</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>February</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2016-02-09 21:25:08</td>\n",
       "      <td>20160209</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>February</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2016-02-09 21:25:09</td>\n",
       "      <td>20160209</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>February</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2016-02-09 21:25:10</td>\n",
       "      <td>20160209</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>February</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Date  Date_Key  Year  Quarter  Month Month_Name  Week  Day  \\\n",
       "0 2016-02-09 21:25:01  20160209  2016        1      2   February     6    9   \n",
       "1 2016-02-09 21:25:02  20160209  2016        1      2   February     6    9   \n",
       "2 2016-02-09 21:25:03  20160209  2016        1      2   February     6    9   \n",
       "3 2016-02-09 21:25:04  20160209  2016        1      2   February     6    9   \n",
       "4 2016-02-09 21:25:05  20160209  2016        1      2   February     6    9   \n",
       "5 2016-02-09 21:25:06  20160209  2016        1      2   February     6    9   \n",
       "6 2016-02-09 21:25:07  20160209  2016        1      2   February     6    9   \n",
       "7 2016-02-09 21:25:08  20160209  2016        1      2   February     6    9   \n",
       "8 2016-02-09 21:25:09  20160209  2016        1      2   February     6    9   \n",
       "9 2016-02-09 21:25:10  20160209  2016        1      2   February     6    9   \n",
       "\n",
       "  Day_Name  Is_Weekend  \n",
       "0  Tuesday       False  \n",
       "1  Tuesday       False  \n",
       "2  Tuesday       False  \n",
       "3  Tuesday       False  \n",
       "4  Tuesday       False  \n",
       "5  Tuesday       False  \n",
       "6  Tuesday       False  \n",
       "7  Tuesday       False  \n",
       "8  Tuesday       False  \n",
       "9  Tuesday       False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Persisted DIM_Status: ../data/silver_layer/DIM_Status.parquet (4 rows)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Status_Key",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Status_Name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Status_Description",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "9339c2d5-bb19-4868-9633-9d7e351391d3",
       "rows": [
        [
         "0",
         "Published",
         "Published",
         "Asset record published"
        ],
        [
         "1",
         "Client-shared",
         "Client-shared",
         "Asset information shared with client"
        ],
        [
         "2",
         "Archived",
         "Archived",
         "Asset record archived"
        ],
        [
         "3",
         "Work in progress",
         "Work in progress",
         "Asset record under development"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 4
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Status_Key</th>\n",
       "      <th>Status_Name</th>\n",
       "      <th>Status_Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Published</td>\n",
       "      <td>Published</td>\n",
       "      <td>Asset record published</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Client-shared</td>\n",
       "      <td>Client-shared</td>\n",
       "      <td>Asset information shared with client</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Archived</td>\n",
       "      <td>Archived</td>\n",
       "      <td>Asset record archived</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Work in progress</td>\n",
       "      <td>Work in progress</td>\n",
       "      <td>Asset record under development</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Status_Key       Status_Name                    Status_Description\n",
       "0         Published         Published                Asset record published\n",
       "1     Client-shared     Client-shared  Asset information shared with client\n",
       "2          Archived          Archived                 Asset record archived\n",
       "3  Work in progress  Work in progress        Asset record under development"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create Additional Dimension Tables\n",
    "\n",
    "# DIM_Date: Extract all unique dates from asset-related columns\n",
    "if 'assets' in locals() and assets is not None:\n",
    "    date_cols = [col for col in assets.columns if 'DATE' in col.upper() or 'TIME' in col.upper()]\n",
    "    \n",
    "    if date_cols:\n",
    "        all_dates = pd.Series(dtype='datetime64[ns]')\n",
    "        for col in date_cols:\n",
    "            try:\n",
    "                dates = pd.to_datetime(assets[col], errors='coerce')\n",
    "                all_dates = pd.concat([all_dates, dates])\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Create dimension\n",
    "        unique_dates = all_dates.dropna().unique()\n",
    "        dim_date = pd.DataFrame({'Date': unique_dates})\n",
    "        dim_date['Date'] = pd.to_datetime(dim_date['Date'])\n",
    "        dim_date = dim_date.sort_values('Date').reset_index(drop=True)\n",
    "        \n",
    "        # Add derived attributes\n",
    "        dim_date['Date_Key'] = dim_date['Date'].dt.strftime('%Y%m%d').astype(int)\n",
    "        dim_date['Year'] = dim_date['Date'].dt.year\n",
    "        dim_date['Quarter'] = dim_date['Date'].dt.quarter\n",
    "        dim_date['Month'] = dim_date['Date'].dt.month\n",
    "        dim_date['Month_Name'] = dim_date['Date'].dt.month_name()\n",
    "        dim_date['Week'] = dim_date['Date'].dt.isocalendar().week\n",
    "        dim_date['Day'] = dim_date['Date'].dt.day\n",
    "        dim_date['Day_Name'] = dim_date['Date'].dt.day_name()\n",
    "        dim_date['Is_Weekend'] = dim_date['Date'].dt.dayofweek.isin([5, 6])\n",
    "        \n",
    "        # Persist\n",
    "        date_output = SILVER_DIR / \"DIM_Date.parquet\"\n",
    "        dim_date.to_parquet(date_output, index=False, engine='pyarrow')\n",
    "        print(f\"Persisted DIM_Date: {date_output} ({len(dim_date):,} rows)\")\n",
    "        print(f\"   Date Range: {dim_date['Date'].min()} to {dim_date['Date'].max()}\\n\")\n",
    "        display(dim_date.head(10))\n",
    "\n",
    "# DIM_Status: Extract unique statuses from fact table\n",
    "if 'fact_assets' in locals() and fact_assets is not None and 'Asset_Status' in fact_assets.columns:\n",
    "    statuses = fact_assets['Asset_Status'].dropna().unique()\n",
    "    \n",
    "    status_descriptions = {\n",
    "        'Published': 'Asset record published',\n",
    "        'Client-shared': 'Asset information shared with client',\n",
    "        'Archived': 'Asset record archived',\n",
    "        'Work in progress': 'Asset record under development'\n",
    "    }\n",
    "    \n",
    "    dim_status = pd.DataFrame({\n",
    "        'Status_Key': statuses,\n",
    "        'Status_Name': statuses,\n",
    "        'Status_Description': [status_descriptions.get(s, 'Status definition pending') for s in statuses]\n",
    "    })\n",
    "    \n",
    "    # Persist\n",
    "    status_output = SILVER_DIR / \"DIM_Status.parquet\"\n",
    "    dim_status.to_parquet(status_output, index=False, engine='pyarrow')\n",
    "    print(f\"\\nPersisted DIM_Status: {status_output} ({len(dim_status):,} rows)\\n\")\n",
    "    display(dim_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0fdeeb",
   "metadata": {},
   "source": [
    "### 3.5: Summary & Next Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d73371f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SILVER LAYER TRANSFORMATION SUMMARY\n",
      "======================================================================\n",
      "\n",
      "TABLES CREATED: 6\n",
      "   1. FACT_Asset_Inventory (100,057 rows)\n",
      "   2. DIM_Route (33 rows)\n",
      "   3. DIM_AssetClass (5,644 rows)\n",
      "   4. DIM_Organisation (28 rows)\n",
      "   5. DIM_Date (84,961 rows)\n",
      "   6. DIM_Status (4 rows)\n",
      "\n",
      "TOTAL DATA VOLUME: 190,727 rows across all tables\n",
      "\n",
      "OUTPUT LOCATION: ../data/silver_layer\n",
      "   Files on disk: 6\n",
      "   - DIM_AssetClass.parquet (0.48 MB)\n",
      "   - DIM_Date.parquet (0.71 MB)\n",
      "   - DIM_Organisation.parquet (3.0 KB)\n",
      "   - DIM_Route.parquet (4.0 KB)\n",
      "   - DIM_Status.parquet (2.9 KB)\n",
      "   - FACT_Asset_Inventory.parquet (2.18 MB)\n",
      "\n",
      "   NOTE: Small dimension table sizes (KB range) are expected.\n",
      "   Dimension tables contain reference data (routes, statuses, etc.)\n",
      "   Fact tables contain transactional data (larger MB range)\n",
      "\n",
      "======================================================================\n",
      "NEXT ACTIONS FOR REPORTING TEAM\n",
      "======================================================================\n",
      "\n",
      "1. POWERBI/TABLEAU CONNECTION\n",
      "   Connect data source to Silver Layer directory specified above\n",
      "\n",
      "2. CONFIGURE RELATIONSHIPS\n",
      "   - FACT_Asset_Inventory.Route_Key -> DIM_Route.Route_Key\n",
      "   - FACT_Asset_Inventory.Class_Key -> DIM_AssetClass.Class_Key\n",
      "   - FACT_Asset_Inventory.Owner_Key -> DIM_Organisation.Owner_Key\n",
      "   - FACT_Asset_Inventory.Asset_Status -> DIM_Status.Status_Key\n",
      "   - Join date fields using DIM_Date for temporal analysis\n",
      "\n",
      "3. RECOMMENDED REPORTS\n",
      "   - Asset Inventory by Route and Asset Class\n",
      "   - Status Distribution Over Time\n",
      "   - Organisational Asset Ownership Analysis\n",
      "   - Geospatial Asset Map (OSGB coordinates)\n",
      "\n",
      "4. AUTOMATION OPTIONS\n",
      "   - Schedule notebook execution in Microsoft Fabric (daily refresh)\n",
      "   - Integrate into Airflow/cron pipeline for automated processing\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Generate dynamic summary based on created tables\n",
    "print(\"=\" * 70)\n",
    "print(\"SILVER LAYER TRANSFORMATION SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "created_tables = []\n",
    "total_rows = 0\n",
    "\n",
    "# Check Fact Table\n",
    "if 'fact_assets' in locals() and fact_assets is not None:\n",
    "    fact_rows = len(fact_assets)\n",
    "    created_tables.append(f\"FACT_Asset_Inventory ({fact_rows:,} rows)\")\n",
    "    total_rows += fact_rows\n",
    "\n",
    "# Check Dimension Tables\n",
    "dim_tables = {\n",
    "    'DIM_Route': 'dim_route',\n",
    "    'DIM_AssetClass': 'dim_class',\n",
    "    'DIM_Organisation': 'dim_org',\n",
    "    'DIM_Date': 'dim_date',\n",
    "    'DIM_Status': 'dim_status'\n",
    "}\n",
    "\n",
    "for table_name, var_name in dim_tables.items():\n",
    "    if var_name in locals() and locals()[var_name] is not None:\n",
    "        rows = len(locals()[var_name])\n",
    "        created_tables.append(f\"{table_name} ({rows:,} rows)\")\n",
    "        total_rows += rows\n",
    "\n",
    "print(f\"\\nTABLES CREATED: {len(created_tables)}\")\n",
    "for i, table in enumerate(created_tables, 1):\n",
    "    print(f\"   {i}. {table}\")\n",
    "\n",
    "print(f\"\\nTOTAL DATA VOLUME: {total_rows:,} rows across all tables\")\n",
    "\n",
    "if 'SILVER_DIR' in locals():\n",
    "    print(f\"\\nOUTPUT LOCATION: {SILVER_DIR}\")\n",
    "    \n",
    "    # List actual files created\n",
    "    import os\n",
    "    if os.path.exists(SILVER_DIR):\n",
    "        files = [f for f in os.listdir(SILVER_DIR) if f.endswith('.parquet')]\n",
    "        if files:\n",
    "            print(f\"   Files on disk: {len(files)}\")\n",
    "            for f in sorted(files):\n",
    "                file_path = SILVER_DIR / f\n",
    "                size_bytes = os.path.getsize(file_path)\n",
    "                size_mb = size_bytes / (1024 * 1024)\n",
    "                \n",
    "                # Display KB for small files, MB for larger files\n",
    "                if size_mb < 0.01:\n",
    "                    size_kb = size_bytes / 1024\n",
    "                    print(f\"   - {f} ({size_kb:.1f} KB)\")\n",
    "                else:\n",
    "                    print(f\"   - {f} ({size_mb:.2f} MB)\")\n",
    "            \n",
    "            print(\"\\n   NOTE: Small dimension table sizes (KB range) are expected.\")\n",
    "            print(\"   Dimension tables contain reference data (routes, statuses, etc.)\")\n",
    "            print(\"   Fact tables contain transactional data (larger MB range)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"NEXT ACTIONS FOR REPORTING TEAM\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\"\"\n",
    "1. POWERBI/TABLEAU CONNECTION\n",
    "   Connect data source to Silver Layer directory specified above\n",
    "\n",
    "2. CONFIGURE RELATIONSHIPS\n",
    "   - FACT_Asset_Inventory.Route_Key -> DIM_Route.Route_Key\n",
    "   - FACT_Asset_Inventory.Class_Key -> DIM_AssetClass.Class_Key\n",
    "   - FACT_Asset_Inventory.Owner_Key -> DIM_Organisation.Owner_Key\"\"\")\n",
    "\n",
    "if 'dim_status' in locals() and locals()['dim_status'] is not None:\n",
    "    print(\"   - FACT_Asset_Inventory.Asset_Status -> DIM_Status.Status_Key\")\n",
    "\n",
    "if 'dim_date' in locals() and locals()['dim_date'] is not None:\n",
    "    print(\"   - Join date fields using DIM_Date for temporal analysis\")\n",
    "\n",
    "print(\"\"\"\n",
    "3. RECOMMENDED REPORTS\n",
    "   - Asset Inventory by Route and Asset Class\n",
    "   - Status Distribution Over Time\n",
    "   - Organisational Asset Ownership Analysis\"\"\")\n",
    "\n",
    "if 'fact_assets' in locals() and 'OSGBEASTING' in fact_assets.columns:\n",
    "    print(\"   - Geospatial Asset Map (OSGB coordinates)\")\n",
    "\n",
    "print(\"\"\"\n",
    "4. AUTOMATION OPTIONS\n",
    "   - Schedule notebook execution in Microsoft Fabric (daily refresh)\n",
    "   - Integrate into Airflow/cron pipeline for automated processing\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aims_data_platform",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
