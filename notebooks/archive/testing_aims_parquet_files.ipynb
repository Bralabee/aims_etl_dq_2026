{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIMS Data Platform - Exploration Notebook\n",
    "\n",
    "This notebook demonstrates the federated data platform capabilities including:\n",
    "- Incremental data loading\n",
    "- Data quality validation with Great Expectations\n",
    "- Data contracts enforcement\n",
    "- Microsoft Fabric compatibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Imports successful\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd() / 'src'))\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "#from config.settings import settings\n",
    "#from contracts.registry import ContractRegistry\n",
    "# from ingestion.watermark_manager import WatermarkManager\n",
    "# from ingestion.incremental_loader import IncrementalLoader\n",
    "\n",
    "print(\"âœ… Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'settings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize settings and ensure directories exist\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m settings\u001b[38;5;241m.\u001b[39mensure_directories()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSource Data Path: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msettings\u001b[38;5;241m.\u001b[39msource_data_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget Data Path: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msettings\u001b[38;5;241m.\u001b[39mtarget_data_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'settings' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize settings and ensure directories exist\n",
    "settings.ensure_directories()\n",
    "\n",
    "print(f\"Source Data Path: {settings.source_data_path}\")\n",
    "print(f\"Target Data Path: {settings.target_data_path}\")\n",
    "print(f\"Watermark DB Path: {settings.watermark_db_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explore Source Parquet Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'settings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# List all parquet files in source directory\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m parquet_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(settings\u001b[38;5;241m.\u001b[39msource_data_path\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(parquet_files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m parquet files:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m parquet_files[:\u001b[38;5;241m10\u001b[39m]:  \u001b[38;5;66;03m# Show first 10\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'settings' is not defined"
     ]
    }
   ],
   "source": [
    "# List all parquet files in source directory\n",
    "parquet_files = list(settings.source_data_path.glob('*.parquet'))\n",
    "print(f\"Found {len(parquet_files)} parquet files:\")\n",
    "for f in parquet_files[:10]:  # Show first 10\n",
    "    print(f\"  - {f.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and inspect first parquet file\n",
    "if parquet_files:\n",
    "    sample_file = parquet_files[0]\n",
    "    df_sample = pd.read_parquet(sample_file)\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Sample from {sample_file.name}:\")\n",
    "    print(f\"Shape: {df_sample.shape}\")\n",
    "    print(f\"\\nColumns: {list(df_sample.columns)}\")\n",
    "    print(f\"\\nData types:\\n{df_sample.dtypes}\")\n",
    "    print(f\"\\nFirst 5 rows:\")\n",
    "    display(df_sample.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get schema information from parquet file\n",
    "if parquet_files:\n",
    "    parquet_file = pq.ParquetFile(parquet_files[0])\n",
    "    schema = parquet_file.schema\n",
    "    print(\"\\nðŸ” Parquet Schema:\")\n",
    "    print(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Data Platform Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"AIMS-DataPlatform\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(f\"âœ… Spark {spark.version} session created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize watermark manager\n",
    "watermark_manager = WatermarkManager(settings.watermark_db_path)\n",
    "print(\"âœ… Watermark manager initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize contract registry\n",
    "contract_registry_path = Path.cwd() / \"src\" / \"contracts\" / \"schemas\"\n",
    "contract_registry = ContractRegistry(contract_registry_path)\n",
    "print(f\"âœ… Contract registry initialized\")\n",
    "print(f\"Available contracts: {contract_registry.list_contracts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize incremental loader\n",
    "loader = IncrementalLoader(\n",
    "    watermark_manager=watermark_manager,\n",
    "    contract_registry=contract_registry,\n",
    "    spark=spark\n",
    ")\n",
    "print(\"âœ… Incremental loader initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Contracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View data contract\n",
    "contract = contract_registry.get(\"aims_data\")\n",
    "if contract:\n",
    "    print(f\"ðŸ“‹ Contract: {contract.name} v{contract.version}\")\n",
    "    print(f\"Description: {contract.description}\")\n",
    "    print(f\"\\nFields:\")\n",
    "    for field in contract.fields:\n",
    "        nullable = \"nullable\" if field.nullable else \"required\"\n",
    "        print(f\"  - {field.name}: {field.data_type} ({nullable})\")\n",
    "    print(f\"\\nPartition Keys: {contract.partition_keys}\")\n",
    "    print(f\"Primary Keys: {contract.primary_keys}\")\n",
    "else:\n",
    "    print(\"âš ï¸ No contract found. You may need to adjust the schema file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Incremental Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current watermark\n",
    "source_name = \"aims_parquet_data\"\n",
    "current_watermark = watermark_manager.get_watermark(source_name)\n",
    "print(f\"Current watermark for '{source_name}': {current_watermark or 'None (first load)'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load data incrementally\n",
    "# Note: Adjust the watermark_column to match your actual data\n",
    "\"\"\"\n",
    "result = loader.load_incremental(\n",
    "    source_name=\"aims_parquet_data\",\n",
    "    source_path=settings.source_data_path,\n",
    "    target_path=settings.target_data_path / \"aims_delta\",\n",
    "    watermark_column=\"timestamp\",  # Adjust based on your data\n",
    "    contract_name=\"aims_data\",\n",
    "    partition_columns=[\"created_date\"]\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ“Š Load Results:\")\n",
    "for key, value in result.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\"\"\"\n",
    "print(\"âš ï¸ Uncomment and adjust the code above to perform incremental load\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Query Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Delta table (if it exists)\n",
    "delta_path = settings.target_data_path / \"aims_delta\"\n",
    "\n",
    "if delta_path.exists():\n",
    "    df_delta = spark.read.format(\"delta\").load(str(delta_path))\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Delta Table Statistics:\")\n",
    "    print(f\"Total records: {df_delta.count()}\")\n",
    "    print(f\"\\nSchema:\")\n",
    "    df_delta.printSchema()\n",
    "    \n",
    "    print(f\"\\nSample data:\")\n",
    "    df_delta.show(5)\n",
    "else:\n",
    "    print(\"âš ï¸ Delta table not found. Run incremental load first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Delta table with Spark SQL\n",
    "if delta_path.exists():\n",
    "    df_delta.createOrReplaceTempView(\"aims_data\")\n",
    "    \n",
    "    # Example queries\n",
    "    print(\"\\nðŸ“Š Data Summary:\")\n",
    "    spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as total_records,\n",
    "            COUNT(DISTINCT id) as unique_ids,\n",
    "            MIN(timestamp) as earliest_timestamp,\n",
    "            MAX(timestamp) as latest_timestamp\n",
    "        FROM aims_data\n",
    "    \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data quality checks\n",
    "if delta_path.exists():\n",
    "    df = spark.read.format(\"delta\").load(str(delta_path))\n",
    "    \n",
    "    print(\"\\nðŸ” Data Quality Checks:\")\n",
    "    \n",
    "    # Check for nulls\n",
    "    print(\"\\nNull counts:\")\n",
    "    df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns]).show()\n",
    "    \n",
    "    # Check for duplicates (if primary keys exist)\n",
    "    if contract and contract.primary_keys:\n",
    "        duplicate_count = (\n",
    "            df.groupBy(*contract.primary_keys)\n",
    "            .count()\n",
    "            .filter(F.col(\"count\") > 1)\n",
    "            .count()\n",
    "        )\n",
    "        print(f\"\\nDuplicate records (by primary keys): {duplicate_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export to Pandas for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Pandas for further analysis\n",
    "if delta_path.exists():\n",
    "    # Sample data to avoid memory issues\n",
    "    df_pandas = df_delta.limit(10000).toPandas()\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Pandas DataFrame:\")\n",
    "    print(f\"Shape: {df_pandas.shape}\")\n",
    "    display(df_pandas.head())\n",
    "    \n",
    "    print(f\"\\nDescriptive statistics:\")\n",
    "    display(df_pandas.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session\n",
    "# spark.stop()\n",
    "# print(\"âœ… Spark session stopped\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdf_extractor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
