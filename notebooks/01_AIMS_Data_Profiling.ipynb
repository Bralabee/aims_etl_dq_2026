{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e6d892f",
   "metadata": {},
   "source": [
    "# 01. AIMS Data Profiling & Configuration\n",
    "**Role: The Architect**\n",
    "\n",
    "This notebook is the first step in the Data Quality Lifecycle.\n",
    "1.  **Profiles** your raw data to understand its structure and quality.\n",
    "2.  **Generates** validation rules (YAML configurations) automatically.\n",
    "3.  **Validates** a sample batch to ensure the rules are sensible.\n",
    "\n",
    "**Output:** A set of YAML configuration files used by the Ingestion Pipeline (Notebook 02)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ab8c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION ---\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. Detect Environment\n",
    "try:\n",
    "    from notebookutils import mssparkutils\n",
    "    IS_FABRIC = True\n",
    "    print(\"Running in Microsoft Fabric\")\n",
    "except ImportError:\n",
    "    IS_FABRIC = False\n",
    "    print(\"Running Locally\")\n",
    "\n",
    "# 2. Define Paths based on Environment\n",
    "if IS_FABRIC:\n",
    "    # Fabric: Use Lakehouse Paths\n",
    "    # Assumes a default Lakehouse is attached\n",
    "    BASE_DIR = Path(\"/lakehouse/default/Files\")\n",
    "    \n",
    "    # Adjust these subpaths to match your Fabric Lakehouse structure\n",
    "    DATA_PATH = BASE_DIR / \"data/Samples_LH_Bronze_Aims_26_parquet\" \n",
    "    OUTPUT_DIR = BASE_DIR / \"dq_configs\"\n",
    "    \n",
    "    # Performance Settings for Spark/Fabric\n",
    "    NUM_WORKERS = 8 \n",
    "else:\n",
    "    # Local: Use .env or defaults\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    \n",
    "    BASE_DIR = Path(os.getenv(\"BASE_DIR\", \"/home/sanmi/Documents/HS2/HS2_PROJECTS_2025/AIMS_LOCAL\"))\n",
    "    DATA_PATH = BASE_DIR / os.getenv(\"DATA_PATH\", \"data/Samples_LH_Bronze_Aims_26_parquet\")\n",
    "    OUTPUT_DIR = BASE_DIR / os.getenv(\"CONFIG_DIR\", \"dq_great_expectations/generated_configs\")\n",
    "    \n",
    "    # Performance Settings for Local\n",
    "    NUM_WORKERS = 4\n",
    "\n",
    "# Common Settings\n",
    "SAMPLE_SIZE = 100000 \n",
    "\n",
    "# Convert to strings for compatibility\n",
    "DATA_PATH = str(DATA_PATH)\n",
    "OUTPUT_DIR = str(OUTPUT_DIR)\n",
    "\n",
    "print(f\"Configuration:\\n Environment: {'Fabric' if IS_FABRIC else 'Local'}\\n Input: {DATA_PATH}\\n Output: {OUTPUT_DIR}\\n Workers: {NUM_WORKERS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fcfad7",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "Set up the input path, output destination, and performance settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08faf920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the library\n",
    "# Local: Installs from the local dist folder\n",
    "# Fabric: Uncomment the line below and ensure the wheel is uploaded to your Lakehouse Files/libs folder\n",
    "\n",
    "if not IS_FABRIC:\n",
    "    %pip install ../dq_great_expectations/dq_package_dist/fabric_data_quality-1.1.2-py3-none-any.whl --force-reinstall\n",
    "else:\n",
    "    # Example Fabric install command (adjust path as needed)\n",
    "    # %pip install /lakehouse/default/Files/libs/fabric_data_quality-1.1.2-py3-none-any.whl --force-reinstall\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25555ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from dq_framework import DataProfiler, DataQualityValidator, ConfigLoader\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739a3bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the parallel profiling using the library's BatchProfiler\n",
    "from dq_framework import BatchProfiler\n",
    "\n",
    "# Define custom thresholds (optional)\n",
    "# These override the defaults in the library\n",
    "custom_thresholds = {\n",
    "    \"severity_threshold\": \"medium\",\n",
    "    \"null_tolerance\": 5.0,  # tolerance (5%)\n",
    "    \"include_structural\": True,\n",
    "    \"include_completeness\": True,\n",
    "    \"include_validity\": True\n",
    "}\n",
    "\n",
    "if os.path.exists(DATA_PATH):\n",
    "    results = BatchProfiler.run_parallel_profiling(\n",
    "        input_dir=DATA_PATH,\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        workers=NUM_WORKERS,\n",
    "        sample_size=SAMPLE_SIZE,\n",
    "        thresholds=custom_thresholds\n",
    "    )\n",
    "else:\n",
    "    print(f\"Input path does not exist: {DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8903029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect one of the generated config files\n",
    "if results:\n",
    "    success_results = [r for r in results if r['status'] == 'success']\n",
    "    if success_results:\n",
    "        last_output = success_results[0]['output']\n",
    "        print(f\"Reading generated config: {last_output}\")\n",
    "        \n",
    "        with open(last_output, 'r') as f:\n",
    "            print(f.read())\n",
    "    else:\n",
    "        print(\"No successful results to inspect.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f36d9b2",
   "metadata": {},
   "source": [
    "## Part 2: Data Validation Workflow\n",
    "\n",
    "Now that we have profiled the data and generated a configuration (expectations), we can use this configuration to validate new batches of data.\n",
    "\n",
    "This implements the workflow:\n",
    "**Load New Data Batch** -> **Load YAML Config** -> **Run Validation** -> **Get Pass/Fail Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea87fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Iterate through all profiled files and validate them\n",
    "from dq_framework import DataLoader\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "if 'results' in locals() and results:\n",
    "    print(f\"Starting Batch Validation for {len(results)} files...\\n\")\n",
    "    \n",
    "    validation_summary = []\n",
    "    failure_details = []\n",
    "    \n",
    "    for i, result in enumerate(results):\n",
    "        if result['status'] != 'success':\n",
    "            continue\n",
    "            \n",
    "        print(f\"[{i+1}/{len(results)}] Validating: {result['file']}...\", end=\"\\r\")\n",
    "        \n",
    "        # Setup paths\n",
    "        config_path = result['output']\n",
    "        data_file_path = os.path.join(DATA_PATH, result['file'])\n",
    "        \n",
    "        try:\n",
    "            # 2. Initialize Validator\n",
    "            validator = DataQualityValidator(config_path=config_path)\n",
    "            \n",
    "            # 3. Load Data Batch (Safe Mode)\n",
    "            df_batch = DataLoader.load_data(data_file_path, sample_size=100000)\n",
    "            \n",
    "            # 4. Run Validation\n",
    "            validation_results = validator.validate(df_batch)\n",
    "            \n",
    "            # 5. Collect Results\n",
    "            success = validation_results['success']\n",
    "            \n",
    "            summary_entry = {\n",
    "                'File': result['file'],\n",
    "                'Status': 'PASSED' if success else 'FAILED',\n",
    "                'Score (%)': round(validation_results['success_rate'], 1),\n",
    "                'Passed Checks': validation_results['successful_checks'],\n",
    "                'Total Checks': validation_results['evaluated_checks'],\n",
    "                'Failed Checks': len(validation_results.get('failed_expectations', []))\n",
    "            }\n",
    "            validation_summary.append(summary_entry)\n",
    "            \n",
    "            # Collect detailed failures\n",
    "            if not success:\n",
    "                for failure in validation_results.get('failed_expectations', []):\n",
    "                    unexpected_pct = failure.get('details', {}).get('unexpected_percent', None)\n",
    "                    unexpected_str = f\"{unexpected_pct:.1f}%\" if unexpected_pct is not None else \"N/A\"\n",
    "                    \n",
    "                    failure_details.append({\n",
    "                        'File': result['file'],\n",
    "                        'Column': failure['column'],\n",
    "                        'Expectation': failure['expectation'],\n",
    "                        'Unexpected %': unexpected_str,\n",
    "                        'Details': str(failure.get('details', ''))\n",
    "                    })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError validating {result['file']}: {str(e)}\")\n",
    "            validation_summary.append({\n",
    "                'File': result['file'],\n",
    "                'Status': 'ERROR',\n",
    "                'Score (%)': 0.0,\n",
    "                'Passed Checks': 0,\n",
    "                'Total Checks': 0,\n",
    "                'Failed Checks': 0\n",
    "            })\n",
    "            \n",
    "    print(f\"\\n\\n{'='*60}\")\n",
    "    print(\"VALIDATION DASHBOARD\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    if validation_summary:\n",
    "        df_summary = pd.DataFrame(validation_summary)\n",
    "        \n",
    "        # Calculate Stats\n",
    "        total = len(df_summary)\n",
    "        passed = len(df_summary[df_summary['Status'].str.contains('PASSED')])\n",
    "        failed = len(df_summary[df_summary['Status'].str.contains('FAILED')])\n",
    "        avg_score = df_summary['Score (%)'].mean()\n",
    "        \n",
    "        # Display Metrics\n",
    "        print(f\"Total Files: {total} | Passed: {passed} | Failed: {failed} | Avg Score: {avg_score:.1f}%\")\n",
    "        \n",
    "        # Display Summary Table\n",
    "        print(\"\\nSummary Report:\")\n",
    "        display(df_summary)\n",
    "        \n",
    "        # Display Failure Details Table\n",
    "        if failure_details:\n",
    "            print(\"\\nFailure Details Report:\")\n",
    "            df_failures = pd.DataFrame(failure_details)\n",
    "            # Reorder columns for better readability\n",
    "            cols = ['File', 'Column', 'Expectation', 'Unexpected %', 'Details']\n",
    "            display(df_failures[cols])\n",
    "        elif failed > 0:\n",
    "             print(\"\\nFailures detected but no details available.\")\n",
    "        else:\n",
    "             print(\"\\nNo failures detected across all files.\")\n",
    "    else:\n",
    "        print(\"No validation results generated.\")\n",
    "        \n",
    "else:\n",
    "    print(\"No profiling results available. Please run Part 1 first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fabric-dq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
